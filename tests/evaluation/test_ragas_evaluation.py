"""
RAGAS Evaluation Test Suite.

Implements RAGAS evaluation for RAG quality metrics.
Uses real LLM calls (not mocked) as required by RAGAS evaluation.

Acceptance Criteria:
- AC#10: Given golden dataset (20+ query-answer pairs), When I run RAGAS eval,
         Then I see faithfulness, relevancy, precision, recall scores
- AC#11: Given RAGAS results, When I check thresholds,
         Then faithfulness > 0.85 and relevancy > 0.80
- AC#12: Given LangFuse, When I view eval results,
         Then I see RAGAS metrics tracked over time

Reference:
- docs/stories/5/5-3/5-3-implement-ragas-evaluation-suite.md
- docs/testing-strategy.md#RAGAS-Evaluation

Note: Tests requiring LLM calls will be skipped if OPENAI_API_KEY is not set.
"""

import asyncio
import json
import logging
import os
from typing import Any, Dict, List, Optional

import pytest
from pydantic_ai import Agent

# Load environment variables from .env if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

logger = logging.getLogger(__name__)


# ============================================================================
# SKIP CONDITIONS
# ============================================================================
OPENAI_API_KEY_AVAILABLE = os.environ.get("OPENAI_API_KEY") is not None
skip_without_openai = pytest.mark.skipif(
    not OPENAI_API_KEY_AVAILABLE,
    reason="OPENAI_API_KEY not set - RAGAS evaluation requires real LLM calls"
)

DATABASE_URL_AVAILABLE = os.environ.get("DATABASE_URL") is not None
skip_without_database = pytest.mark.skipif(
    not DATABASE_URL_AVAILABLE,
    reason="DATABASE_URL not set - RAG queries require database connection"
)

# ============================================================================
# THRESHOLDS (from testing-strategy.md)
# ============================================================================
FAITHFULNESS_THRESHOLD = 0.85
ANSWER_RELEVANCY_THRESHOLD = 0.80
CONTEXT_PRECISION_THRESHOLD = 0.75
CONTEXT_RECALL_THRESHOLD = 0.70


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def load_golden_dataset(dataset_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Load golden dataset from JSON file.

    Args:
        dataset_path: Optional path to dataset. Defaults to tests/fixtures/golden_dataset.json

    Returns:
        Dict containing version, description, thresholds, and queries

    Raises:
        FileNotFoundError: If dataset file doesn't exist
        json.JSONDecodeError: If dataset is not valid JSON
    """
    if dataset_path is None:
        dataset_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "fixtures", "golden_dataset.json"
        )

    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Validate minimum requirements
    queries = data.get("queries", [])
    if len(queries) < 20:
        raise ValueError(f"Golden dataset must have at least 20 query-answer pairs, got {len(queries)}")

    logger.info(f"Loaded golden dataset with {len(queries)} query-answer pairs")
    return data


def prepare_ragas_dataset(
    queries: List[Dict[str, Any]],
    rag_answers: List[str],
    rag_contexts: List[List[str]],
) -> "EvaluationDataset":
    """
    Convert golden dataset queries with RAG results to RAGAS EvaluationDataset format.

    Args:
        queries: List of golden dataset query entries
        rag_answers: List of answers generated by RAG system
        rag_contexts: List of context lists retrieved by RAG system

    Returns:
        RAGAS EvaluationDataset with user_input, retrieved_contexts, response, reference

    Reference: https://docs.ragas.io/en/stable/getstarted/rag_eval/
    """
    from ragas import EvaluationDataset

    # Prepare data in RAGAS v0.3.9 expected format
    data_list = []
    for q, answer, contexts in zip(queries, rag_answers, rag_contexts):
        data_list.append({
            "user_input": q["query"],
            "retrieved_contexts": contexts,
            "response": answer,
            "reference": q.get("ground_truth", q.get("expected_answer", "")),
        })

    dataset = EvaluationDataset.from_list(data_list)
    logger.info(f"Prepared RAGAS dataset with {len(dataset)} samples")
    return dataset


_embedder_initialized = False
_db_initialized = False


async def reset_db_pool():
    """Reset database pool to recover from event loop corruption.
    
    RAGAS evaluate() can corrupt the event loop even when run in a thread.
    This function closes and reinitializes the DB pool to recover.
    """
    global _db_initialized
    from utils.db_utils import db_pool
    
    if db_pool.pool:
        try:
            await db_pool.close()
            logger.info("Database pool closed for reset")
        except Exception as e:
            logger.warning(f"Error closing DB pool: {e}")
            db_pool.pool = None
    
    _db_initialized = False


async def ensure_embedder_initialized():
    """Ensure global embedder is initialized for RAG queries."""
    global _embedder_initialized, _db_initialized

    # Initialize database pool first
    if not _db_initialized:
        from utils.db_utils import db_pool
        # Reset pool if it exists but might be corrupted
        if db_pool.pool:
            try:
                # Test if pool is healthy
                async with db_pool.acquire() as conn:
                    await conn.fetchval("SELECT 1")
            except Exception:
                # Pool is corrupted, reset it
                logger.warning("DB pool corrupted, resetting...")
                db_pool.pool = None
        
        await db_pool.initialize()
        _db_initialized = True
        logger.info("Database pool initialized for RAGAS evaluation")

    # Initialize embedder
    if not _embedder_initialized:
        from core.rag_service import initialize_global_embedder, _embedder_ready
        await initialize_global_embedder()
        # Wait for initialization to complete (may take ~40s on cold start)
        logger.info("Waiting for embedder initialization (may take ~40s)...")
        await _embedder_ready.wait()
        _embedder_initialized = True
        logger.info("Global embedder initialized for RAGAS evaluation")


async def run_rag_query(query: str, limit: int = 5) -> tuple[str, List[str]]:
    """
    Execute RAG query using core/rag_service.py and generate answer via LLM.

    Args:
        query: User query string
        limit: Number of context chunks to retrieve

    Returns:
        Tuple of (answer, contexts) where:
        - answer: LLM-generated answer based on contexts
        - contexts: List of retrieved context strings
    """
    from core.rag_service import search_knowledge_base_structured

    try:
        # Ensure embedder is initialized before running RAG query
        await ensure_embedder_initialized()

        # Step 1: Retrieve contexts using embedding search
        result = await search_knowledge_base_structured(query, limit=limit)
        results = result.get("results", [])

        # Extract contexts from search results
        contexts = [r["content"] for r in results]

        if not contexts:
            return "No relevant information found in the knowledge base.", []

        # Step 2: Generate answer using LLM with retrieved contexts
        # PydanticAI uses model string format, API key from OPENAI_API_KEY env var
        model_name = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
        
        agent = Agent(
            f"openai:{model_name}",
            system_prompt=(
                "You are a helpful assistant that answers questions based on provided context. "
                "Use ONLY the information from the context to answer. "
                "If the context doesn't contain enough information, say so clearly. "
                "Be concise and accurate."
            ),
        )

        # Prepare context for LLM (use top 3 contexts)
        context_text = "\n\n".join([
            f"Context {i+1}:\n{ctx}" 
            for i, ctx in enumerate(contexts[:3])
        ])

        prompt = f"""Based on the following context, answer the question.

Context:
{context_text}

Question: {query}

Provide a clear, concise answer based only on the context provided."""

        result = await agent.run(prompt)
        answer = result.output if hasattr(result, 'output') else str(result.data)

        logger.info(f"Generated answer for query '{query[:50]}...'")

        return answer, contexts

    except Exception as e:
        logger.error(f"RAG query failed for '{query}': {e}", exc_info=True)
        # Return error message that won't artificially inflate scores
        return (
            "I apologize, but I encountered an error while processing your question. "
            "Please try again later.",
            []
        )


async def generate_evaluation_batch(
    queries: List[Dict[str, Any]], limit: int = 5
) -> tuple[List[str], List[List[str]]]:
    """
    Generate batch evaluation data from golden dataset by running RAG queries.

    Args:
        queries: List of golden dataset query entries
        limit: Number of context chunks per query

    Returns:
        Tuple of (answers, contexts) lists
    
    Note: Queries are executed sequentially to avoid event loop conflicts
    when RAGAS evaluation runs in a separate thread.
    """
    answers = []
    all_contexts = []

    for i, q in enumerate(queries):
        logger.info(f"Processing query {i+1}/{len(queries)}: {q['query'][:50]}...")
        answer, contexts = await run_rag_query(q["query"], limit=limit)
        answers.append(answer)
        all_contexts.append(contexts if contexts else ["No context retrieved"])

    return answers, all_contexts


def initialize_ragas_metrics():
    """
    Initialize RAGAS metrics.

    Returns:
        Tuple of (metrics list, evaluator_llm) for RAGAS v0.3.9

    Reference: https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/
    """
    from langchain_openai import ChatOpenAI
    from ragas.llms import LangchainLLMWrapper
    from ragas.metrics import (
        Faithfulness,
        LLMContextRecall,
        ResponseRelevancy,
    )

    # Initialize evaluator LLM (RAGAS v0.3.9 passes LLM to evaluate())
    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o-mini"))

    # Create metric instances (core RAGAS metrics)
    metrics = [
        Faithfulness(),
        ResponseRelevancy(),
        LLMContextRecall(),
    ]

    logger.info(f"Initialized {len(metrics)} RAGAS metrics")
    return metrics, evaluator_llm


async def execute_ragas_evaluation(
    dataset: "EvaluationDataset",
    metrics: List,
    evaluator_llm: Any,
) -> Dict[str, float]:
    """
    Execute RAGAS evaluation on dataset.

    Args:
        dataset: RAGAS EvaluationDataset with user_input, retrieved_contexts, response, reference
        metrics: List of initialized RAGAS metrics
        evaluator_llm: LLM wrapper for evaluation

    Returns:
        Dict of metric_name -> score (float)
    
    Note:
        Uses asyncio.to_thread with evaluate() instead of aevaluate() because
        aevaluate() closes the event loop after completion, breaking subsequent
        async operations in pytest's session-scoped event loop.
    """
    from ragas import evaluate
    from ragas.run_config import RunConfig
    
    logger.info(f"Starting RAGAS evaluation with {len(metrics)} metrics on {len(dataset)} samples...")
    
    # Use evaluate() in thread to isolate RAGAS from pytest's event loop
    # aevaluate() causes "Event loop is closed" errors in subsequent tests
    run_config = RunConfig(max_workers=10, timeout=120)
    
    result = await asyncio.to_thread(
        evaluate,
        dataset=dataset,
        metrics=metrics,
        llm=evaluator_llm,
        run_config=run_config,
        raise_exceptions=False,
        allow_nest_asyncio=False,  # Prevent event loop corruption in pytest
    )
    
    # Reset DB pool after RAGAS evaluation to recover from potential event loop corruption
    # RAGAS can corrupt asyncpg pool even with allow_nest_asyncio=False
    await reset_db_pool()

    # Convert to pandas DataFrame and extract mean scores
    df = result.to_pandas()

    # Extract mean scores for each metric from DataFrame columns
    scores = {}
    for metric in metrics:
        metric_name = metric.name
        if metric_name in df.columns:
            # Calculate mean score (ignoring NaN values)
            mean_score = df[metric_name].mean()
            scores[metric_name] = float(mean_score)
            logger.info(f"  {metric_name}: {mean_score:.4f}")

    return scores


def track_ragas_results(
    scores: Dict[str, float],
    trace_metadata: Optional[Dict[str, Any]] = None,
) -> Optional[str]:
    """
    Upload RAGAS scores to LangFuse for tracking over time.

    Args:
        scores: Dict of metric_name -> score
        trace_metadata: Optional metadata for the trace

    Returns:
        Trace ID if successful, None if LangFuse unavailable

    Reference: https://langfuse.com/guides/cookbook/evaluation_of_rag_with_ragas
    """
    try:
        from langfuse import Langfuse

        langfuse = Langfuse()

        # Create trace for evaluation run
        trace = langfuse.trace(
            name="ragas_evaluation",
            metadata={
                "source": "ragas_evaluation",
                "evaluation_type": "ragas",
                **(trace_metadata or {}),
            },
        )

        # Upload each score
        for metric_name, score_value in scores.items():
            langfuse.score(
                name=metric_name,
                value=float(score_value),
                trace_id=trace.id,
            )
            logger.info(f"Uploaded score to LangFuse: {metric_name}={score_value:.4f}")

        # Flush to ensure data is sent
        langfuse.flush()

        logger.info(f"RAGAS scores tracked in LangFuse (trace_id={trace.id})")
        return trace.id

    except Exception as e:
        logger.warning(f"LangFuse tracking failed (graceful degradation): {e}")
        return None


def verify_thresholds(scores: Dict[str, float]) -> tuple[bool, List[str]]:
    """
    Verify RAGAS scores meet required thresholds.

    Args:
        scores: Dict of metric_name -> score

    Returns:
        Tuple of (all_passed, list of failure messages)
    """
    failures = []

    faithfulness = scores.get("faithfulness", 0)
    if faithfulness <= FAITHFULNESS_THRESHOLD:
        failures.append(
            f"faithfulness={faithfulness:.4f} (required > {FAITHFULNESS_THRESHOLD})"
        )

    relevancy = scores.get("answer_relevancy", 0)
    if relevancy <= ANSWER_RELEVANCY_THRESHOLD:
        failures.append(
            f"answer_relevancy={relevancy:.4f} (required > {ANSWER_RELEVANCY_THRESHOLD})"
        )

    return len(failures) == 0, failures


# ============================================================================
# PYTEST FIXTURES
# ============================================================================


@pytest.fixture
def golden_dataset_data():
    """Load golden dataset for tests."""
    return load_golden_dataset()


@pytest.fixture
def ragas_metrics():
    """Initialize RAGAS metrics and evaluator LLM."""
    if not OPENAI_API_KEY_AVAILABLE:
        pytest.skip("OPENAI_API_KEY not set - cannot initialize RAGAS metrics")
    metrics, evaluator_llm = initialize_ragas_metrics()
    return metrics, evaluator_llm


# ============================================================================
# RAGAS EVALUATION TESTS
# ============================================================================


@pytest.mark.ragas
@pytest.mark.asyncio
async def test_golden_dataset_has_minimum_queries(golden_dataset_data):
    """
    Verify golden dataset contains at least 20 query-answer pairs.

    AC: #1/AC#10 - Golden dataset requirement
    """
    # Arrange
    queries = golden_dataset_data["queries"]

    # Act
    query_count = len(queries)

    # Assert
    assert query_count >= 20, f"Golden dataset must have at least 20 queries, got {query_count}"
    logger.info(f"Golden dataset validated: {query_count} query-answer pairs")


@pytest.mark.ragas
@pytest.mark.asyncio
@skip_without_openai
@skip_without_database
async def test_ragas_evaluation_calculates_all_metrics(golden_dataset_data, ragas_metrics):
    """
    Verify RAGAS evaluation calculates all metrics (faithfulness, relevancy, recall, correctness).

    AC: #1/AC#10 - RAGAS eval shows all metrics
    """
    # Arrange
    metrics, evaluator_llm = ragas_metrics
    queries = golden_dataset_data["queries"][:5]  # Use subset for faster test
    answers, contexts = await generate_evaluation_batch(queries, limit=3)
    dataset = prepare_ragas_dataset(queries, answers, contexts)

    # Act
    scores = await execute_ragas_evaluation(dataset, metrics, evaluator_llm)

    # Assert - Verify core metrics are present (RAGAS v0.3.9)
    # Note: Some metrics may not be computed if data is incomplete
    core_metrics = ["faithfulness", "answer_relevancy", "context_recall"]
    for metric in core_metrics:
        assert metric in scores, f"Missing metric: {metric}. Available: {list(scores.keys())}"
        assert isinstance(scores[metric], float), f"Metric {metric} should be float"

    # Verify at least some metrics are in valid range
    assert len(scores) >= 3, f"Expected at least 3 metrics, got {len(scores)}"
    logger.info(f"All metrics calculated successfully: {scores}")


@pytest.mark.ragas
@pytest.mark.asyncio
@skip_without_openai
@skip_without_database
async def test_ragas_evaluation_meets_thresholds(golden_dataset_data, ragas_metrics):
    """
    Verify RAGAS evaluation meets quality thresholds.

    AC: #2/AC#11 - faithfulness > 0.85, relevancy > 0.80
    """
    # Arrange
    metrics, evaluator_llm = ragas_metrics
    queries = golden_dataset_data["queries"][:10]  # Use subset for faster test
    answers, contexts = await generate_evaluation_batch(queries, limit=5)
    dataset = prepare_ragas_dataset(queries, answers, contexts)

    # Act
    scores = await execute_ragas_evaluation(dataset, metrics, evaluator_llm)
    passed, failures = verify_thresholds(scores)

    # Assert
    if not passed:
        failure_msg = "\n".join(failures)
        pytest.fail(f"RAGAS thresholds not met:\n{failure_msg}")

    logger.info(f"All thresholds met: faithfulness={scores.get('faithfulness', 0):.4f}, "
                f"answer_relevancy={scores.get('answer_relevancy', 0):.4f}")


@pytest.mark.ragas
@pytest.mark.asyncio
@skip_without_openai
@skip_without_database
async def test_ragas_evaluation_tracks_in_langfuse(golden_dataset_data, ragas_metrics):
    """
    Verify RAGAS scores are uploaded to LangFuse.

    AC: #3/AC#12 - RAGAS metrics tracked in LangFuse
    """
    # Arrange
    metrics, evaluator_llm = ragas_metrics
    queries = golden_dataset_data["queries"][:3]  # Minimal subset for tracking test
    answers, contexts = await generate_evaluation_batch(queries, limit=3)
    dataset = prepare_ragas_dataset(queries, answers, contexts)

    # Act
    scores = await execute_ragas_evaluation(dataset, metrics, evaluator_llm)
    trace_id = track_ragas_results(scores, {"test_name": "test_ragas_evaluation_tracks_in_langfuse"})

    # Assert - If LangFuse is available, trace_id should be set
    # If not available, graceful degradation means trace_id is None but test passes
    if trace_id:
        logger.info(f"RAGAS scores tracked in LangFuse: trace_id={trace_id}")
        assert isinstance(trace_id, str), "trace_id should be a string"
    else:
        logger.warning("LangFuse tracking skipped (graceful degradation)")


@pytest.mark.ragas
@pytest.mark.asyncio
@skip_without_openai
@skip_without_database
async def test_ragas_evaluation_graceful_degradation(golden_dataset_data, ragas_metrics):
    """
    Verify evaluation continues if LangFuse is unavailable.

    AC: #3/AC#12 - Graceful degradation
    """
    import sys
    from types import ModuleType
    
    # Arrange
    metrics, evaluator_llm = ragas_metrics
    queries = golden_dataset_data["queries"][:2]  # Minimal subset
    answers, contexts = await generate_evaluation_batch(queries, limit=2)
    dataset = prepare_ragas_dataset(queries, answers, contexts)

    # Remove langfuse from sys.modules to force re-import in track_ragas_results
    original_langfuse = sys.modules.pop("langfuse", None)
    
    try:
        # Create mock module that raises on Langfuse() instantiation
        mock_langfuse_module = ModuleType("langfuse")
        
        def mock_langfuse_init(*args, **kwargs):
            raise ConnectionError("LangFuse service unavailable")
        
        mock_langfuse_module.Langfuse = mock_langfuse_init
        sys.modules["langfuse"] = mock_langfuse_module

        # Act
        scores = await execute_ragas_evaluation(dataset, metrics, evaluator_llm)
        trace_id = track_ragas_results(scores)  # Should not raise

        # Assert - Evaluation completed, tracking gracefully degraded
        assert len(scores) > 0, "Scores should be calculated even without LangFuse"
        assert trace_id is None, "trace_id should be None when LangFuse unavailable"
        logger.info("Graceful degradation verified: evaluation completed without LangFuse")
    finally:
        # Restore original langfuse module
        if original_langfuse is not None:
            sys.modules["langfuse"] = original_langfuse
        else:
            sys.modules.pop("langfuse", None)


@pytest.mark.ragas
@pytest.mark.asyncio
async def test_prepare_ragas_dataset_format(golden_dataset_data):
    """
    Verify prepare_ragas_dataset creates correct RAGAS EvaluationDataset format.

    AC: #1/AC#10 - Dataset preparation
    """
    # Arrange
    queries = golden_dataset_data["queries"][:3]
    mock_answers = ["answer1", "answer2", "answer3"]
    mock_contexts = [["ctx1"], ["ctx2a", "ctx2b"], ["ctx3"]]

    # Act
    dataset = prepare_ragas_dataset(queries, mock_answers, mock_contexts)

    # Assert
    assert len(dataset) == 3, f"Dataset should have 3 samples, got {len(dataset)}"

    # RAGAS v0.3.9 uses EvaluationDataset - verify first sample
    sample = dataset[0]
    assert sample.user_input == queries[0]["query"], "user_input mismatch"
    assert sample.response == "answer1", "response mismatch"
    assert sample.retrieved_contexts == ["ctx1"], "retrieved_contexts mismatch"


@pytest.mark.ragas
@pytest.mark.asyncio
async def test_load_golden_dataset_validates_minimum_queries():
    """
    Verify load_golden_dataset raises error if dataset has < 20 queries.

    AC: #1/AC#10 - Dataset validation
    """
    # Arrange - Create temp file with insufficient queries
    import tempfile

    insufficient_data = {
        "version": "1.0",
        "queries": [{"query": "q1", "expected_answer": "a1"}] * 10  # Only 10 queries
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(insufficient_data, f)
        temp_path = f.name

    # Act & Assert
    try:
        with pytest.raises(ValueError, match="at least 20"):
            load_golden_dataset(temp_path)
    finally:
        os.unlink(temp_path)


@pytest.mark.ragas
@pytest.mark.asyncio
async def test_verify_thresholds_detects_failures():
    """
    Verify threshold verification correctly detects failures.

    AC: #2/AC#11 - Threshold verification
    """
    # Arrange - Scores below thresholds
    low_scores = {
        "faithfulness": 0.70,  # Below 0.85
        "answer_relevancy": 0.60,  # Below 0.80
    }

    # Act
    passed, failures = verify_thresholds(low_scores)

    # Assert
    assert not passed, "Should detect threshold failures"
    assert len(failures) == 2, f"Should have 2 failures, got {len(failures)}"
    assert any("faithfulness" in f for f in failures)
    assert any("answer_relevancy" in f for f in failures)


@pytest.mark.ragas
@pytest.mark.asyncio
async def test_verify_thresholds_passes_when_met():
    """
    Verify threshold verification passes when thresholds are met.

    AC: #2/AC#11 - Threshold verification
    """
    # Arrange - Scores above thresholds
    high_scores = {
        "faithfulness": 0.90,  # Above 0.85
        "answer_relevancy": 0.85,  # Above 0.80
    }

    # Act
    passed, failures = verify_thresholds(high_scores)

    # Assert
    assert passed, "Should pass when thresholds met"
    assert len(failures) == 0, f"Should have no failures, got {failures}"

