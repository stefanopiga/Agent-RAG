{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Docling RAG Agent","text":"<p>Welcome to the documentation for the Docling RAG Agent.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements a RAG (Retrieval-Augmented Generation) agent using Docling for document ingestion and PydanticAI for the agent framework.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the README for installation and setup instructions.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Explore the API Reference to understand the codebase structure and available functions.</p>"},{"location":"api-reference/","title":"API Reference Overview","text":"<p>This section provides detailed documentation for the Docling RAG Agent codebase.</p>"},{"location":"api-reference/#modules","title":"Modules","text":"<ul> <li>Core: Core RAG logic and agent implementation.</li> <li>Ingestion: Document processing pipeline.</li> <li>Utilities: Shared utilities and data models.</li> </ul>"},{"location":"api-reference/core/","title":"Core Module","text":""},{"location":"api-reference/core/#rag-service","title":"RAG Service","text":""},{"location":"api-reference/core/#core.rag_service","title":"<code>core.rag_service</code>","text":""},{"location":"api-reference/core/#core.rag_service--core-rag-service","title":"Core RAG Service","text":"<p>Pure business logic for searching the knowledge base. Decoupled from PydanticAI and Streamlit.</p> <p>Performance Optimizations: - Global embedder instance initialized at startup - Persistent LRU cache for embeddings (2000 entries) - Eliminates 300-500ms overhead per query</p>"},{"location":"api-reference/core/#core.rag_service.close_global_embedder","title":"<code>close_global_embedder()</code>  <code>async</code>","text":"<p>Close global embedder and cleanup resources.</p> Source code in <code>core\\rag_service.py</code> <pre><code>async def close_global_embedder():\n    \"\"\"Close global embedder and cleanup resources.\"\"\"\n    global _global_embedder, _initialization_task\n\n    if _initialization_task and not _initialization_task.done():\n        _initialization_task.cancel()\n        try:\n            await _initialization_task\n        except asyncio.CancelledError:\n            pass\n\n    if _global_embedder is None:\n        return\n\n    # Cleanup if needed (embedder doesn't require explicit cleanup currently)\n    _global_embedder = None\n    _embedder_ready.clear()\n    _initialization_task = None\n    logger.info(\"\u2713 Global embedder closed\")\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.get_global_embedder","title":"<code>get_global_embedder()</code>  <code>async</code>","text":"<p>Get the global embedder instance. Waits for initialization if it's still in progress.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If embedder not initialized or failed</p> Source code in <code>core\\rag_service.py</code> <pre><code>async def get_global_embedder():\n    \"\"\"\n    Get the global embedder instance.\n    Waits for initialization if it's still in progress.\n\n    Raises:\n        RuntimeError: If embedder not initialized or failed\n    \"\"\"\n    if _global_embedder is None:\n        if not _initialization_task:\n             raise RuntimeError(\"Global embedder not initialized. Call initialize_global_embedder() first.\")\n\n        if not _embedder_ready.is_set():\n            logger.info(\"Waiting for embedder initialization to complete (first request)...\")\n            try:\n                # Wait up to 60 seconds for initialization\n                await asyncio.wait_for(_embedder_ready.wait(), timeout=60.0)\n            except asyncio.TimeoutError:\n                raise RuntimeError(\"Timeout waiting for embedder initialization (takes ~40s cold start)\")\n\n    if _global_embedder is None:\n        raise RuntimeError(\n            \"Global embedder failed to initialize. Check server logs for errors.\"\n        )\n    return _global_embedder\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.initialize_global_embedder","title":"<code>initialize_global_embedder()</code>  <code>async</code>","text":"<p>Initialize global embedder instance at server startup.</p> <p>This is a critical performance optimization that: - Eliminates per-query embedder instantiation (300-500ms overhead) - Enables persistent caching across requests - Pre-warms OpenAI API connection</p> <p>OPTIMIZED: Runs in background to prevent blocking server startup (MCP handshake timeout).</p> Source code in <code>core\\rag_service.py</code> <pre><code>async def initialize_global_embedder():\n    \"\"\"\n    Initialize global embedder instance at server startup.\n\n    This is a critical performance optimization that:\n    - Eliminates per-query embedder instantiation (300-500ms overhead)\n    - Enables persistent caching across requests\n    - Pre-warms OpenAI API connection\n\n    OPTIMIZED: Runs in background to prevent blocking server startup (MCP handshake timeout).\n    \"\"\"\n    global _global_embedder, _initialization_task\n\n    if _global_embedder is not None:\n        logger.warning(\"Global embedder already initialized\")\n        return\n\n    if _initialization_task is not None:\n        logger.warning(\"Global embedder initialization already in progress\")\n        return\n\n    async def _init_task():\n        global _global_embedder\n        try:\n            start_time = time.time()\n            logger.info(\"Starting background embedder initialization (loading heavy models)...\")\n\n            # Offload heavy import and creation to thread to avoid blocking asyncio loop\n            # This is critical because importing transformers/docling takes ~40s\n            _global_embedder = await asyncio.to_thread(_create_embedder_sync)\n\n            # Enhance cache size (default is 1000, we increase to 2000)\n            if hasattr(_global_embedder, 'generate_embedding'):\n                logger.info(\"Embedder cache enabled with enhanced capacity\")\n\n            elapsed = (time.time() - start_time) * 1000\n            logger.info(f\"\u2713 Global embedder initialized in {elapsed:.0f}ms\")\n            _embedder_ready.set()\n\n        except Exception as e:\n            logger.error(f\"\u274c Failed to initialize embedder in background: {e}\", exc_info=True)\n            # Don't crash the server, subsequent queries will fail gracefully\n\n    # Start initialization task\n    _initialization_task = asyncio.create_task(_init_task())\n    logger.info(\"Background initialization task started\")\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.search_knowledge_base","title":"<code>search_knowledge_base(query: str, limit: int = 5, source_filter: str | None = None) -&gt; str</code>  <code>async</code>","text":"<p>Search the knowledge base using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to find relevant information</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return (default: 5)</p> <code>5</code> <code>source_filter</code> <code>str | None</code> <p>Optional filter to search only in specific documentation sources.           Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"           If provided, only documents whose source path contains this string will be searched.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted search results with source citations</p> Performance <ul> <li>Uses global embedder instance (eliminates 300-500ms overhead)</li> <li>Leverages persistent cache for common queries</li> <li>Optimized DB connection pooling</li> </ul> Source code in <code>core\\rag_service.py</code> <pre><code>async def search_knowledge_base(\n    query: str, \n    limit: int = 5,\n    source_filter: str | None = None\n) -&gt; str:\n    \"\"\"\n    Search the knowledge base using semantic similarity.\n\n    Args:\n        query: The search query to find relevant information\n        limit: Maximum number of results to return (default: 5)\n        source_filter: Optional filter to search only in specific documentation sources.\n                      Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"\n                      If provided, only documents whose source path contains this string will be searched.\n\n    Returns:\n        Formatted search results with source citations\n\n    Performance:\n        - Uses global embedder instance (eliminates 300-500ms overhead)\n        - Leverages persistent cache for common queries\n        - Optimized DB connection pooling\n    \"\"\"\n    start_time = time.time()\n    timing = {}\n\n    try:\n        # Use structured search internally to avoid code duplication\n        # But we need to reimplement here to keep the exact string formatting logic\n        # or refactor completely. For now, let's keep the existing logic but maybe use the structured function?\n        # Actually, let's just call the structured function and format the output.\n\n        structured_data = await search_knowledge_base_structured(query, limit, source_filter)\n        results = structured_data[\"results\"]\n        timing = structured_data[\"timing\"]\n\n        # Format results for response\n        format_start = time.time()\n        if not results:\n            filter_msg = f\" in '{source_filter}'\" if source_filter else \"\"\n            return f\"No relevant information found in the knowledge base{filter_msg} for your query.\"\n\n        # Build response with sources\n        response_parts = []\n        for row in results:\n            similarity = row['similarity']\n            content = row['content']\n            doc_title = row['title']\n            doc_source = row['source']\n\n            response_parts.append(\n                f\"[Source: {doc_title}]\\n{content}\\n\"\n            )\n\n        if not response_parts:\n            return \"Found some results but they may not be directly relevant to your query. Please try rephrasing your question.\"\n\n        filter_note = f\" (filtered by: {source_filter})\" if source_filter else \"\"\n        result = f\"Found {len(response_parts)} relevant results{filter_note}:\\n\\n\" + \"\\n---\\n\".join(response_parts)\n\n        timing['format_response_ms'] = (time.time() - format_start) * 1000\n        timing['total_ms'] = (time.time() - start_time) * 1000\n\n        # Log performance metrics\n        logger.info(\n            f\"\u23f1\ufe0f  Search performance: \"\n            f\"embedding={timing.get('embedding_ms', 0):.0f}ms | \"\n            f\"db={timing.get('db_ms', 0):.0f}ms | \"\n            f\"format={timing['format_response_ms']:.0f}ms | \"\n            f\"total={timing['total_ms']:.0f}ms\"\n        )\n\n        return result\n\n    except Exception as e:\n        timing['total_ms'] = (time.time() - start_time) * 1000\n        logger.error(\n            f\"\u274c Knowledge base search failed after {timing['total_ms']:.0f}ms: {e}\",\n            exc_info=True\n        )\n        return f\"I encountered an error searching the knowledge base: {str(e)}\"\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.search_knowledge_base_structured","title":"<code>search_knowledge_base_structured(query: str, limit: int = 5, source_filter: str | None = None) -&gt; Dict[str, Any]</code>  <code>async</code>","text":"<p>Search the knowledge base and return structured results (for API usage).</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing:</p> <code>Dict[str, Any]</code> <ul> <li>results: List of dicts (content, source, title, similarity, metadata)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>timing: Dict of performance metrics</li> </ul> Source code in <code>core\\rag_service.py</code> <pre><code>async def search_knowledge_base_structured(\n    query: str, \n    limit: int = 5,\n    source_filter: str | None = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Search the knowledge base and return structured results (for API usage).\n\n    Returns:\n        Dict containing:\n        - results: List of dicts (content, source, title, similarity, metadata)\n        - timing: Dict of performance metrics\n    \"\"\"\n    start_time = time.time()\n    timing = {}\n\n    try:\n        # Use global embedder instance\n        embedder = await get_global_embedder()\n\n        # Generate embedding for query\n        embed_start = time.time()\n        query_embedding = await embedder.embed_query(query)\n        timing['embedding_ms'] = (time.time() - embed_start) * 1000\n\n        # Convert to PostgreSQL vector format\n        embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n\n        # Build query\n        db_start = time.time()\n\n        base_query = \"\"\"\n            SELECT \n                c.id AS chunk_id,\n                c.document_id,\n                c.content,\n                1 - (c.embedding &lt;=&gt; $1::vector) AS similarity,\n                c.metadata,\n                d.title AS document_title,\n                d.source AS document_source\n            FROM chunks c\n            JOIN documents d ON c.document_id = d.id\n            WHERE c.embedding IS NOT NULL\n        \"\"\"\n\n        if source_filter:\n            sql_query = base_query + \" AND d.source ILIKE $3 ORDER BY c.embedding &lt;=&gt; $1::vector LIMIT $2\"\n            source_pattern = f\"%{source_filter}%\"\n            args = [embedding_str, limit, source_pattern]\n        else:\n            sql_query = base_query + \" ORDER BY c.embedding &lt;=&gt; $1::vector LIMIT $2\"\n            args = [embedding_str, limit]\n\n        async with global_db_pool.acquire() as conn:\n            results = await conn.fetch(sql_query, *args)\n\n        timing['db_ms'] = (time.time() - db_start) * 1000\n        timing['total_ms'] = (time.time() - start_time) * 1000\n\n        # Format results\n        structured_results = []\n        for row in results:\n            structured_results.append({\n                \"content\": row['content'],\n                \"similarity\": float(row['similarity']),\n                \"source\": row['document_source'],\n                \"title\": row['document_title'],\n                \"metadata\": json.loads(row['metadata']) if isinstance(row['metadata'], str) else row['metadata']\n            })\n\n        return {\n            \"results\": structured_results,\n            \"timing\": timing\n        }\n\n    except Exception as e:\n        logger.error(f\"Structured search failed: {e}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api-reference/core/#agent","title":"Agent","text":""},{"location":"api-reference/core/#core.agent","title":"<code>core.agent</code>","text":""},{"location":"api-reference/core/#core.agent--pydanticai-agent-definition","title":"PydanticAI Agent Definition","text":"<p>Defines the agent and its tools, wrapping the core RAG service.</p>"},{"location":"api-reference/core/#core.agent.search_knowledge_base","title":"<code>search_knowledge_base(ctx: RunContext[None], query: str, limit: int = 5, source_filter: str | None = None) -&gt; str</code>  <code>async</code>","text":"<p>Search the knowledge base using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to find relevant information</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return (default: 5)</p> <code>5</code> <code>source_filter</code> <code>str | None</code> <p>Optional filter to search only in specific documentation sources.           Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"           If provided, only documents whose source path contains this string will be searched.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted search results with source citations</p> Source code in <code>core\\agent.py</code> <pre><code>async def search_knowledge_base(\n    ctx: RunContext[None], \n    query: str, \n    limit: int = 5,\n    source_filter: str | None = None\n) -&gt; str:\n    \"\"\"\n    Search the knowledge base using semantic similarity.\n\n    Args:\n        query: The search query to find relevant information\n        limit: Maximum number of results to return (default: 5)\n        source_filter: Optional filter to search only in specific documentation sources.\n                      Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"\n                      If provided, only documents whose source path contains this string will be searched.\n\n    Returns:\n        Formatted search results with source citations\n    \"\"\"\n    try:\n        # Call API\n        response = await client.search(query, limit, source_filter)\n        results = response.get(\"results\", [])\n\n        if not results:\n            filter_msg = f\" in '{source_filter}'\" if source_filter else \"\"\n            return f\"No relevant information found in the knowledge base{filter_msg} for your query.\"\n\n        # Format results for LLM consumption\n        response_parts = []\n        for row in results:\n            # Handle both dictionary access (API) and object access (if we were local)\n            # API returns dicts\n            title = row.get('title', 'Unknown')\n            content = row.get('content', '')\n\n            response_parts.append(\n                f\"[Source: {title}]\\n{content}\\n\"\n            )\n\n        return \"\\n---\\n\".join(response_parts)\n\n    except Exception as e:\n        logger.error(f\"Search failed: {e}\")\n        return f\"Error searching knowledge base: {str(e)}\"\n</code></pre>"},{"location":"api-reference/ingestion/","title":"Ingestion Module","text":""},{"location":"api-reference/ingestion/#ingest-pipeline","title":"Ingest Pipeline","text":""},{"location":"api-reference/ingestion/#ingestion.ingest","title":"<code>ingestion.ingest</code>","text":"<p>Main ingestion script for processing markdown documents into vector DB and knowledge graph.</p>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline","title":"<code>DocumentIngestionPipeline</code>","text":"<p>Pipeline for ingesting documents into vector DB and knowledge graph.</p> Source code in <code>ingestion\\ingest.py</code> <pre><code>class DocumentIngestionPipeline:\n    \"\"\"Pipeline for ingesting documents into vector DB and knowledge graph.\"\"\"\n\n    def __init__(\n        self,\n        config: IngestionConfig,\n        documents_folder: str = \"documents\",\n        clean_before_ingest: bool = True,\n        fast_mode: bool = False\n    ):\n        \"\"\"\n        Initialize ingestion pipeline.\n\n        Args:\n            config: Ingestion configuration\n            documents_folder: Folder containing markdown documents\n            clean_before_ingest: Whether to clean existing data before ingestion (default: True)\n            fast_mode: Whether to use fast mode (disable OCR and table structure)\n        \"\"\"\n        self.config = config\n        self.documents_folder = documents_folder\n        self.clean_before_ingest = clean_before_ingest\n        self.fast_mode = fast_mode\n\n        # Initialize components\n        self.chunker_config = ChunkingConfig(\n            chunk_size=config.chunk_size,\n            chunk_overlap=config.chunk_overlap,\n            max_chunk_size=config.max_chunk_size,\n            use_semantic_splitting=config.use_semantic_chunking\n        )\n\n        self.chunker = create_chunker(self.chunker_config)\n        self.embedder = create_embedder()\n\n        self._initialized = False\n\n    async def initialize(self):\n        \"\"\"Initialize database connections.\"\"\"\n        if self._initialized:\n            return\n\n        logger.info(\"Initializing ingestion pipeline...\")\n\n        # Initialize database connections\n        await initialize_database()\n\n        self._initialized = True\n        logger.info(\"Ingestion pipeline initialized\")\n\n    async def close(self):\n        \"\"\"Close database connections.\"\"\"\n        if self._initialized:\n            await close_database()\n            self._initialized = False\n\n    async def ingest_documents(\n        self,\n        progress_callback: Optional[callable] = None\n    ) -&gt; List[IngestionResult]:\n        \"\"\"\n        Ingest all documents from the documents folder.\n\n        Args:\n            progress_callback: Optional callback for progress updates\n\n        Returns:\n            List of ingestion results\n        \"\"\"\n        if not self._initialized:\n            await self.initialize()\n\n        # Clean existing data if requested\n        if self.clean_before_ingest:\n            await self._clean_databases()\n\n        # Find all supported document files\n        document_files = self._find_document_files()\n\n        if not document_files:\n            logger.warning(f\"No supported document files found in {self.documents_folder}\")\n            return []\n\n        logger.info(f\"Found {len(document_files)} document files to process\")\n\n        results = []\n\n        for i, file_path in enumerate(document_files):\n            try:\n                logger.info(f\"Processing file {i+1}/{len(document_files)}: {file_path}\")\n\n                result = await self._ingest_single_document(file_path)\n                results.append(result)\n\n                if progress_callback:\n                    progress_callback(i + 1, len(document_files))\n\n            except Exception as e:\n                logger.error(f\"Failed to process {file_path}: {e}\")\n                results.append(IngestionResult(\n                    document_id=\"\",\n                    title=os.path.basename(file_path),\n                    chunks_created=0,\n                    entities_extracted=0,\n                    relationships_created=0,\n                    processing_time_ms=0,\n                    errors=[str(e)]\n                ))\n\n        # Log summary\n        total_chunks = sum(r.chunks_created for r in results)\n        total_errors = sum(len(r.errors) for r in results)\n\n        logger.info(f\"Ingestion complete: {len(results)} documents, {total_chunks} chunks, {total_errors} errors\")\n\n        return results\n\n    async def _ingest_single_document(self, file_path: str) -&gt; IngestionResult:\n        \"\"\"\n        Ingest a single document.\n\n        Args:\n            file_path: Path to the document file\n\n        Returns:\n            Ingestion result\n        \"\"\"\n        start_time = datetime.now()\n\n        # Read document (returns tuple: content, docling_doc)\n        document_content, docling_doc = self._read_document(file_path)\n        document_title = self._extract_title(document_content, file_path)\n        document_source = os.path.relpath(file_path, self.documents_folder)\n\n        # Extract metadata from content\n        document_metadata = self._extract_document_metadata(document_content, file_path)\n\n        logger.info(f\"Processing document: {document_title}\")\n\n        # Chunk the document - pass DoclingDocument for HybridChunker\n        chunks = await self.chunker.chunk_document(\n            content=document_content,\n            title=document_title,\n            source=document_source,\n            metadata=document_metadata,\n            docling_doc=docling_doc  # Pass DoclingDocument for HybridChunker\n        )\n\n        if not chunks:\n            logger.warning(f\"No chunks created for {document_title}\")\n            return IngestionResult(\n                document_id=\"\",\n                title=document_title,\n                chunks_created=0,\n                entities_extracted=0,\n                relationships_created=0,\n                processing_time_ms=(datetime.now() - start_time).total_seconds() * 1000,\n                errors=[\"No chunks created\"]\n            )\n\n        logger.info(f\"Created {len(chunks)} chunks\")\n\n        # Entity extraction removed (graph-related functionality)\n        entities_extracted = 0\n\n        # Generate embeddings\n        embedded_chunks = await self.embedder.embed_chunks(chunks)\n        logger.info(f\"Generated embeddings for {len(embedded_chunks)} chunks\")\n\n        # Save to PostgreSQL\n        document_id = await self._save_to_postgres(\n            document_title,\n            document_source,\n            document_content,\n            embedded_chunks,\n            document_metadata\n        )\n\n        logger.info(f\"Saved document to PostgreSQL with ID: {document_id}\")\n\n        # Knowledge graph functionality removed\n        relationships_created = 0\n        graph_errors = []\n\n        # Calculate processing time\n        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n\n        return IngestionResult(\n            document_id=document_id,\n            title=document_title,\n            chunks_created=len(chunks),\n            entities_extracted=entities_extracted,\n            relationships_created=relationships_created,\n            processing_time_ms=processing_time,\n            errors=graph_errors\n        )\n\n    def _find_document_files(self) -&gt; List[str]:\n        \"\"\"Find all supported document files in the documents folder.\"\"\"\n        if not os.path.exists(self.documents_folder):\n            logger.error(f\"Documents folder not found: {self.documents_folder}\")\n            return []\n\n        # Supported file patterns - Docling + text formats\n        patterns = [\n            \"*.md\", \"*.markdown\", \"*.txt\",  # Text formats\n            \"*.pdf\",  # PDF\n            \"*.docx\", \"*.doc\",  # Word\n            \"*.pptx\", \"*.ppt\",  # PowerPoint\n            \"*.xlsx\", \"*.xls\",  # Excel\n            \"*.html\", \"*.htm\",  # HTML\n        ]\n        files = []\n\n        for pattern in patterns:\n            files.extend(glob.glob(os.path.join(self.documents_folder, \"**\", pattern), recursive=True))\n\n        return sorted(files)\n\n    def _read_document(self, file_path: str) -&gt; tuple[str, Optional[Any]]:\n        \"\"\"\n        Read document content from file - supports multiple formats via Docling.\n\n        Returns:\n            Tuple of (markdown_content, docling_document)\n            docling_document is None for text files\n        \"\"\"\n        file_ext = os.path.splitext(file_path)[1].lower()\n\n        # Docling-supported formats (convert to markdown)\n        docling_formats = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', '.html', '.htm']\n\n        if file_ext in docling_formats:\n            try:\n                from docling.document_converter import DocumentConverter, PdfFormatOption\n                from docling.datamodel.pipeline_options import PdfPipelineOptions\n                from docling.datamodel.base_models import InputFormat\n\n                logger.info(f\"Converting {file_ext} file using Docling: {os.path.basename(file_path)}\")\n\n                # Configure pipeline options\n                pipeline_options = PdfPipelineOptions()\n\n                # Configure options for PDF files if in fast mode\n                if self.fast_mode and file_ext == '.pdf':\n                    logger.info(f\"Fast mode enabled for {os.path.basename(file_path)}: Disabling OCR and heavy layout analysis\")\n                    pipeline_options.do_ocr = False\n                    pipeline_options.do_table_structure = False\n                    # Disable picture classification/description for speed\n                    if hasattr(pipeline_options, 'do_picture_classification'):\n                        pipeline_options.do_picture_classification = False\n                    if hasattr(pipeline_options, 'do_picture_description'):\n                        pipeline_options.do_picture_description = False\n\n                # Configure converter with options\n                converter = DocumentConverter(\n                    format_options={\n                        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n                    }\n                )\n\n                result = converter.convert(file_path)\n\n                # Export to markdown for consistent processing\n                markdown_content = result.document.export_to_markdown()\n                logger.info(f\"Successfully converted {os.path.basename(file_path)} to markdown\")\n\n                # Return both markdown and DoclingDocument for HybridChunker\n                return (markdown_content, result.document)\n\n            except Exception as e:\n                logger.error(f\"Failed to convert {file_path} with Docling: {e}\")\n                # Fall back to raw text if Docling fails\n                logger.warning(f\"Falling back to raw text extraction for {file_path}\")\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        return (f.read(), None)\n                except:\n                    return (f\"[Error: Could not read file {os.path.basename(file_path)}]\", None)\n\n        # Text-based formats (read directly)\n        else:\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return (f.read(), None)\n            except UnicodeDecodeError:\n                # Try with different encoding\n                with open(file_path, 'r', encoding='latin-1') as f:\n                    return (f.read(), None)\n\n    def _extract_title(self, content: str, file_path: str) -&gt; str:\n        \"\"\"Extract title from document content or filename.\"\"\"\n        # Try to find markdown title\n        lines = content.split('\\n')\n        for line in lines[:10]:  # Check first 10 lines\n            line = line.strip()\n            if line.startswith('# '):\n                return line[2:].strip()\n\n        # Fallback to filename\n        return os.path.splitext(os.path.basename(file_path))[0]\n\n    def _extract_document_metadata(self, content: str, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata from document content.\"\"\"\n        metadata = {\n            \"file_path\": file_path,\n            \"file_size\": len(content),\n            \"ingestion_date\": datetime.now().isoformat()\n        }\n\n        # Try to extract YAML frontmatter\n        if content.startswith('---'):\n            try:\n                import yaml\n                end_marker = content.find('\\n---\\n', 4)\n                if end_marker != -1:\n                    frontmatter = content[4:end_marker]\n                    yaml_metadata = yaml.safe_load(frontmatter)\n                    if isinstance(yaml_metadata, dict):\n                        metadata.update(yaml_metadata)\n            except ImportError:\n                logger.warning(\"PyYAML not installed, skipping frontmatter extraction\")\n            except Exception as e:\n                logger.warning(f\"Failed to parse frontmatter: {e}\")\n\n        # Extract some basic metadata from content\n        lines = content.split('\\n')\n        metadata['line_count'] = len(lines)\n        metadata['word_count'] = len(content.split())\n\n        return metadata\n\n    async def _save_to_postgres(\n        self,\n        title: str,\n        source: str,\n        content: str,\n        chunks: List[DocumentChunk],\n        metadata: Dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Save document and chunks to PostgreSQL.\"\"\"\n        async with db_pool.acquire() as conn:\n            async with conn.transaction():\n                # Insert document\n                document_result = await conn.fetchrow(\n                    \"\"\"\n                    INSERT INTO documents (title, source, content, metadata)\n                    VALUES ($1, $2, $3, $4)\n                    RETURNING id::text\n                    \"\"\",\n                    title,\n                    source,\n                    content,\n                    json.dumps(metadata)\n                )\n\n                document_id = document_result[\"id\"]\n\n                # Insert chunks\n                for chunk in chunks:\n                    # Convert embedding to PostgreSQL vector string format\n                    embedding_data = None\n                    if hasattr(chunk, 'embedding') and chunk.embedding:\n                        # PostgreSQL vector format: '[1.0,2.0,3.0]' (no spaces after commas)\n                        embedding_data = '[' + ','.join(map(str, chunk.embedding)) + ']'\n\n                    await conn.execute(\n                        \"\"\"\n                        INSERT INTO chunks (document_id, content, embedding, chunk_index, metadata, token_count)\n                        VALUES ($1::uuid, $2, $3::vector, $4, $5, $6)\n                        \"\"\",\n                        document_id,\n                        chunk.content,\n                        embedding_data,\n                        chunk.index,\n                        json.dumps(chunk.metadata),\n                        chunk.token_count\n                    )\n\n                return document_id\n\n    async def _clean_databases(self):\n        \"\"\"Clean existing data from databases.\"\"\"\n        logger.warning(\"Cleaning existing data from databases...\")\n\n        # Clean PostgreSQL\n        async with db_pool.acquire() as conn:\n            async with conn.transaction():\n                await conn.execute(\"DELETE FROM chunks\")\n                await conn.execute(\"DELETE FROM documents\")\n\n        logger.info(\"Cleaned PostgreSQL database\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.__init__","title":"<code>__init__(config: IngestionConfig, documents_folder: str = 'documents', clean_before_ingest: bool = True, fast_mode: bool = False)</code>","text":"<p>Initialize ingestion pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>IngestionConfig</code> <p>Ingestion configuration</p> required <code>documents_folder</code> <code>str</code> <p>Folder containing markdown documents</p> <code>'documents'</code> <code>clean_before_ingest</code> <code>bool</code> <p>Whether to clean existing data before ingestion (default: True)</p> <code>True</code> <code>fast_mode</code> <code>bool</code> <p>Whether to use fast mode (disable OCR and table structure)</p> <code>False</code> Source code in <code>ingestion\\ingest.py</code> <pre><code>def __init__(\n    self,\n    config: IngestionConfig,\n    documents_folder: str = \"documents\",\n    clean_before_ingest: bool = True,\n    fast_mode: bool = False\n):\n    \"\"\"\n    Initialize ingestion pipeline.\n\n    Args:\n        config: Ingestion configuration\n        documents_folder: Folder containing markdown documents\n        clean_before_ingest: Whether to clean existing data before ingestion (default: True)\n        fast_mode: Whether to use fast mode (disable OCR and table structure)\n    \"\"\"\n    self.config = config\n    self.documents_folder = documents_folder\n    self.clean_before_ingest = clean_before_ingest\n    self.fast_mode = fast_mode\n\n    # Initialize components\n    self.chunker_config = ChunkingConfig(\n        chunk_size=config.chunk_size,\n        chunk_overlap=config.chunk_overlap,\n        max_chunk_size=config.max_chunk_size,\n        use_semantic_splitting=config.use_semantic_chunking\n    )\n\n    self.chunker = create_chunker(self.chunker_config)\n    self.embedder = create_embedder()\n\n    self._initialized = False\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close database connections.</p> Source code in <code>ingestion\\ingest.py</code> <pre><code>async def close(self):\n    \"\"\"Close database connections.\"\"\"\n    if self._initialized:\n        await close_database()\n        self._initialized = False\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.ingest_documents","title":"<code>ingest_documents(progress_callback: Optional[callable] = None) -&gt; List[IngestionResult]</code>  <code>async</code>","text":"<p>Ingest all documents from the documents folder.</p> <p>Parameters:</p> Name Type Description Default <code>progress_callback</code> <code>Optional[callable]</code> <p>Optional callback for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>List[IngestionResult]</code> <p>List of ingestion results</p> Source code in <code>ingestion\\ingest.py</code> <pre><code>async def ingest_documents(\n    self,\n    progress_callback: Optional[callable] = None\n) -&gt; List[IngestionResult]:\n    \"\"\"\n    Ingest all documents from the documents folder.\n\n    Args:\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        List of ingestion results\n    \"\"\"\n    if not self._initialized:\n        await self.initialize()\n\n    # Clean existing data if requested\n    if self.clean_before_ingest:\n        await self._clean_databases()\n\n    # Find all supported document files\n    document_files = self._find_document_files()\n\n    if not document_files:\n        logger.warning(f\"No supported document files found in {self.documents_folder}\")\n        return []\n\n    logger.info(f\"Found {len(document_files)} document files to process\")\n\n    results = []\n\n    for i, file_path in enumerate(document_files):\n        try:\n            logger.info(f\"Processing file {i+1}/{len(document_files)}: {file_path}\")\n\n            result = await self._ingest_single_document(file_path)\n            results.append(result)\n\n            if progress_callback:\n                progress_callback(i + 1, len(document_files))\n\n        except Exception as e:\n            logger.error(f\"Failed to process {file_path}: {e}\")\n            results.append(IngestionResult(\n                document_id=\"\",\n                title=os.path.basename(file_path),\n                chunks_created=0,\n                entities_extracted=0,\n                relationships_created=0,\n                processing_time_ms=0,\n                errors=[str(e)]\n            ))\n\n    # Log summary\n    total_chunks = sum(r.chunks_created for r in results)\n    total_errors = sum(len(r.errors) for r in results)\n\n    logger.info(f\"Ingestion complete: {len(results)} documents, {total_chunks} chunks, {total_errors} errors\")\n\n    return results\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize database connections.</p> Source code in <code>ingestion\\ingest.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize database connections.\"\"\"\n    if self._initialized:\n        return\n\n    logger.info(\"Initializing ingestion pipeline...\")\n\n    # Initialize database connections\n    await initialize_database()\n\n    self._initialized = True\n    logger.info(\"Ingestion pipeline initialized\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.main","title":"<code>main()</code>  <code>async</code>","text":"<p>Main function for running ingestion.</p> Source code in <code>ingestion\\ingest.py</code> <pre><code>async def main():\n    \"\"\"Main function for running ingestion.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Ingest documents into vector DB\")\n    parser.add_argument(\"--documents\", \"-d\", default=\"documents\", help=\"Documents folder path\")\n    parser.add_argument(\"--no-clean\", action=\"store_true\", help=\"Skip cleaning existing data before ingestion (default: cleans automatically)\")\n    parser.add_argument(\"--chunk-size\", type=int, default=1000, help=\"Chunk size for splitting documents\")\n    parser.add_argument(\"--chunk-overlap\", type=int, default=200, help=\"Chunk overlap size\")\n    parser.add_argument(\"--no-semantic\", action=\"store_true\", help=\"Disable semantic chunking\")\n    parser.add_argument(\"--fast\", action=\"store_true\", help=\"Enable fast mode (disable OCR and table structure recognition)\")\n    # Graph-related arguments removed\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose logging\")\n\n    args = parser.parse_args()\n\n    # Configure logging\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n\n    # Create ingestion configuration\n    config = IngestionConfig(\n        chunk_size=args.chunk_size,\n        chunk_overlap=args.chunk_overlap,\n        use_semantic_chunking=not args.no_semantic\n    )\n\n    # Create and run pipeline - clean by default unless --no-clean is specified\n    pipeline = DocumentIngestionPipeline(\n        config=config,\n        documents_folder=args.documents,\n        clean_before_ingest=not args.no_clean,  # Clean by default\n        fast_mode=args.fast\n    )\n\n    def progress_callback(current: int, total: int):\n        print(f\"Progress: {current}/{total} documents processed\")\n\n    try:\n        start_time = datetime.now()\n\n        results = await pipeline.ingest_documents(progress_callback)\n\n        end_time = datetime.now()\n        total_time = (end_time - start_time).total_seconds()\n\n        # Print summary\n        print(\"\\n\" + \"=\"*50)\n        print(\"INGESTION SUMMARY\")\n        print(\"=\"*50)\n        print(f\"Documents processed: {len(results)}\")\n        print(f\"Total chunks created: {sum(r.chunks_created for r in results)}\")\n        # Graph-related stats removed\n        print(f\"Total errors: {sum(len(r.errors) for r in results)}\")\n        print(f\"Total processing time: {total_time:.2f} seconds\")\n        print()\n\n        # Print individual results\n        for result in results:\n            status = \"\u2713\" if not result.errors else \"\u2717\"\n            print(f\"{status} {result.title}: {result.chunks_created} chunks\")\n\n            if result.errors:\n                for error in result.errors:\n                    print(f\"  Error: {error}\")\n\n    except KeyboardInterrupt:\n        print(\"\\nIngestion interrupted by user\")\n    except Exception as e:\n        logger.error(f\"Ingestion failed: {e}\")\n        raise\n    finally:\n        await pipeline.close()\n</code></pre>"},{"location":"api-reference/ingestion/#chunker","title":"Chunker","text":""},{"location":"api-reference/ingestion/#ingestion.chunker","title":"<code>ingestion.chunker</code>","text":"<p>Docling HybridChunker implementation for intelligent document splitting.</p> <p>This module uses Docling's built-in HybridChunker which combines: - Token-aware chunking (uses actual tokenizer) - Document structure preservation (headings, sections, tables) - Semantic boundary respect (paragraphs, code blocks) - Contextualized output (chunks include heading hierarchy)</p> <p>Benefits over custom chunking: - Fast (no LLM API calls) - Token-precise (not character-based estimates) - Better for RAG (chunks include document context) - Battle-tested (maintained by Docling team)</p>"},{"location":"api-reference/ingestion/#ingestion.chunker.ChunkingConfig","title":"<code>ChunkingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for chunking.</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>@dataclass\nclass ChunkingConfig:\n    \"\"\"Configuration for chunking.\"\"\"\n    chunk_size: int = 1000  # Target characters per chunk\n    chunk_overlap: int = 200  # Character overlap between chunks\n    max_chunk_size: int = 2000  # Maximum chunk size\n    min_chunk_size: int = 100  # Minimum chunk size\n    use_semantic_splitting: bool = True  # Use HybridChunker (recommended)\n    preserve_structure: bool = True  # Preserve document structure\n    max_tokens: int = 512  # Maximum tokens for embedding models\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        if self.chunk_overlap &gt;= self.chunk_size:\n            raise ValueError(\"Chunk overlap must be less than chunk size\")\n        if self.min_chunk_size &lt;= 0:\n            raise ValueError(\"Minimum chunk size must be positive\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.ChunkingConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration.</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration.\"\"\"\n    if self.chunk_overlap &gt;= self.chunk_size:\n        raise ValueError(\"Chunk overlap must be less than chunk size\")\n    if self.min_chunk_size &lt;= 0:\n        raise ValueError(\"Minimum chunk size must be positive\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker","title":"<code>DoclingHybridChunker</code>","text":"<p>Docling HybridChunker wrapper for intelligent document splitting.</p> <p>This chunker uses Docling's built-in HybridChunker which: - Respects document structure (sections, paragraphs, tables) - Is token-aware (fits embedding model limits) - Preserves semantic coherence - Includes heading context in chunks</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>class DoclingHybridChunker:\n    \"\"\"\n    Docling HybridChunker wrapper for intelligent document splitting.\n\n    This chunker uses Docling's built-in HybridChunker which:\n    - Respects document structure (sections, paragraphs, tables)\n    - Is token-aware (fits embedding model limits)\n    - Preserves semantic coherence\n    - Includes heading context in chunks\n    \"\"\"\n\n    def __init__(self, config: ChunkingConfig):\n        \"\"\"\n        Initialize chunker.\n\n        Args:\n            config: Chunking configuration\n        \"\"\"\n        self.config = config\n\n        # Initialize tokenizer for token-aware chunking\n        model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n        logger.info(f\"Initializing tokenizer: {model_id}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        # Create HybridChunker\n        self.chunker = HybridChunker(\n            tokenizer=self.tokenizer,\n            max_tokens=config.max_tokens,\n            merge_peers=True  # Merge small adjacent chunks\n        )\n\n        logger.info(f\"HybridChunker initialized (max_tokens={config.max_tokens})\")\n\n    async def chunk_document(\n        self,\n        content: str,\n        title: str,\n        source: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        docling_doc: Optional[DoclingDocument] = None\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Chunk a document using Docling's HybridChunker.\n\n        Args:\n            content: Document content (markdown format)\n            title: Document title\n            source: Document source\n            metadata: Additional metadata\n            docling_doc: Optional pre-converted DoclingDocument (for efficiency)\n\n        Returns:\n            List of document chunks with contextualized content\n        \"\"\"\n        if not content.strip():\n            return []\n\n        base_metadata = {\n            \"title\": title,\n            \"source\": source,\n            \"chunk_method\": \"hybrid\",\n            **(metadata or {})\n        }\n\n        # If we don't have a DoclingDocument, we need to create one from markdown\n        if docling_doc is None:\n            # For markdown content, we need to convert it to DoclingDocument\n            # This is a simplified version - in practice, content comes from\n            # Docling's document converter in the ingestion pipeline\n            logger.warning(\"No DoclingDocument provided, using simple chunking fallback\")\n            return self._simple_fallback_chunk(content, base_metadata)\n\n        try:\n            # Use HybridChunker to chunk the DoclingDocument\n            chunk_iter = self.chunker.chunk(dl_doc=docling_doc)\n            chunks = list(chunk_iter)\n\n            # Convert Docling chunks to DocumentChunk objects\n            document_chunks = []\n            current_pos = 0\n\n            for i, chunk in enumerate(chunks):\n                # Get contextualized text (includes heading hierarchy)\n                contextualized_text = self.chunker.contextualize(chunk=chunk)\n\n                # Count actual tokens\n                token_count = len(self.tokenizer.encode(contextualized_text))\n\n                # Create chunk metadata\n                chunk_metadata = {\n                    **base_metadata,\n                    \"total_chunks\": len(chunks),\n                    \"token_count\": token_count,\n                    \"has_context\": True  # Flag indicating contextualized chunk\n                }\n\n                # Estimate character positions\n                start_char = current_pos\n                end_char = start_char + len(contextualized_text)\n\n                document_chunks.append(DocumentChunk(\n                    content=contextualized_text.strip(),\n                    index=i,\n                    start_char=start_char,\n                    end_char=end_char,\n                    metadata=chunk_metadata,\n                    token_count=token_count\n                ))\n\n                current_pos = end_char\n\n            logger.info(f\"Created {len(document_chunks)} chunks using HybridChunker\")\n            return document_chunks\n\n        except Exception as e:\n            logger.error(f\"HybridChunker failed: {e}, falling back to simple chunking\")\n            return self._simple_fallback_chunk(content, base_metadata)\n\n    def _simple_fallback_chunk(\n        self,\n        content: str,\n        base_metadata: Dict[str, Any]\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Simple fallback chunking when HybridChunker can't be used.\n\n        This is used when:\n        - No DoclingDocument is provided\n        - HybridChunker fails\n\n        Args:\n            content: Content to chunk\n            base_metadata: Base metadata for chunks\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        chunks = []\n        chunk_size = self.config.chunk_size\n        overlap = self.config.chunk_overlap\n\n        # Simple sliding window approach\n        start = 0\n        chunk_index = 0\n\n        while start &lt; len(content):\n            end = start + chunk_size\n\n            if end &gt;= len(content):\n                # Last chunk\n                chunk_text = content[start:]\n            else:\n                # Try to end at sentence boundary\n                chunk_end = end\n                for i in range(end, max(start + self.config.min_chunk_size, end - 200), -1):\n                    if i &lt; len(content) and content[i] in '.!?\\n':\n                        chunk_end = i + 1\n                        break\n                chunk_text = content[start:chunk_end]\n                end = chunk_end\n\n            if chunk_text.strip():\n                token_count = len(self.tokenizer.encode(chunk_text))\n\n                chunks.append(DocumentChunk(\n                    content=chunk_text.strip(),\n                    index=chunk_index,\n                    start_char=start,\n                    end_char=end,\n                    metadata={\n                        **base_metadata,\n                        \"chunk_method\": \"simple_fallback\",\n                        \"total_chunks\": -1  # Will update after\n                    },\n                    token_count=token_count\n                ))\n\n                chunk_index += 1\n\n            # Move forward with overlap\n            start = end - overlap\n\n        # Update total chunks\n        for chunk in chunks:\n            chunk.metadata[\"total_chunks\"] = len(chunks)\n\n        logger.info(f\"Created {len(chunks)} chunks using simple fallback\")\n        return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker.__init__","title":"<code>__init__(config: ChunkingConfig)</code>","text":"<p>Initialize chunker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChunkingConfig</code> <p>Chunking configuration</p> required Source code in <code>ingestion\\chunker.py</code> <pre><code>def __init__(self, config: ChunkingConfig):\n    \"\"\"\n    Initialize chunker.\n\n    Args:\n        config: Chunking configuration\n    \"\"\"\n    self.config = config\n\n    # Initialize tokenizer for token-aware chunking\n    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n    logger.info(f\"Initializing tokenizer: {model_id}\")\n    self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    # Create HybridChunker\n    self.chunker = HybridChunker(\n        tokenizer=self.tokenizer,\n        max_tokens=config.max_tokens,\n        merge_peers=True  # Merge small adjacent chunks\n    )\n\n    logger.info(f\"HybridChunker initialized (max_tokens={config.max_tokens})\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker.chunk_document","title":"<code>chunk_document(content: str, title: str, source: str, metadata: Optional[Dict[str, Any]] = None, docling_doc: Optional[DoclingDocument] = None) -&gt; List[DocumentChunk]</code>  <code>async</code>","text":"<p>Chunk a document using Docling's HybridChunker.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Document content (markdown format)</p> required <code>title</code> <code>str</code> <p>Document title</p> required <code>source</code> <code>str</code> <p>Document source</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata</p> <code>None</code> <code>docling_doc</code> <code>Optional[DoclingDocument]</code> <p>Optional pre-converted DoclingDocument (for efficiency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DocumentChunk]</code> <p>List of document chunks with contextualized content</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>async def chunk_document(\n    self,\n    content: str,\n    title: str,\n    source: str,\n    metadata: Optional[Dict[str, Any]] = None,\n    docling_doc: Optional[DoclingDocument] = None\n) -&gt; List[DocumentChunk]:\n    \"\"\"\n    Chunk a document using Docling's HybridChunker.\n\n    Args:\n        content: Document content (markdown format)\n        title: Document title\n        source: Document source\n        metadata: Additional metadata\n        docling_doc: Optional pre-converted DoclingDocument (for efficiency)\n\n    Returns:\n        List of document chunks with contextualized content\n    \"\"\"\n    if not content.strip():\n        return []\n\n    base_metadata = {\n        \"title\": title,\n        \"source\": source,\n        \"chunk_method\": \"hybrid\",\n        **(metadata or {})\n    }\n\n    # If we don't have a DoclingDocument, we need to create one from markdown\n    if docling_doc is None:\n        # For markdown content, we need to convert it to DoclingDocument\n        # This is a simplified version - in practice, content comes from\n        # Docling's document converter in the ingestion pipeline\n        logger.warning(\"No DoclingDocument provided, using simple chunking fallback\")\n        return self._simple_fallback_chunk(content, base_metadata)\n\n    try:\n        # Use HybridChunker to chunk the DoclingDocument\n        chunk_iter = self.chunker.chunk(dl_doc=docling_doc)\n        chunks = list(chunk_iter)\n\n        # Convert Docling chunks to DocumentChunk objects\n        document_chunks = []\n        current_pos = 0\n\n        for i, chunk in enumerate(chunks):\n            # Get contextualized text (includes heading hierarchy)\n            contextualized_text = self.chunker.contextualize(chunk=chunk)\n\n            # Count actual tokens\n            token_count = len(self.tokenizer.encode(contextualized_text))\n\n            # Create chunk metadata\n            chunk_metadata = {\n                **base_metadata,\n                \"total_chunks\": len(chunks),\n                \"token_count\": token_count,\n                \"has_context\": True  # Flag indicating contextualized chunk\n            }\n\n            # Estimate character positions\n            start_char = current_pos\n            end_char = start_char + len(contextualized_text)\n\n            document_chunks.append(DocumentChunk(\n                content=contextualized_text.strip(),\n                index=i,\n                start_char=start_char,\n                end_char=end_char,\n                metadata=chunk_metadata,\n                token_count=token_count\n            ))\n\n            current_pos = end_char\n\n        logger.info(f\"Created {len(document_chunks)} chunks using HybridChunker\")\n        return document_chunks\n\n    except Exception as e:\n        logger.error(f\"HybridChunker failed: {e}, falling back to simple chunking\")\n        return self._simple_fallback_chunk(content, base_metadata)\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DocumentChunk","title":"<code>DocumentChunk</code>  <code>dataclass</code>","text":"<p>Represents a document chunk with optional embedding.</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>@dataclass\nclass DocumentChunk:\n    \"\"\"Represents a document chunk with optional embedding.\"\"\"\n    content: str\n    index: int\n    start_char: int\n    end_char: int\n    metadata: Dict[str, Any]\n    token_count: Optional[int] = None\n    embedding: Optional[List[float]] = None  # For embedder compatibility\n\n    def __post_init__(self):\n        \"\"\"Calculate token count if not provided.\"\"\"\n        if self.token_count is None:\n            # Rough estimation: ~4 characters per token\n            self.token_count = len(self.content) // 4\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DocumentChunk.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Calculate token count if not provided.</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Calculate token count if not provided.\"\"\"\n    if self.token_count is None:\n        # Rough estimation: ~4 characters per token\n        self.token_count = len(self.content) // 4\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker","title":"<code>SimpleChunker</code>","text":"<p>Simple non-semantic chunker for faster processing without Docling.</p> <p>This is kept as a lightweight alternative when: - Speed is critical - Document structure is simple - Token precision is not required</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>class SimpleChunker:\n    \"\"\"\n    Simple non-semantic chunker for faster processing without Docling.\n\n    This is kept as a lightweight alternative when:\n    - Speed is critical\n    - Document structure is simple\n    - Token precision is not required\n    \"\"\"\n\n    def __init__(self, config: ChunkingConfig):\n        \"\"\"Initialize simple chunker.\"\"\"\n        self.config = config\n\n    async def chunk_document(\n        self,\n        content: str,\n        title: str,\n        source: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        **kwargs  # Ignore extra args like docling_doc\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Chunk document using simple paragraph-based rules.\n\n        Args:\n            content: Document content\n            title: Document title\n            source: Document source\n            metadata: Additional metadata\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        if not content.strip():\n            return []\n\n        base_metadata = {\n            \"title\": title,\n            \"source\": source,\n            \"chunk_method\": \"simple\",\n            **(metadata or {})\n        }\n\n        # Split on double newlines (paragraphs)\n        import re\n        paragraphs = re.split(r'\\n\\s*\\n', content)\n\n        chunks = []\n        current_chunk = \"\"\n        current_pos = 0\n        chunk_index = 0\n\n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n\n            # Check if adding this paragraph exceeds chunk size\n            potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n\n            if len(potential_chunk) &lt;= self.config.chunk_size:\n                current_chunk = potential_chunk\n            else:\n                # Save current chunk if it exists\n                if current_chunk:\n                    chunks.append(self._create_chunk(\n                        current_chunk,\n                        chunk_index,\n                        current_pos,\n                        current_pos + len(current_chunk),\n                        base_metadata.copy()\n                    ))\n\n                    current_pos += len(current_chunk)\n                    chunk_index += 1\n\n                # Start new chunk with current paragraph\n                current_chunk = paragraph\n\n        # Add final chunk\n        if current_chunk:\n            chunks.append(self._create_chunk(\n                current_chunk,\n                chunk_index,\n                current_pos,\n                current_pos + len(current_chunk),\n                base_metadata.copy()\n            ))\n\n        # Update total chunks in metadata\n        for chunk in chunks:\n            chunk.metadata[\"total_chunks\"] = len(chunks)\n\n        return chunks\n\n    def _create_chunk(\n        self,\n        content: str,\n        index: int,\n        start_pos: int,\n        end_pos: int,\n        metadata: Dict[str, Any]\n    ) -&gt; DocumentChunk:\n        \"\"\"Create a DocumentChunk object.\"\"\"\n        return DocumentChunk(\n            content=content.strip(),\n            index=index,\n            start_char=start_pos,\n            end_char=end_pos,\n            metadata=metadata\n        )\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker.__init__","title":"<code>__init__(config: ChunkingConfig)</code>","text":"<p>Initialize simple chunker.</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>def __init__(self, config: ChunkingConfig):\n    \"\"\"Initialize simple chunker.\"\"\"\n    self.config = config\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker.chunk_document","title":"<code>chunk_document(content: str, title: str, source: str, metadata: Optional[Dict[str, Any]] = None, **kwargs) -&gt; List[DocumentChunk]</code>  <code>async</code>","text":"<p>Chunk document using simple paragraph-based rules.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Document content</p> required <code>title</code> <code>str</code> <p>Document title</p> required <code>source</code> <code>str</code> <p>Document source</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DocumentChunk]</code> <p>List of document chunks</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>async def chunk_document(\n    self,\n    content: str,\n    title: str,\n    source: str,\n    metadata: Optional[Dict[str, Any]] = None,\n    **kwargs  # Ignore extra args like docling_doc\n) -&gt; List[DocumentChunk]:\n    \"\"\"\n    Chunk document using simple paragraph-based rules.\n\n    Args:\n        content: Document content\n        title: Document title\n        source: Document source\n        metadata: Additional metadata\n\n    Returns:\n        List of document chunks\n    \"\"\"\n    if not content.strip():\n        return []\n\n    base_metadata = {\n        \"title\": title,\n        \"source\": source,\n        \"chunk_method\": \"simple\",\n        **(metadata or {})\n    }\n\n    # Split on double newlines (paragraphs)\n    import re\n    paragraphs = re.split(r'\\n\\s*\\n', content)\n\n    chunks = []\n    current_chunk = \"\"\n    current_pos = 0\n    chunk_index = 0\n\n    for paragraph in paragraphs:\n        paragraph = paragraph.strip()\n        if not paragraph:\n            continue\n\n        # Check if adding this paragraph exceeds chunk size\n        potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n\n        if len(potential_chunk) &lt;= self.config.chunk_size:\n            current_chunk = potential_chunk\n        else:\n            # Save current chunk if it exists\n            if current_chunk:\n                chunks.append(self._create_chunk(\n                    current_chunk,\n                    chunk_index,\n                    current_pos,\n                    current_pos + len(current_chunk),\n                    base_metadata.copy()\n                ))\n\n                current_pos += len(current_chunk)\n                chunk_index += 1\n\n            # Start new chunk with current paragraph\n            current_chunk = paragraph\n\n    # Add final chunk\n    if current_chunk:\n        chunks.append(self._create_chunk(\n            current_chunk,\n            chunk_index,\n            current_pos,\n            current_pos + len(current_chunk),\n            base_metadata.copy()\n        ))\n\n    # Update total chunks in metadata\n    for chunk in chunks:\n        chunk.metadata[\"total_chunks\"] = len(chunks)\n\n    return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.create_chunker","title":"<code>create_chunker(config: ChunkingConfig)</code>","text":"<p>Create appropriate chunker based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChunkingConfig</code> <p>Chunking configuration</p> required <p>Returns:</p> Type Description <p>Chunker instance</p> Source code in <code>ingestion\\chunker.py</code> <pre><code>def create_chunker(config: ChunkingConfig):\n    \"\"\"\n    Create appropriate chunker based on configuration.\n\n    Args:\n        config: Chunking configuration\n\n    Returns:\n        Chunker instance\n    \"\"\"\n    if config.use_semantic_splitting:\n        return DoclingHybridChunker(config)\n    else:\n        return SimpleChunker(config)\n</code></pre>"},{"location":"api-reference/ingestion/#embedder","title":"Embedder","text":""},{"location":"api-reference/ingestion/#ingestion.embedder","title":"<code>ingestion.embedder</code>","text":""},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder","title":"<code>BaseEmbedder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for embedding providers.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>class BaseEmbedder(ABC):\n    \"\"\"Abstract base class for embedding providers.\"\"\"\n\n    @abstractmethod\n    async def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query string.\"\"\"\n        pass\n\n    @abstractmethod\n    async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of texts.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder.embed_documents","title":"<code>embed_documents(texts: List[str]) -&gt; List[List[float]]</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Embed a list of texts.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>@abstractmethod\nasync def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Embed a list of texts.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder.embed_query","title":"<code>embed_query(text: str) -&gt; List[float]</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Embed a single query string.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>@abstractmethod\nasync def embed_query(self, text: str) -&gt; List[float]:\n    \"\"\"Embed a single query string.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingCache","title":"<code>EmbeddingCache</code>","text":"<p>Simple in-memory cache for embeddings.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>class EmbeddingCache:\n    \"\"\"Simple in-memory cache for embeddings.\"\"\"\n    def __init__(self, max_size: int = 1000):\n        self.cache: Dict[str, List[float]] = {}\n        self.max_size = max_size\n\n    def get(self, text: str) -&gt; Optional[List[float]]:\n        return self.cache.get(text)\n\n    def set(self, text: str, embedding: List[float]):\n        if len(self.cache) &gt;= self.max_size:\n            # Simple eviction: remove first key (FIFO-ish)\n            try:\n                first_key = next(iter(self.cache))\n                del self.cache[first_key]\n            except StopIteration:\n                pass\n        self.cache[text] = embedding\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator","title":"<code>EmbeddingGenerator</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Generates embeddings using OpenAI compatible API.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>class EmbeddingGenerator(BaseEmbedder):\n    \"\"\"Generates embeddings using OpenAI compatible API.\"\"\"\n\n    def __init__(\n        self, \n        model_name: str = \"text-embedding-3-small\",\n        batch_size: int = 100,\n        use_cache: bool = True,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None\n    ):\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.use_cache = use_cache\n\n        # Initialize OpenAI client\n        from openai import AsyncOpenAI\n\n        # Use provided config or fallback to env vars/provider config\n        provider_config = get_provider_config()\n        self.api_key = api_key or provider_config.api_key\n        self.base_url = base_url or provider_config.base_url\n\n        self.client = AsyncOpenAI(\n            api_key=self.api_key,\n            base_url=self.base_url\n        )\n\n        if self.use_cache:\n            self.cache = EmbeddingCache()\n        else:\n            self.cache = None\n\n        logger.info(f\"Initialized EmbeddingGenerator with model={self.model_name}\")\n\n    async def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query string.\"\"\"\n        # Check cache first\n        if self.use_cache and self.cache:\n            cached = self.cache.get(text)\n            if cached:\n                return cached\n\n        try:\n            embedding = await self._generate_single_embedding(text)\n\n            if self.use_cache and self.cache:\n                self.cache.set(text, embedding)\n\n            return embedding\n        except Exception as e:\n            logger.error(f\"Failed to embed query: {e}\")\n            raise\n\n    async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of texts (chunks).\"\"\"\n        all_embeddings = []\n\n        # Process in batches\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Check cache for each item in batch\n            batch_embeddings = [None] * len(batch)\n            indices_to_fetch = []\n            texts_to_fetch = []\n\n            if self.use_cache and self.cache:\n                for j, text in enumerate(batch):\n                    cached = self.cache.get(text)\n                    if cached:\n                        batch_embeddings[j] = cached\n                    else:\n                        indices_to_fetch.append(j)\n                        texts_to_fetch.append(text)\n            else:\n                indices_to_fetch = list(range(len(batch)))\n                texts_to_fetch = batch\n\n            # Fetch missing embeddings\n            if texts_to_fetch:\n                try:\n                    fetched_embeddings = await self._generate_batch_embeddings(texts_to_fetch)\n\n                    # Fill back into batch_embeddings and update cache\n                    for idx, embedding in zip(indices_to_fetch, fetched_embeddings):\n                        batch_embeddings[idx] = embedding\n                        if self.use_cache and self.cache:\n                            self.cache.set(batch[idx], embedding)\n\n                except Exception as e:\n                    logger.error(f\"Failed to embed batch: {e}\")\n                    raise\n\n            # Filter out Nones (should not happen if logic is correct)\n            valid_embeddings = [e for e in batch_embeddings if e is not None]\n            all_embeddings.extend(valid_embeddings)\n\n        return all_embeddings\n\n    async def embed_chunks(\n        self,\n        chunks: List[Any], # Typed as Any to avoid circular import with chunker.DocumentChunk\n        progress_callback: Optional[callable] = None\n    ) -&gt; List[Any]:\n        \"\"\"\n        Generate embeddings for document chunks.\n        Kept for backward compatibility with ingest.py\n        \"\"\"\n        if not chunks:\n            return chunks\n\n        logger.info(f\"Generating embeddings for {len(chunks)} chunks\")\n\n        # Extract texts\n        texts = [chunk.content for chunk in chunks]\n\n        # Generate all embeddings\n        embeddings = await self.embed_documents(texts)\n\n        # Assign back to chunks\n        for i, chunk in enumerate(chunks):\n            chunk.embedding = embeddings[i]\n            if chunk.metadata:\n                chunk.metadata[\"embedding_model\"] = self.model_name\n                chunk.metadata[\"embedding_generated_at\"] = datetime.now().isoformat()\n\n        return chunks\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def _generate_single_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Generate embedding for a single text with retry logic.\"\"\"\n        response = await self.client.embeddings.create(\n            model=self.model_name,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def _generate_batch_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Generate embeddings for a batch of texts with retry logic.\"\"\"\n        # Filter empty strings to avoid API errors\n        processed_texts = [t if t.strip() else \" \" for t in texts]\n\n        response = await self.client.embeddings.create(\n            model=self.model_name,\n            input=processed_texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_chunks","title":"<code>embed_chunks(chunks: List[Any], progress_callback: Optional[callable] = None) -&gt; List[Any]</code>  <code>async</code>","text":"<p>Generate embeddings for document chunks. Kept for backward compatibility with ingest.py</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>async def embed_chunks(\n    self,\n    chunks: List[Any], # Typed as Any to avoid circular import with chunker.DocumentChunk\n    progress_callback: Optional[callable] = None\n) -&gt; List[Any]:\n    \"\"\"\n    Generate embeddings for document chunks.\n    Kept for backward compatibility with ingest.py\n    \"\"\"\n    if not chunks:\n        return chunks\n\n    logger.info(f\"Generating embeddings for {len(chunks)} chunks\")\n\n    # Extract texts\n    texts = [chunk.content for chunk in chunks]\n\n    # Generate all embeddings\n    embeddings = await self.embed_documents(texts)\n\n    # Assign back to chunks\n    for i, chunk in enumerate(chunks):\n        chunk.embedding = embeddings[i]\n        if chunk.metadata:\n            chunk.metadata[\"embedding_model\"] = self.model_name\n            chunk.metadata[\"embedding_generated_at\"] = datetime.now().isoformat()\n\n    return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_documents","title":"<code>embed_documents(texts: List[str]) -&gt; List[List[float]]</code>  <code>async</code>","text":"<p>Embed a list of texts (chunks).</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Embed a list of texts (chunks).\"\"\"\n    all_embeddings = []\n\n    # Process in batches\n    for i in range(0, len(texts), self.batch_size):\n        batch = texts[i:i + self.batch_size]\n\n        # Check cache for each item in batch\n        batch_embeddings = [None] * len(batch)\n        indices_to_fetch = []\n        texts_to_fetch = []\n\n        if self.use_cache and self.cache:\n            for j, text in enumerate(batch):\n                cached = self.cache.get(text)\n                if cached:\n                    batch_embeddings[j] = cached\n                else:\n                    indices_to_fetch.append(j)\n                    texts_to_fetch.append(text)\n        else:\n            indices_to_fetch = list(range(len(batch)))\n            texts_to_fetch = batch\n\n        # Fetch missing embeddings\n        if texts_to_fetch:\n            try:\n                fetched_embeddings = await self._generate_batch_embeddings(texts_to_fetch)\n\n                # Fill back into batch_embeddings and update cache\n                for idx, embedding in zip(indices_to_fetch, fetched_embeddings):\n                    batch_embeddings[idx] = embedding\n                    if self.use_cache and self.cache:\n                        self.cache.set(batch[idx], embedding)\n\n            except Exception as e:\n                logger.error(f\"Failed to embed batch: {e}\")\n                raise\n\n        # Filter out Nones (should not happen if logic is correct)\n        valid_embeddings = [e for e in batch_embeddings if e is not None]\n        all_embeddings.extend(valid_embeddings)\n\n    return all_embeddings\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_query","title":"<code>embed_query(text: str) -&gt; List[float]</code>  <code>async</code>","text":"<p>Embed a single query string.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>async def embed_query(self, text: str) -&gt; List[float]:\n    \"\"\"Embed a single query string.\"\"\"\n    # Check cache first\n    if self.use_cache and self.cache:\n        cached = self.cache.get(text)\n        if cached:\n            return cached\n\n    try:\n        embedding = await self._generate_single_embedding(text)\n\n        if self.use_cache and self.cache:\n            self.cache.set(text, embedding)\n\n        return embedding\n    except Exception as e:\n        logger.error(f\"Failed to embed query: {e}\")\n        raise\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.create_embedder","title":"<code>create_embedder(use_cache: bool = True, batch_size: int = 100, max_retries: int = 3, retry_delay: float = 1.0, model_name: Optional[str] = None) -&gt; BaseEmbedder</code>","text":"<p>Factory function to create an embedder instance.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>def create_embedder(\n    use_cache: bool = True, \n    batch_size: int = 100,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n    model_name: Optional[str] = None\n) -&gt; BaseEmbedder:\n    \"\"\"Factory function to create an embedder instance.\"\"\"\n\n    # Get model from env if not provided\n    if not model_name:\n        model_name = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n\n    return EmbeddingGenerator(\n        model_name=model_name,\n        batch_size=batch_size,\n        use_cache=use_cache\n    )\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.main","title":"<code>main()</code>  <code>async</code>","text":"<p>Example usage.</p> Source code in <code>ingestion\\embedder.py</code> <pre><code>async def main():\n    \"\"\"Example usage.\"\"\"\n    embedder = create_embedder()\n    res = await embedder.embed_query(\"Hello world\")\n    print(f\"Embedding dimension: {len(res)}\")\n</code></pre>"},{"location":"api-reference/utils/","title":"Utilities Module","text":""},{"location":"api-reference/utils/#database-utilities","title":"Database Utilities","text":""},{"location":"api-reference/utils/#utils.db_utils","title":"<code>utils.db_utils</code>","text":"<p>Database utilities for PostgreSQL connection and operations.</p>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool","title":"<code>DatabasePool</code>","text":"<p>Manages PostgreSQL connection pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>class DatabasePool:\n    \"\"\"Manages PostgreSQL connection pool.\"\"\"\n\n    def __init__(self, database_url: Optional[str] = None):\n        \"\"\"\n        Initialize database pool.\n\n        Args:\n            database_url: PostgreSQL connection URL\n        \"\"\"\n        self.database_url = database_url or os.getenv(\"DATABASE_URL\")\n        if not self.database_url:\n            raise ValueError(\"DATABASE_URL environment variable not set\")\n\n        self.pool: Optional[Pool] = None\n\n    async def initialize(self):\n        \"\"\"Create optimized connection pool.\"\"\"\n        if not self.pool:\n            # Optimized pool settings for MCP server workload\n            # - min_size=2: Keep minimal connections warm (MCP has bursty traffic)\n            # - max_size=10: Sufficient for concurrent queries (was 20, reduced overhead)\n            # - max_queries=50000: Connection recycling threshold\n            # - statement_cache_size=100: Enable prepared statements (was 0)\n            #   Note: Set to 0 only if using PgBouncer in transaction mode\n            self.pool = await asyncpg.create_pool(\n                self.database_url,\n                min_size=2,  # Reduced from 5 (lower idle overhead)\n                max_size=10,  # Reduced from 20 (sufficient for MCP workload)\n                max_inactive_connection_lifetime=300,\n                command_timeout=60,\n                max_queries=50000,  # Recycle connections periodically\n                statement_cache_size=100,  # Enable prepared statement cache\n                # Set to 0 if using PgBouncer in transaction pooling mode\n            )\n            logger.info(\n                \"\u2713 Database connection pool initialized \"\n                \"(min=2, max=10, statement_cache=100)\"\n            )\n\n    async def close(self):\n        \"\"\"Close connection pool.\"\"\"\n        if self.pool:\n            await self.pool.close()\n            self.pool = None\n            logger.info(\"Database connection pool closed\")\n\n    @asynccontextmanager\n    async def acquire(self):\n        \"\"\"Acquire a connection from the pool.\"\"\"\n        if not self.pool:\n            await self.initialize()\n\n        async with self.pool.acquire() as connection:\n            yield connection\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.__init__","title":"<code>__init__(database_url: Optional[str] = None)</code>","text":"<p>Initialize database pool.</p> <p>Parameters:</p> Name Type Description Default <code>database_url</code> <code>Optional[str]</code> <p>PostgreSQL connection URL</p> <code>None</code> Source code in <code>utils\\db_utils.py</code> <pre><code>def __init__(self, database_url: Optional[str] = None):\n    \"\"\"\n    Initialize database pool.\n\n    Args:\n        database_url: PostgreSQL connection URL\n    \"\"\"\n    self.database_url = database_url or os.getenv(\"DATABASE_URL\")\n    if not self.database_url:\n        raise ValueError(\"DATABASE_URL environment variable not set\")\n\n    self.pool: Optional[Pool] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.acquire","title":"<code>acquire()</code>  <code>async</code>","text":"<p>Acquire a connection from the pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>@asynccontextmanager\nasync def acquire(self):\n    \"\"\"Acquire a connection from the pool.\"\"\"\n    if not self.pool:\n        await self.initialize()\n\n    async with self.pool.acquire() as connection:\n        yield connection\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close connection pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def close(self):\n    \"\"\"Close connection pool.\"\"\"\n    if self.pool:\n        await self.pool.close()\n        self.pool = None\n        logger.info(\"Database connection pool closed\")\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Create optimized connection pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def initialize(self):\n    \"\"\"Create optimized connection pool.\"\"\"\n    if not self.pool:\n        # Optimized pool settings for MCP server workload\n        # - min_size=2: Keep minimal connections warm (MCP has bursty traffic)\n        # - max_size=10: Sufficient for concurrent queries (was 20, reduced overhead)\n        # - max_queries=50000: Connection recycling threshold\n        # - statement_cache_size=100: Enable prepared statements (was 0)\n        #   Note: Set to 0 only if using PgBouncer in transaction mode\n        self.pool = await asyncpg.create_pool(\n            self.database_url,\n            min_size=2,  # Reduced from 5 (lower idle overhead)\n            max_size=10,  # Reduced from 20 (sufficient for MCP workload)\n            max_inactive_connection_lifetime=300,\n            command_timeout=60,\n            max_queries=50000,  # Recycle connections periodically\n            statement_cache_size=100,  # Enable prepared statement cache\n            # Set to 0 if using PgBouncer in transaction pooling mode\n        )\n        logger.info(\n            \"\u2713 Database connection pool initialized \"\n            \"(min=2, max=10, statement_cache=100)\"\n        )\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.close_database","title":"<code>close_database()</code>  <code>async</code>","text":"<p>Close database connection pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def close_database():\n    \"\"\"Close database connection pool.\"\"\"\n    await db_pool.close()\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.execute_query","title":"<code>execute_query(query: str, *params) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Execute a custom query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query</p> required <code>*params</code> <p>Query parameters</p> <code>()</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Query results</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def execute_query(query: str, *params) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Execute a custom query.\n\n    Args:\n        query: SQL query\n        *params: Query parameters\n\n    Returns:\n        Query results\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        results = await conn.fetch(query, *params)\n        return [dict(row) for row in results]\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.get_document","title":"<code>get_document(document_id: str) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Get document by ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>Document UUID</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Document data or None if not found</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def get_document(document_id: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Get document by ID.\n\n    Args:\n        document_id: Document UUID\n\n    Returns:\n        Document data or None if not found\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        result = await conn.fetchrow(\n            \"\"\"\n            SELECT \n                id::text,\n                title,\n                source,\n                content,\n                metadata,\n                created_at,\n                updated_at\n            FROM documents\n            WHERE id = $1::uuid\n            \"\"\",\n            document_id\n        )\n\n        if result:\n            return {\n                \"id\": result[\"id\"],\n                \"title\": result[\"title\"],\n                \"source\": result[\"source\"],\n                \"content\": result[\"content\"],\n                \"metadata\": json.loads(result[\"metadata\"]),\n                \"created_at\": result[\"created_at\"].isoformat(),\n                \"updated_at\": result[\"updated_at\"].isoformat()\n            }\n\n        return None\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.initialize_database","title":"<code>initialize_database()</code>  <code>async</code>","text":"<p>Initialize database connection pool.</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def initialize_database():\n    \"\"\"Initialize database connection pool.\"\"\"\n    await db_pool.initialize()\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.list_documents","title":"<code>list_documents(limit: int = 100, offset: int = 0, metadata_filter: Optional[Dict[str, Any]] = None) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>List documents with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of documents to return</p> <code>100</code> <code>offset</code> <code>int</code> <p>Number of documents to skip</p> <code>0</code> <code>metadata_filter</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata filter</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of documents</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def list_documents(\n    limit: int = 100,\n    offset: int = 0,\n    metadata_filter: Optional[Dict[str, Any]] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    List documents with optional filtering.\n\n    Args:\n        limit: Maximum number of documents to return\n        offset: Number of documents to skip\n        metadata_filter: Optional metadata filter\n\n    Returns:\n        List of documents\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        query = \"\"\"\n            SELECT \n                d.id::text,\n                d.title,\n                d.source,\n                d.metadata,\n                d.created_at,\n                d.updated_at,\n                COUNT(c.id) AS chunk_count\n            FROM documents d\n            LEFT JOIN chunks c ON d.id = c.document_id\n        \"\"\"\n\n        params = []\n        conditions = []\n\n        if metadata_filter:\n            conditions.append(f\"d.metadata @&gt; ${len(params) + 1}::jsonb\")\n            params.append(json.dumps(metadata_filter))\n\n        if conditions:\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        query += \"\"\"\n            GROUP BY d.id, d.title, d.source, d.metadata, d.created_at, d.updated_at\n            ORDER BY d.created_at DESC\n            LIMIT $%d OFFSET $%d\n        \"\"\" % (len(params) + 1, len(params) + 2)\n\n        params.extend([limit, offset])\n\n        results = await conn.fetch(query, *params)\n\n        return [\n            {\n                \"id\": row[\"id\"],\n                \"title\": row[\"title\"],\n                \"source\": row[\"source\"],\n                \"metadata\": json.loads(row[\"metadata\"]),\n                \"created_at\": row[\"created_at\"].isoformat(),\n                \"updated_at\": row[\"updated_at\"].isoformat(),\n                \"chunk_count\": row[\"chunk_count\"]\n            }\n            for row in results\n        ]\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.test_connection","title":"<code>test_connection() -&gt; bool</code>  <code>async</code>","text":"<p>Test database connection.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connection successful</p> Source code in <code>utils\\db_utils.py</code> <pre><code>async def test_connection() -&gt; bool:\n    \"\"\"\n    Test database connection.\n\n    Returns:\n        True if connection successful\n    \"\"\"\n    try:\n        async with db_pool.acquire() as conn:\n            await conn.fetchval(\"SELECT 1\")\n        return True\n    except Exception as e:\n        logger.error(f\"Database connection test failed: {e}\")\n        return False\n</code></pre>"},{"location":"api-reference/utils/#models","title":"Models","text":""},{"location":"api-reference/utils/#utils.models","title":"<code>utils.models</code>","text":"<p>Pydantic models for data validation and serialization.</p>"},{"location":"api-reference/utils/#utils.models.AgentContext","title":"<code>AgentContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Agent execution context.</p> Source code in <code>utils\\models.py</code> <pre><code>class AgentContext(BaseModel):\n    \"\"\"Agent execution context.\"\"\"\n    session_id: str\n    messages: List[Message] = Field(default_factory=list)\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n    search_results: List[ChunkResult] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.AgentDependencies","title":"<code>AgentDependencies</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dependencies for the agent.</p> Source code in <code>utils\\models.py</code> <pre><code>class AgentDependencies(BaseModel):\n    \"\"\"Dependencies for the agent.\"\"\"\n    session_id: str\n    database_url: Optional[str] = None\n    openai_api_key: Optional[str] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChatResponse","title":"<code>ChatResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Chat response model.</p> Source code in <code>utils\\models.py</code> <pre><code>class ChatResponse(BaseModel):\n    \"\"\"Chat response model.\"\"\"\n    message: str\n    session_id: str\n    sources: List[DocumentMetadata] = Field(default_factory=list)\n    tools_used: List[ToolCall] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document chunk model.</p> Source code in <code>utils\\models.py</code> <pre><code>class Chunk(BaseModel):\n    \"\"\"Document chunk model.\"\"\"\n    id: Optional[str] = None\n    document_id: str\n    content: str\n    embedding: Optional[List[float]] = None\n    chunk_index: int\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    token_count: Optional[int] = None\n    created_at: Optional[datetime] = None\n\n    @field_validator('embedding')\n    @classmethod\n    def validate_embedding(cls, v: Optional[List[float]]) -&gt; Optional[List[float]]:\n        \"\"\"Validate embedding dimensions.\"\"\"\n        if v is not None and len(v) != 1536:  # OpenAI text-embedding-3-small\n            raise ValueError(f\"Embedding must have 1536 dimensions, got {len(v)}\")\n        return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Chunk.validate_embedding","title":"<code>validate_embedding(v: Optional[List[float]]) -&gt; Optional[List[float]]</code>  <code>classmethod</code>","text":"<p>Validate embedding dimensions.</p> Source code in <code>utils\\models.py</code> <pre><code>@field_validator('embedding')\n@classmethod\ndef validate_embedding(cls, v: Optional[List[float]]) -&gt; Optional[List[float]]:\n    \"\"\"Validate embedding dimensions.\"\"\"\n    if v is not None and len(v) != 1536:  # OpenAI text-embedding-3-small\n        raise ValueError(f\"Embedding must have 1536 dimensions, got {len(v)}\")\n    return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChunkResult","title":"<code>ChunkResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Chunk search result model.</p> Source code in <code>utils\\models.py</code> <pre><code>class ChunkResult(BaseModel):\n    \"\"\"Chunk search result model.\"\"\"\n    chunk_id: str\n    document_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    document_title: str\n    document_source: str\n\n    @field_validator('score')\n    @classmethod\n    def validate_score(cls, v: float) -&gt; float:\n        \"\"\"Ensure score is between 0 and 1.\"\"\"\n        return max(0.0, min(1.0, v))\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChunkResult.validate_score","title":"<code>validate_score(v: float) -&gt; float</code>  <code>classmethod</code>","text":"<p>Ensure score is between 0 and 1.</p> Source code in <code>utils\\models.py</code> <pre><code>@field_validator('score')\n@classmethod\ndef validate_score(cls, v: float) -&gt; float:\n    \"\"\"Ensure score is between 0 and 1.\"\"\"\n    return max(0.0, min(1.0, v))\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document model.</p> Source code in <code>utils\\models.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"Document model.\"\"\"\n    id: Optional[str] = None\n    title: str\n    source: str\n    content: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.DocumentMetadata","title":"<code>DocumentMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document metadata model.</p> Source code in <code>utils\\models.py</code> <pre><code>class DocumentMetadata(BaseModel):\n    \"\"\"Document metadata model.\"\"\"\n    id: str\n    title: str\n    source: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime\n    updated_at: datetime\n    chunk_count: Optional[int] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionConfig","title":"<code>IngestionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for document ingestion.</p> Source code in <code>utils\\models.py</code> <pre><code>class IngestionConfig(BaseModel):\n    \"\"\"Configuration for document ingestion.\"\"\"\n    chunk_size: int = Field(default=1000, ge=100, le=5000)\n    chunk_overlap: int = Field(default=200, ge=0, le=1000)\n    max_chunk_size: int = Field(default=2000, ge=500, le=10000)\n    use_semantic_chunking: bool = True\n\n    @field_validator('chunk_overlap')\n    @classmethod\n    def validate_overlap(cls, v: int, info) -&gt; int:\n        \"\"\"Ensure overlap is less than chunk size.\"\"\"\n        chunk_size = info.data.get('chunk_size', 1000)\n        if v &gt;= chunk_size:\n            raise ValueError(f\"Chunk overlap ({v}) must be less than chunk size ({chunk_size})\")\n        return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionConfig.validate_overlap","title":"<code>validate_overlap(v: int, info) -&gt; int</code>  <code>classmethod</code>","text":"<p>Ensure overlap is less than chunk size.</p> Source code in <code>utils\\models.py</code> <pre><code>@field_validator('chunk_overlap')\n@classmethod\ndef validate_overlap(cls, v: int, info) -&gt; int:\n    \"\"\"Ensure overlap is less than chunk size.\"\"\"\n    chunk_size = info.data.get('chunk_size', 1000)\n    if v &gt;= chunk_size:\n        raise ValueError(f\"Chunk overlap ({v}) must be less than chunk size ({chunk_size})\")\n    return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionResult","title":"<code>IngestionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of document ingestion.</p> Source code in <code>utils\\models.py</code> <pre><code>class IngestionResult(BaseModel):\n    \"\"\"Result of document ingestion.\"\"\"\n    document_id: str\n    title: str\n    chunks_created: int\n    processing_time_ms: float\n    errors: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message model.</p> Source code in <code>utils\\models.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"Message model.\"\"\"\n    id: Optional[str] = None\n    session_id: str\n    role: MessageRole\n    content: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n\n    model_config = ConfigDict(use_enum_values=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.MessageRole","title":"<code>MessageRole</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enum.</p> Source code in <code>utils\\models.py</code> <pre><code>class MessageRole(str, Enum):\n    \"\"\"Message role enum.\"\"\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    SYSTEM = \"system\"\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchRequest","title":"<code>SearchRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search request model.</p> Source code in <code>utils\\models.py</code> <pre><code>class SearchRequest(BaseModel):\n    \"\"\"Search request model.\"\"\"\n    query: str = Field(..., description=\"Search query\")\n    search_type: SearchType = Field(default=SearchType.SEMANTIC, description=\"Type of search\")\n    limit: int = Field(default=10, ge=1, le=50, description=\"Maximum results\")\n    filters: Dict[str, Any] = Field(default_factory=dict, description=\"Search filters\")\n\n    model_config = ConfigDict(use_enum_values=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchResponse","title":"<code>SearchResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search response model.</p> Source code in <code>utils\\models.py</code> <pre><code>class SearchResponse(BaseModel):\n    \"\"\"Search response model.\"\"\"\n    results: List[ChunkResult] = Field(default_factory=list)\n    total_results: int = 0\n    search_type: SearchType\n    query_time_ms: float\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchType","title":"<code>SearchType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Search type enum.</p> Source code in <code>utils\\models.py</code> <pre><code>class SearchType(str, Enum):\n    \"\"\"Search type enum.\"\"\"\n    SEMANTIC = \"semantic\"\n    KEYWORD = \"keyword\"\n    HYBRID = \"hybrid\"\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Session","title":"<code>Session</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Session model.</p> Source code in <code>utils\\models.py</code> <pre><code>class Session(BaseModel):\n    \"\"\"Session model.\"\"\"\n    id: Optional[str] = None\n    user_id: Optional[str] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n    expires_at: Optional[datetime] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.StreamDelta","title":"<code>StreamDelta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming response delta.</p> Source code in <code>utils\\models.py</code> <pre><code>class StreamDelta(BaseModel):\n    \"\"\"Streaming response delta.\"\"\"\n    content: str\n    delta_type: Literal[\"text\", \"tool_call\", \"end\"] = \"text\"\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool call information model.</p> Source code in <code>utils\\models.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"Tool call information model.\"\"\"\n    tool_name: str\n    args: Dict[str, Any] = Field(default_factory=dict)\n    tool_call_id: Optional[str] = None\n</code></pre>"},{"location":"api-reference/utils/#providers","title":"Providers","text":""},{"location":"api-reference/utils/#utils.providers","title":"<code>utils.providers</code>","text":"<p>Simplified provider configuration for OpenAI models only.</p>"},{"location":"api-reference/utils/#utils.providers.ProviderConfig","title":"<code>ProviderConfig</code>  <code>dataclass</code>","text":"<p>Configuration for AI provider.</p> Source code in <code>utils\\providers.py</code> <pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"Configuration for AI provider.\"\"\"\n    api_key: Optional[str] = None\n    base_url: Optional[str] = None\n    embedding_model: str = \"text-embedding-3-small\"\n    llm_model: str = \"gpt-4.1-mini\"\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_embedding_client","title":"<code>get_embedding_client() -&gt; openai.AsyncOpenAI</code>","text":"<p>Get OpenAI client for embeddings.</p> <p>Returns:</p> Type Description <code>AsyncOpenAI</code> <p>Configured OpenAI client for embeddings</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_embedding_client() -&gt; openai.AsyncOpenAI:\n    \"\"\"\n    Get OpenAI client for embeddings.\n\n    Returns:\n        Configured OpenAI client for embeddings\n    \"\"\"\n    api_key = os.getenv('OPENAI_API_KEY')\n\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n    return openai.AsyncOpenAI(api_key=api_key)\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_embedding_model","title":"<code>get_embedding_model() -&gt; str</code>","text":"<p>Get embedding model name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Embedding model name</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_embedding_model() -&gt; str:\n    \"\"\"\n    Get embedding model name.\n\n    Returns:\n        Embedding model name\n    \"\"\"\n    return os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_ingestion_model","title":"<code>get_ingestion_model() -&gt; OpenAIModel</code>","text":"<p>Get model for ingestion tasks (uses same model as main LLM).</p> <p>Returns:</p> Type Description <code>OpenAIModel</code> <p>Configured model for ingestion tasks</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_ingestion_model() -&gt; OpenAIModel:\n    \"\"\"\n    Get model for ingestion tasks (uses same model as main LLM).\n\n    Returns:\n        Configured model for ingestion tasks\n    \"\"\"\n    return get_llm_model()\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_llm_model","title":"<code>get_llm_model() -&gt; OpenAIModel</code>","text":"<p>Get LLM model configuration for OpenAI.</p> <p>Returns:</p> Type Description <code>OpenAIModel</code> <p>Configured OpenAI model</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_llm_model() -&gt; OpenAIModel:\n    \"\"\"\n    Get LLM model configuration for OpenAI.\n\n    Returns:\n        Configured OpenAI model\n    \"\"\"\n    llm_choice = os.getenv('LLM_CHOICE', 'gpt-4.1-mini')\n    api_key = os.getenv('OPENAI_API_KEY')\n\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n    return OpenAIModel(llm_choice, provider=OpenAIProvider(api_key=api_key))\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_model_info","title":"<code>get_model_info() -&gt; dict</code>","text":"<p>Get information about current model configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with model configuration info</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_model_info() -&gt; dict:\n    \"\"\"\n    Get information about current model configuration.\n\n    Returns:\n        Dictionary with model configuration info\n    \"\"\"\n    return {\n        \"llm_provider\": \"openai\",\n        \"llm_model\": os.getenv('LLM_CHOICE', 'gpt-4.1-mini'),\n        \"embedding_provider\": \"openai\",\n        \"embedding_model\": get_embedding_model(),\n    }\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_provider_config","title":"<code>get_provider_config() -&gt; ProviderConfig</code>","text":"<p>Get provider configuration from environment.</p> Source code in <code>utils\\providers.py</code> <pre><code>def get_provider_config() -&gt; ProviderConfig:\n    \"\"\"Get provider configuration from environment.\"\"\"\n    return ProviderConfig(\n        api_key=os.getenv('OPENAI_API_KEY'),\n        base_url=os.getenv('OPENAI_BASE_URL'),\n        embedding_model=os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small'),\n        llm_model=os.getenv('LLM_CHOICE', 'gpt-4.1-mini')\n    )\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.validate_configuration","title":"<code>validate_configuration() -&gt; bool</code>","text":"<p>Validate that required environment variables are set.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p> Source code in <code>utils\\providers.py</code> <pre><code>def validate_configuration() -&gt; bool:\n    \"\"\"\n    Validate that required environment variables are set.\n\n    Returns:\n        True if configuration is valid\n    \"\"\"\n    required_vars = [\n        'OPENAI_API_KEY',\n        'DATABASE_URL'\n    ]\n\n    missing_vars = []\n    for var in required_vars:\n        if not os.getenv(var):\n            missing_vars.append(var)\n\n    if missing_vars:\n        print(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n        return False\n\n    return True\n</code></pre>"}]}