{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Docling RAG Agent - Documentazione","text":"<p>Benvenuto nella documentazione del progetto Docling RAG Agent.</p>"},{"location":"#panoramica","title":"Panoramica","text":"<p>Questo progetto implementa un agente RAG (Retrieval-Augmented Generation) utilizzando Docling per l'ingestione documenti e PydanticAI come framework agent.</p>"},{"location":"#guide","title":"Guide","text":""},{"location":"#guida-allo-sviluppo","title":"Guida allo Sviluppo","text":"<p>Informazioni dettagliate su:</p> <ul> <li>Struttura del progetto e organizzazione directory</li> <li>Pattern FastMCP per sviluppo MCP server</li> <li>Testing con PydanticAI (TestModel, FunctionModel, override)</li> <li>Walkthrough dell'implementazione</li> </ul>"},{"location":"#guida-al-troubleshooting","title":"Guida al Troubleshooting","text":"<p>Soluzioni per problemi comuni:</p> <ul> <li>Configurazione e troubleshooting MCP server</li> <li>Problemi di connessione database</li> <li>Problemi di ambiente e dipendenze</li> <li>Script di verifica e avvio automatico API</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>Esplora la API Reference per comprendere la struttura del codebase e le funzioni disponibili.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Consulta la Guida Getting Started per istruzioni di installazione e setup.</p>"},{"location":"architecture/","title":"Architecture - docling-rag-agent","text":""},{"location":"architecture/#executive-summary","title":"Executive Summary","text":"<p>docling-rag-agent \u00e8 un sistema RAG (Retrieval Augmented Generation) production-ready che fornisce accesso conversazionale a knowledge base documentali tramite Streamlit UI e MCP Server. L'architettura \u00e8 basata su Service-Oriented Architecture (SOA) con core business logic decoupled, LangFuse observability integrata, e MCP server standalone. Il sistema \u00e8 progettato per prevenire conflitti tra agenti AI attraverso decisioni architetturali esplicite e pattern di implementazione rigorosi.</p>"},{"location":"architecture/#decision-summary","title":"Decision Summary","text":"Category Decision Version Verified Affects Epics Rationale Observability Integration LangFuse decorator-based (<code>@observe()</code>) LangFuse Python SDK v3.0.0+ 2025-11-26 Epic 2, Epic 3 Trace gerarchici automatici, cost tracking integrato, graceful degradation Cost Tracking LangFuse auto-tracking con <code>langfuse.openai</code> wrapper OpenAI SDK 2.8.1+ 2025-11-26 Epic 2, Epic 3 Zero codice aggiuntivo, pricing sempre aggiornato, breakdown automatico MCP Server Architecture Modulo <code>mcp/</code> con tools separati per dominio FastMCP 0.4.x+ 2025-11-26 Epic 2 Standalone, direct service integration, pattern FastMCP nativi Error Handling FastMCP <code>ToolError</code> Pattern FastMCP 0.4.x+ 2025-11-26 Epic 2, Epic 4 Messaggi informativi, logging strutturato, <code>mask_error_details=True</code> in produzione Logging Pattern JSON Structured Logging python-json-logger 4.0.0+ 2025-11-26 Epic 2, Epic 4 Facile parsing, integrazione monitoring, ricerca efficiente API Response Format Direct Pydantic Models Pydantic 2.x 2025-11-26 Epic 4 Type-safe, semplice, gi\u00e0 implementato Testing Infrastructure TDD Structure Rigorosa pytest 8.x+ 2025-11-26 Epic 5 Coverage &gt;70% enforcement, RAGAS evaluation, Playwright E2E Project Structure Mapping epics \u2192 directories con convenzioni - - Epic 6 Organizzazione per responsabilit\u00e0, zero file sparsi Date/Time Handling ISO 8601 strings, UTC storage datetime standard - All Epics Standard internazionale, parsing facile Retry Pattern Exponential backoff, max 3 tentativi tenacity 9.1.2+ 2025-11-26 Epic 2, Epic 4 Resilienza per transient errors Git Workflow Git Flow semplificato + Conventional Commits - - Epic 4 Branch protection, commit standardization CI/CD Pipeline GitHub Actions: lint, type-check, test, secret scan GitHub Actions - Epic 4 Quality gates automatici, security scanning Secret Scanning TruffleHog OSS su ogni PR TruffleHog OSS - Epic 4 Prevenzione leak secrets, fail build se rilevati Code Review CodeRabbit AI su ogni PR CodeRabbit - Epic 4 Code quality, best practices, security Versionamento Semantic Versioning + CHANGELOG.md SemVer - Epic 4 Standard industry, tracciabilit\u00e0"},{"location":"architecture/#project-structure","title":"Project Structure","text":"<pre><code>docling-rag-agent/\n\u251c\u2500\u2500 docling_mcp/                      # Epic 2: MCP Server (standalone)\n\u2502   \u251c\u2500\u2500 __init__.py                   # Module exports\n\u2502   \u251c\u2500\u2500 server.py                     # FastMCP instance + tool definitions\n\u2502   \u251c\u2500\u2500 lifespan.py                   # Server lifecycle (DB init, embedder init)\n\u2502   \u251c\u2500\u2500 metrics.py                    # Prometheus metrics definitions (Story 2.3)\n\u2502   \u251c\u2500\u2500 health.py                     # Health check logic (Story 2.3)\n\u2502   \u251c\u2500\u2500 http_server.py                # FastAPI /metrics and /health endpoints (Story 2.3)\n\u2502   \u2514\u2500\u2500 tools/                        # Tool modules (for reference/documentation)\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 search.py                 # query_knowledge_base, ask_knowledge_base\n\u2502       \u251c\u2500\u2500 documents.py              # list_knowledge_base_documents, get_knowledge_base_document\n\u2502       \u2514\u2500\u2500 overview.py               # get_knowledge_base_overview\n\u2502   # Note: Directory named docling_mcp/ to avoid conflict with FastMCP's mcp package\n\u2502\n\u251c\u2500\u2500 core/                             # Epic 2: RAG Business Logic\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 agent.py                      # PydanticAI agent wrapper (Streamlit)\n\u2502   \u2514\u2500\u2500 rag_service.py                # Pure RAG logic (decoupled)\n\u2502\n\u251c\u2500\u2500 ingestion/                        # Epic 1: Document Processing\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 ingest.py                     # DocumentIngestionPipeline\n\u2502   \u251c\u2500\u2500 chunker.py                    # HybridChunker, SimpleChunker\n\u2502   \u2514\u2500\u2500 embedder.py                   # EmbeddingGenerator (OpenAI)\n\u2502\n\u251c\u2500\u2500 utils/                            # Shared Utilities\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 db_utils.py                   # AsyncPG connection pooling\n\u2502   \u251c\u2500\u2500 models.py                     # Pydantic data models\n\u2502   \u251c\u2500\u2500 providers.py                  # OpenAI provider config\n\u2502   \u251c\u2500\u2500 session_manager.py            # Epic 3: Session tracking and persistence\n\u2502   \u251c\u2500\u2500 langfuse_streamlit.py        # Epic 3: LangFuse context injection for Streamlit\n\u2502   \u251c\u2500\u2500 cost_monitor.py               # Epic 3: Cost monitoring and enforcement (optional, security)\n\u2502   \u251c\u2500\u2500 rate_limiter.py               # Epic 3: Rate limiting (optional, security)\n\u2502   \u2514\u2500\u2500 streamlit_auth.py            # Epic 3: Simple authentication (optional, security)\n\u2502\n\u251c\u2500\u2500 api/                              # Epic 4: FastAPI Service (optional)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py                       # FastAPI app + endpoints\n\u2502   \u2514\u2500\u2500 models.py                     # API request/response models\n\u2502\n\u251c\u2500\u2500 tests/                            # Epic 5: Testing Infrastructure\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py                   # Shared fixtures\n\u2502   \u251c\u2500\u2500 unit/                         # Unit tests (&gt;70% coverage)\n\u2502   \u2502   \u251c\u2500\u2500 test_rag_service.py\n\u2502   \u2502   \u251c\u2500\u2500 test_embedder.py\n\u2502   \u2502   \u2514\u2500\u2500 test_chunker.py\n\u2502   \u251c\u2500\u2500 integration/                 # Integration tests\n\u2502   \u2502   \u251c\u2500\u2500 test_mcp_server.py\n\u2502   \u2502   \u2514\u2500\u2500 test_api_endpoints.py\n\u2502   \u251c\u2500\u2500 e2e/                         # E2E tests (Playwright)\n\u2502   \u2502   \u2514\u2500\u2500 test_streamlit_workflow.py\n\u2502   \u2514\u2500\u2500 fixtures/                    # Test fixtures + golden dataset\n\u2502       \u2514\u2500\u2500 golden_dataset.json      # 20+ query-answer pairs (RAGAS)\n\u2502\n\u251c\u2500\u2500 scripts/                          # Epic 4: Utility Scripts\n\u2502   \u251c\u2500\u2500 verification/                 # Verification scripts\n\u2502   \u2502   \u251c\u2500\u2500 verify_api_endpoints.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_mcp_setup.py\n\u2502   \u2502   \u2514\u2500\u2500 verify_client_integration.py\n\u2502   \u2514\u2500\u2500 debug/                        # Debug utilities\n\u2502       \u2514\u2500\u2500 debug_mcp_tools.py\n\u2502\n\u251c\u2500\u2500 docs/                            # Epic 1: Documentation\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 architecture.md              # This file\n\u2502   \u251c\u2500\u2500 prd.md\n\u2502   \u251c\u2500\u2500 epics.md\n\u2502   \u251c\u2500\u2500 development-guide.md\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 sql/                             # Database Schema\n\u2502   \u251c\u2500\u2500 schema.sql                   # PostgreSQL + PGVector schema\n\u2502   \u251c\u2500\u2500 optimize_index.sql\n\u2502   \u2514\u2500\u2500 removeDocuments.sql\n\u2502\n\u251c\u2500\u2500 .github/                         # Epic 4: CI/CD Workflows\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 ci.yml                   # Lint, type-check, test, build\n\u2502       \u2514\u2500\u2500 release.yml              # Release automation\n\u2502\n\u251c\u2500\u2500 app.py                           # Epic 3: Streamlit UI entry point\n\u251c\u2500\u2500 pyproject.toml                   # Project configuration (UV)\n\u251c\u2500\u2500 uv.lock                          # Dependency lock file\n\u251c\u2500\u2500 docker-compose.yml               # Docker orchestration\n\u251c\u2500\u2500 Dockerfile                       # Streamlit container\n\u251c\u2500\u2500 Dockerfile.api                   # API container (optional)\n\u251c\u2500\u2500 .env.example                     # Environment variables template\n\u251c\u2500\u2500 coderabbit.yaml                  # CodeRabbit configuration\n\u251c\u2500\u2500 CHANGELOG.md                     # Semantic versioning changelog\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md                        # Project documentation\n</code></pre>"},{"location":"architecture/#epic-to-architecture-mapping","title":"Epic to Architecture Mapping","text":"Epic Stories Directory/Component Responsibility Epic 1: Core RAG Baseline 1.1-1.4 <code>docs/</code>, <code>ingestion/</code> Documentation + ingestion pipeline Epic 2: MCP Observability 2.1-2.5 <code>mcp/</code>, <code>core/rag_service.py</code> LangFuse integration + MCP standalone Epic 3: Streamlit Observability 3.1-3.2 <code>app.py</code>, <code>utils/session_manager.py</code>, <code>utils/langfuse_streamlit.py</code>, <code>utils/cost_monitor.py</code> (optional), <code>utils/rate_limiter.py</code> (optional), <code>utils/streamlit_auth.py</code> (optional) Session tracking + LangFuse tracing + Security hardening Epic 4: Production Infrastructure 4.1-4.3 <code>api/</code>, <code>scripts/</code>, <code>.github/workflows/</code> CI/CD, health checks, Docker optimization Epic 5: Testing &amp; QA 5.1-5.4 <code>tests/</code> TDD infrastructure, unit/integration/E2E tests Epic 6: Project Structure 6.1-6.2 All directories Cleanup + validation"},{"location":"architecture/#technology-stack-details","title":"Technology Stack Details","text":""},{"location":"architecture/#core-technologies","title":"Core Technologies","text":"Component Technology Version Verified Purpose Notes Language Python 3.11 2025-11-26 Runtime Requires &gt;=3.10 Package Manager UV 0.9.13+ 2025-11-26 Dependency Management Fast, reliable Vector Database PostgreSQL 16+ 2025-11-26 Storage With PGVector extension Vector Extension PGVector 0.8.0+ 2025-11-26 Vector Search HNSW index for performance LLM Provider OpenAI 2.8.1+ 2025-11-26 Generation GPT-4o-mini Embeddings OpenAI 2.8.1+ 2025-11-26 Vectors text-embedding-3-small (1536 dims) UI Framework Streamlit 1.31+ 2025-11-26 Web Interface Chat interface MCP Framework FastMCP 0.4.x+ 2025-11-26 MCP Server Standalone server. Breaking changes: lifespan pattern, ToolError handling API Framework FastAPI 0.109+ 2025-11-26 API Service Optional, for scaling Agent Framework PydanticAI 0.7.4+ 2025-11-26 LLM Agent Streamlit integration Document Processing Docling 2.55+ 2025-11-26 Multi-format PDF, DOCX, PPTX, XLSX, HTML, MD, TXT Observability LangFuse 3.0.0+ 2025-11-26 Tracing &amp; Monitoring Python SDK v3 (OTel-based) Testing Framework pytest 8.x+ 2025-11-26 Testing With pytest-asyncio, pytest-cov E2E Testing Playwright 1.40.0+ 2025-11-26 E2E Tests Streamlit workflow testing RAG Evaluation RAGAS 0.1.0+ 2025-11-26 Quality Metrics Faithfulness, relevancy scores Logging python-json-logger 4.0.0+ 2025-11-26 Structured Logging JSON format for production Retry Logic tenacity 9.1.2+ 2025-11-26 Resilience Exponential backoff <p>Note on Version Verification: All versions listed above were verified via WebSearch on 2025-11-26. Version numbers use minimum version format (e.g., \"0.4.x+\") to allow flexibility while ensuring compatibility. Breaking changes are documented in ADRs where relevant (e.g., FastMCP 0.4.x+ breaking changes documented in ADR-002). Versions should be re-verified periodically, especially before major releases or when upgrading dependencies.</p>"},{"location":"architecture/#integration-points","title":"Integration Points","text":"<p>1. MCP Server \u2192 Core RAG Service (Pattern: Direct Service Integration)</p> <ul> <li>Pattern Name: Direct Service Integration Pattern</li> <li>Implementation: Direct import <code>from core.rag_service import search_knowledge_base_structured</code></li> <li>Communication: Function calls (no HTTP overhead)</li> <li>Lifecycle: FastMCP lifespan initializes DB pool + embedder at startup</li> <li>Error Handling: <code>ToolError</code> for user-facing errors, exception wrapping for unexpected errors</li> <li>Implementation Guide:</li> </ul> <p>```python   # mcp/server.py   from fastmcp import FastMCP   from core.rag_service import search_knowledge_base_structured</p> <p>mcp = FastMCP(\"docling-rag-agent\")</p> <p>@mcp.tool   async def query_knowledge_base(query: str, limit: int = 5):       # Direct function call, no HTTP overhead       return await search_knowledge_base_structured(query, limit)   ```</p> <p>2. Streamlit \u2192 Core Agent (Pattern: Agent Wrapper Integration)</p> <ul> <li>Pattern Name: Agent Wrapper Integration Pattern</li> <li>Implementation: <code>from core.agent import RAGAgent</code></li> <li>Communication: PydanticAI agent wrapper</li> <li>Lifecycle: Streamlit session manages agent instance</li> <li>Session Tracking: <code>st.session_state</code> for session_id, LangFuse context injection</li> </ul> <p>3. Core RAG Service \u2192 Utils (Pattern: Shared Resource Pattern)</p> <ul> <li>Pattern Name: Shared Resource Pattern</li> <li>Implementation: <code>from utils.db_utils import db_pool</code>, <code>from utils.providers import get_provider_config</code></li> <li>Communication: Shared connection pool + config</li> <li>Lifecycle: Global singleton pattern for embedder, connection pool per process</li> </ul> <p>4. Ingestion \u2192 Database (Pattern: Direct Database Access)</p> <ul> <li>Pattern Name: Direct Database Access Pattern</li> <li>Implementation: Direct DB connection via <code>utils.db_utils</code></li> <li>Communication: AsyncPG connection pool</li> <li>Lifecycle: Per-ingestion connection, cleanup on completion</li> </ul> <p>5. LangFuse Integration (Pattern: Decorator-Based Observability)</p> <ul> <li>Pattern Name: Decorator-Based Observability Pattern</li> <li>Implementation: <code>@observe()</code> decorator on critical functions</li> <li>Communication: LangFuse SDK v3 (async HTTP, OpenTelemetry-based)</li> <li>Lifecycle: Initialized at startup via env vars, graceful degradation if unavailable</li> <li>Cost Tracking: Automatic via <code>langfuse.openai</code> wrapper (Story 2.2)</li> <li>Embedding cost: Tracked via <code>langfuse.openai.AsyncOpenAI</code> in <code>ingestion/embedder.py</code></li> <li>LLM cost: Tracked via <code>langfuse.openai.chat.completions.create()</code> (future LLM integration)</li> <li>Nested spans: <code>embedding-generation</code> span in <code>docling_mcp/server.py</code> for cost breakdown visibility</li> <li>Implementation Guide:</li> </ul> <p>```python   # core/rag_service.py   from langfuse import observe, get_client</p> <p>langfuse = get_client()</p> <p>@observe(name=\"search_knowledge_base\")   async def search_knowledge_base_structured(query: str, limit: int = 5):       \"\"\"       LangFuse automatically captures:       - Function inputs/outputs       - Execution time       - Errors (if any)       - Nested spans for child operations       \"\"\"       # Embedding generation (nested span)       with langfuse.start_as_current_observation(           as_type=\"span\", name=\"embedding-generation\"       ) as embedding_span:           embedding = await generate_embedding(query)           embedding_span.update(output={\"embedding_dim\": len(embedding)})</p> <pre><code>  # DB search (nested span)\n  with langfuse.start_as_current_observation(\n      as_type=\"span\", name=\"vector-search\"\n  ) as search_span:\n      results = await search_vector_db(embedding, limit)\n      search_span.update(output={\"results_count\": len(results)})\n\n  return results\n</code></pre> <p># For LLM calls, use generation type:   @observe(name=\"llm-generation\", as_type=\"generation\")   async def generate_response(context: str, query: str):       from langfuse.openai import openai</p> <pre><code>  response = await openai.chat.completions.create(\n      model=\"gpt-4o-mini\",\n      messages=[\n          {\"role\": \"system\", \"content\": f\"Context: {context}\"},\n          {\"role\": \"user\", \"content\": query}\n      ]\n  )\n  # Cost tracking automatic via langfuse.openai wrapper\n  return response.choices[0].message.content\n</code></pre> <p>```</p> <p>Key Points:</p> <ul> <li>Use <code>@observe()</code> for automatic tracing of function calls</li> <li>Use <code>langfuse.start_as_current_observation()</code> for nested spans</li> <li>Use <code>as_type=\"generation\"</code> for LLM calls to enable cost tracking</li> <li>Use <code>langfuse.openai</code> wrapper instead of direct <code>openai</code> import for automatic cost tracking</li> <li>Trace attributes (user_id, session_id) can be set via <code>propagate_attributes()</code> context manager</li> </ul> <p>6b. LangFuse Streamlit Context Injection (Pattern: Context Manager Integration - Story 3.2)</p> <ul> <li>Pattern Name: Streamlit Context Injection Pattern</li> <li>Implementation: <code>utils/langfuse_streamlit.py</code> module with <code>with_streamlit_context()</code> context manager</li> <li>Communication: LangFuse SDK v3 context propagation via <code>propagate_attributes()</code></li> <li>Purpose: Create root trace for Streamlit queries with session_id propagation to all nested spans</li> <li>Implementation Guide:</li> </ul> <p>```python   # utils/langfuse_streamlit.py   from langfuse import get_client, propagate_attributes   from contextlib import contextmanager   from uuid import UUID</p> <p>@contextmanager   def with_streamlit_context(session_id: UUID, query: str):       \"\"\"       Context manager for LangFuse tracing in Streamlit.       Creates root span 'streamlit_query' with session_id propagation.       Implements graceful degradation if LangFuse unavailable.       \"\"\"       langfuse = get_client()</p> <pre><code>  with langfuse.start_as_current_observation(\n      as_type=\"span\",\n      name=\"streamlit_query\",\n      input={\"query\": query}\n  ) as root_span:\n      with propagate_attributes(\n          session_id=str(session_id),\n          metadata={\"source\": \"streamlit\", \"query_text\": query}\n      ):\n          ctx = StreamlitTraceContext(trace_id=root_span.trace_id)\n          yield ctx\n</code></pre> <p># Usage in app.py   from utils.langfuse_streamlit import with_streamlit_context</p> <p>with with_streamlit_context(session_id, query) as ctx:       response = await run_agent(query)       # Nested spans (embedding, DB, LLM) automatically inherit session_id       trace_id = ctx.trace_id  # For cost extraction   ```</p> <p>Key Points:</p> <ul> <li>Root span named <code>streamlit_query</code> separates Streamlit traces from MCP traces</li> <li><code>metadata={\"source\": \"streamlit\"}</code> enables dashboard filtering by source</li> <li><code>session_id</code> propagated to all child observations via <code>propagate_attributes()</code></li> <li>Graceful degradation: if LangFuse unavailable, continues without tracing</li> <li>Trace ID returned for post-execution cost extraction via <code>extract_cost_from_langfuse()</code></li> </ul> <p>6. Prometheus Metrics Integration (Pattern: Metrics Instrumentation)</p> <ul> <li>Pattern Name: Metrics Instrumentation Pattern</li> <li>Implementation: <code>prometheus_client</code> library with Counter, Histogram, Gauge types</li> <li>Communication: HTTP <code>/metrics</code> endpoint in Prometheus format</li> <li>Lifecycle: Metrics initialized lazily on first use, graceful degradation if unavailable</li> <li>Components:</li> <li><code>docling_mcp/metrics.py</code>: Metric definitions and recording functions</li> <li><code>docling_mcp/health.py</code>: Health check logic for database, langfuse, embedder</li> <li><code>docling_mcp/http_server.py</code>: FastAPI server for <code>/metrics</code> and <code>/health</code> endpoints</li> <li>Implementation Guide:</li> </ul> <p>```python   # docling_mcp/server.py - Instrument MCP tools   from docling_mcp.metrics import record_request_start, record_request_end</p> <p>@mcp.tool()   @observe(name=\"query_knowledge_base\")   async def query_knowledge_base(query: str, limit: int = 5):       tool_name = \"query_knowledge_base\"       request_start = record_request_start(tool_name)       status = \"success\"</p> <pre><code>  try:\n      # ... tool logic ...\n      return results\n  except Exception as e:\n      status = \"error\"\n      raise\n  finally:\n      record_request_end(tool_name, request_start, status)\n</code></pre> <p>```</p> <p>Metrics Exposed:</p> Metric Type Labels SLO Alignment <code>mcp_requests_total</code> Counter tool_name, status Request tracking <code>mcp_request_duration_seconds</code> Histogram tool_name &lt;2s p95 <code>rag_embedding_time_seconds</code> Histogram - &lt;500ms <code>rag_db_search_time_seconds</code> Histogram - &lt;100ms <code>rag_llm_generation_time_seconds</code> Histogram - &lt;1.5s <code>mcp_active_requests</code> Gauge - Concurrency monitoring"},{"location":"architecture/#implementation-patterns","title":"Implementation Patterns","text":"<p>These patterns ensure consistent implementation across all AI agents:</p>"},{"location":"architecture/#naming-patterns","title":"Naming Patterns","text":"<p>File Naming:</p> <ul> <li>Python files: <code>snake_case.py</code> (es. <code>rag_service.py</code>, <code>mcp_server.py</code>)</li> <li>Directories: <code>snake_case/</code> (es. <code>mcp/</code>, <code>core/</code>, <code>ingestion/</code>)</li> <li>Test files: <code>test_*.py</code> o <code>*_test.py</code> (es. <code>test_rag_service.py</code>)</li> </ul> <p>Code Naming:</p> <ul> <li>Classes: <code>PascalCase</code> (es. <code>RAGService</code>, <code>EmbeddingGenerator</code>)</li> <li>Functions: <code>snake_case</code> (es. <code>query_knowledge_base</code>, <code>search_knowledge_base_structured</code>)</li> <li>Constants: <code>UPPER_SNAKE_CASE</code> (es. <code>MAX_RETRIES</code>, <code>DEFAULT_LIMIT</code>)</li> <li>Variables: <code>snake_case</code> (es. <code>query_embedding</code>, <code>search_results</code>)</li> </ul> <p>API Endpoints:</p> <ul> <li>REST routes: Plural nouns, lowercase (es. <code>/v1/documents</code>, <code>/v1/search</code>)</li> <li>Route parameters: <code>{document_id}</code> format</li> <li>Query parameters: <code>snake_case</code> (es. <code>source_filter</code>, <code>limit</code>)</li> </ul> <p>Database:</p> <ul> <li>Tables: <code>snake_case</code>, plural (es. <code>documents</code>, <code>chunks</code>)</li> <li>Columns: <code>snake_case</code> (es. <code>document_id</code>, <code>chunk_content</code>)</li> <li>Indexes: <code>idx_&lt;table&gt;_&lt;column&gt;_&lt;type&gt;</code> (es. <code>idx_chunks_embedding_hnsw</code>)</li> </ul>"},{"location":"architecture/#structure-patterns","title":"Structure Patterns","text":"<p>Test Organization:</p> <ul> <li>Unit tests: <code>tests/unit/</code> - Isolated, fast, mocked dependencies</li> <li>Integration tests: <code>tests/integration/</code> - With mocked DB/API, no real external services</li> <li>E2E tests: <code>tests/e2e/</code> - Full system tests with Playwright</li> <li>Fixtures: <code>tests/fixtures/</code> - Shared test data, golden dataset for RAGAS</li> </ul> <p>Component Organization:</p> <ul> <li>By responsibility: <code>mcp/</code> (MCP server), <code>core/</code> (business logic), <code>ingestion/</code> (processing)</li> <li>Shared utilities: <code>utils/</code> (DB, models, providers)</li> <li>Entry points: Root level (<code>app.py</code> for Streamlit, <code>mcp/server.py</code> for MCP)</li> </ul> <p>Script Organization:</p> <ul> <li>Verification: <code>scripts/verification/</code> - Setup/health check scripts</li> <li>Debug: <code>scripts/debug/</code> - Debug utilities</li> <li>Performance: <code>scripts/</code> or <code>tests/performance/</code> - Performance testing</li> </ul>"},{"location":"architecture/#format-patterns","title":"Format Patterns","text":"<p>API Responses:</p> <ul> <li>Format: Direct Pydantic models (no wrapper)</li> <li>Success: Pydantic model instance (es. <code>SearchResponse</code>, <code>IngestResponse</code>)</li> <li>Errors: <code>HTTPException</code> with status codes (400, 404, 500)</li> <li>Date format: ISO 8601 strings (<code>2025-11-26T10:30:00Z</code>)</li> </ul> <p>Error Format:</p> <ul> <li>MCP: <code>ToolError(\"User-friendly message\")</code> for handled errors</li> <li>API: <code>HTTPException(status_code=500, detail=\"Error message\")</code></li> <li>Logging: JSON structured with <code>{\"error\": str, \"context\": dict, \"stack_trace\": str}</code></li> </ul> <p>Date/Time:</p> <ul> <li>Storage: UTC, ISO 8601 format (<code>2025-11-26T10:30:00Z</code>)</li> <li>Display: Locale-aware formatting in UI</li> <li>Library: Standard <code>datetime</code> (no additional dependencies)</li> </ul>"},{"location":"architecture/#communication-patterns","title":"Communication Patterns","text":"<p>MCP Tools:</p> <ul> <li>Tool naming: <code>verb_noun</code> pattern (es. <code>query_knowledge_base</code>, <code>list_knowledge_base_documents</code>)</li> <li>Parameters: <code>snake_case</code>, descriptive names</li> <li>Return: String (formatted) or structured dict for complex data</li> </ul> <p>API Endpoints:</p> <ul> <li>RESTful: Resource-based URLs (<code>/v1/documents</code>, <code>/v1/documents/{id}</code>)</li> <li>Methods: GET (read), POST (create/search), PUT/PATCH (update), DELETE (delete)</li> <li>Status codes: 200 (success), 400 (bad request), 404 (not found), 500 (server error)</li> </ul> <p>LangFuse Tracing:</p> <ul> <li>Decorator: <code>@observe()</code> on critical functions</li> <li>Context: Session ID, metadata via LangFuse context injection</li> <li>Spans: Hierarchical (query \u2192 embedding \u2192 retrieval \u2192 generation)</li> </ul>"},{"location":"architecture/#lifecycle-patterns","title":"Lifecycle Patterns","text":"<p>Loading States:</p> <ul> <li>Pattern: Async initialization with <code>asyncio.Event</code> for readiness</li> <li>Example: Global embedder initialization with <code>_embedder_ready</code> event</li> <li>Error handling: Timeout after 60s, clear error messages</li> </ul> <p>Error Recovery:</p> <ul> <li>Pattern: Exponential backoff with <code>tenacity</code> (max 3 retries)</li> <li>Applicable to: OpenAI API calls, DB connections</li> <li>Non-retryable: HTTP 4xx errors, validation errors</li> </ul> <p>Retry Logic:</p> <ul> <li>Pattern: <code>@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))</code></li> <li>Scope: Transient errors only (timeout, network errors)</li> <li>Logging: Retry attempts logged with attempt number</li> </ul> <p>Session Management:</p> <ul> <li>Streamlit: <code>st.session_state</code> for session_id persistence</li> <li>MCP: Session ID from LangFuse context injection</li> <li>Storage: PostgreSQL or in-memory (per session)</li> </ul>"},{"location":"architecture/#location-patterns","title":"Location Patterns","text":"<p>API Route Structure:</p> <ul> <li>Base: <code>/v1/</code> prefix for versioning</li> <li>Resources: <code>/v1/documents</code>, <code>/v1/search</code>, <code>/v1/ingest</code></li> <li>Health: <code>/health</code> (no version prefix)</li> </ul> <p>Static Assets:</p> <ul> <li>Documentation: <code>docs/</code> directory</li> <li>SQL scripts: <code>sql/</code> directory</li> <li>Config files: Root level (<code>.env.example</code>, <code>pyproject.toml</code>)</li> </ul> <p>Config File Locations:</p> <ul> <li>Environment: <code>.env</code> (gitignored), <code>.env.example</code> (template)</li> <li>Project config: <code>pyproject.toml</code> (UV/Python)</li> <li>Docker: <code>docker-compose.yml</code>, <code>Dockerfile</code></li> <li>CI/CD: <code>.github/workflows/</code></li> </ul>"},{"location":"architecture/#consistency-patterns","title":"Consistency Patterns","text":"<p>Date Formatting:</p> <ul> <li>UI: Locale-aware, human-readable (es. \"26 Nov 2025, 10:30\")</li> <li>API: ISO 8601 strings</li> <li>Logs: ISO 8601 strings for parsing</li> </ul> <p>Logging Format:</p> <ul> <li>Production: JSON structured (<code>{\"timestamp\": \"...\", \"level\": \"INFO\", \"module\": \"...\", \"message\": \"...\", \"context\": {...}}</code>)</li> <li>Development: Text format (readable)</li> <li>Levels: DEBUG (dev), INFO (production), WARNING, ERROR, CRITICAL</li> </ul> <p>User-Facing Errors:</p> <ul> <li>Format: Clear, actionable messages (no stack traces)</li> <li>Context: What went wrong, what user can do</li> <li>Example: \"RAG API Service is unavailable. Please check if the service is running.\"</li> </ul>"},{"location":"architecture/#consistency-rules","title":"Consistency Rules","text":""},{"location":"architecture/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Files: <code>snake_case.py</code></li> <li>Directories: <code>snake_case/</code></li> <li>Classes: <code>PascalCase</code></li> <li>Functions: <code>snake_case</code></li> <li>Constants: <code>UPPER_SNAKE_CASE</code></li> <li>API endpoints: Plural nouns, lowercase</li> <li>Database tables: Plural, <code>snake_case</code></li> </ul>"},{"location":"architecture/#code-organization","title":"Code Organization","text":"<ul> <li>By responsibility: <code>mcp/</code>, <code>core/</code>, <code>ingestion/</code>, <code>utils/</code>, <code>api/</code></li> <li>Tests: <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code></li> <li>Scripts: <code>scripts/verification/</code>, <code>scripts/debug/</code></li> <li>Documentation: <code>docs/</code> (centralized)</li> </ul>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<ul> <li>MCP: <code>ToolError</code> for handled errors, exception wrapping for unexpected</li> <li>API: <code>HTTPException</code> with appropriate status codes</li> <li>Logging: JSON structured with full context</li> <li>User messages: Clear, actionable, no technical jargon</li> </ul>"},{"location":"architecture/#logging-strategy","title":"Logging Strategy","text":"<ul> <li>Format: JSON structured (production), text (development)</li> <li>Levels: DEBUG (dev), INFO (production), WARNING, ERROR, CRITICAL</li> <li>Destination: stdout (Docker-friendly)</li> <li>Context: Include request_id, session_id, user_id when available</li> </ul>"},{"location":"architecture/#data-architecture","title":"Data Architecture","text":""},{"location":"architecture/#database-schema","title":"Database Schema","text":"<p>Tables:</p> <ul> <li><code>documents</code>: Document metadata (id, title, source, created_at, updated_at)</li> <li><code>chunks</code>: Document chunks with embeddings (id, document_id, content, embedding, metadata)</li> <li><code>sessions</code>: Streamlit session tracking (Epic 3) - session_id, query_count, total_cost, total_latency_ms</li> <li><code>query_logs</code>: Query logging per sessione (Epic 3) - session_id, query_text, cost, latency_ms, langfuse_trace_id</li> </ul> <p>Indexes:</p> <ul> <li><code>idx_chunks_embedding_hnsw</code>: HNSW index for vector similarity search (m=16, ef_construction=64)</li> <li><code>idx_documents_source</code>: B-tree index for source filtering</li> <li><code>idx_chunks_document_id</code>: B-tree index for document-chunk relationships</li> <li><code>idx_query_logs_session_id</code>: B-tree index for session query lookups (Epic 3)</li> <li><code>idx_query_logs_timestamp</code>: B-tree index for time-based queries (Epic 3)</li> <li><code>idx_sessions_last_activity</code>: B-tree index for session activity tracking (Epic 3)</li> </ul> <p>Row Level Security (RLS):</p> <ul> <li>All tables in <code>public</code> schema have RLS enabled</li> <li><code>sessions</code> and <code>query_logs</code>: Policies restrict access to <code>service_role</code> only (backend access)</li> <li><code>documents</code> and <code>chunks</code>: RLS enabled, no policies (protected by default, backend access only)</li> </ul> <p>Connection Pool:</p> <ul> <li><code>min_size</code>: 2 (reduced idle overhead)</li> <li><code>max_size</code>: 10 (right-sized for MCP workload)</li> <li><code>statement_cache_size</code>: 100 (prepared statements)</li> <li><code>max_queries</code>: 50000 (connection recycling)</li> </ul>"},{"location":"architecture/#data-models","title":"Data Models","text":"<p>Pydantic Models (<code>utils/models.py</code>):</p> <ul> <li><code>Document</code>: Document metadata</li> <li><code>Chunk</code>: Chunk with embedding</li> <li><code>Session</code>: Session model (generic, Epic 3 uses custom SessionStats model)</li> <li>Request/Response models in <code>api/models.py</code></li> </ul> <p>Epic 3 Models (to be added to <code>utils/models.py</code>):</p> <ul> <li><code>SessionStats</code>: Session statistics (session_id, query_count, total_cost, avg_latency_ms)</li> <li><code>QueryLog</code>: Query log entry (session_id, query_text, cost, latency_ms, langfuse_trace_id)</li> </ul> <p>API Models (<code>api/models.py</code>):</p> <ul> <li><code>SearchRequest</code>: Query, limit, source_filter</li> <li><code>SearchResponse</code>: Results list, count, processing_time_ms</li> <li><code>SearchResult</code>: Content, similarity, source, title, metadata</li> <li><code>IngestRequest</code>: Documents folder, clean flag, fast mode</li> <li><code>IngestResponse</code>: Status, message, task_id</li> </ul>"},{"location":"architecture/#api-contracts","title":"API Contracts","text":""},{"location":"architecture/#search-endpoint","title":"Search Endpoint","text":"<p>POST <code>/v1/search</code></p> <ul> <li>Request: <code>SearchRequest</code> (query: str, limit: int, source_filter: Optional[str])</li> <li>Response: <code>SearchResponse</code> (results: List[SearchResult], count: int, processing_time_ms: float)</li> <li>Errors: 400 (bad request), 500 (server error)</li> <li>Timing: Breakdown in response (embedding_ms, db_ms, total_ms)</li> </ul>"},{"location":"architecture/#documents-endpoint","title":"Documents Endpoint","text":"<p>GET <code>/v1/documents</code></p> <ul> <li>Query Params: <code>limit</code> (default: 100), <code>offset</code> (default: 0)</li> <li>Response: <code>{\"documents\": [...], \"count\": int}</code></li> <li>Errors: 500 (server error)</li> </ul> <p>GET <code>/v1/documents/{document_id}</code></p> <ul> <li>Path Params: <code>document_id</code> (UUID)</li> <li>Response: Document object with full content</li> <li>Errors: 404 (not found), 500 (server error)</li> </ul>"},{"location":"architecture/#overview-endpoint","title":"Overview Endpoint","text":"<p>GET <code>/v1/overview</code></p> <ul> <li>Response: <code>{\"total_documents\": int, \"total_chunks\": int, \"unique_sources\": int, \"sources\": [...], \"documents\": [...]}</code></li> <li>Errors: 500 (server error)</li> </ul>"},{"location":"architecture/#health-check","title":"Health Check","text":"<p>GET <code>/health</code></p> <ul> <li>Response: <code>{\"status\": \"ok\", \"timestamp\": float}</code></li> <li>Purpose: Service availability check</li> </ul>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>Pattern: No user authentication (RAG system, no user management)</li> <li>API Keys: Environment variables (<code>OPENAI_API_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>)</li> <li>Protection: Keys never logged, never committed to git</li> <li>Validation: Secret scanning in CI/CD (TruffleHog)</li> </ul>"},{"location":"architecture/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption: PostgreSQL connection string encrypted in production</li> <li>Secrets Management: Environment variables, <code>.env</code> file (gitignored)</li> <li>Logging: No secrets in logs, masking enabled for sensitive data</li> </ul>"},{"location":"architecture/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Secret Scanning: TruffleHog OSS on every PR</li> <li>Dependency Scanning: Regular dependency updates, security advisories</li> <li>Input Validation: Pydantic models for all API inputs</li> <li>Error Messages: No sensitive information in user-facing errors</li> </ul>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#latency-targets-from-nfrs","title":"Latency Targets (from NFRs)","text":"<ul> <li>MCP Query: &lt; 2s (95th percentile)</li> <li>Embedding Generation: &lt; 500ms per batch (100 chunks)</li> <li>DB Vector Search: &lt; 100ms per query (with HNSW index)</li> <li>Throughput: 50 queries/second with &lt;10% degradation</li> </ul>"},{"location":"architecture/#optimization-strategies","title":"Optimization Strategies","text":"<p>Global Embedder:</p> <ul> <li>Singleton pattern, initialized once at startup</li> <li>Persistent cache (2000 entries) across requests</li> <li>Eliminates 300-500ms overhead per query</li> </ul> <p>Database:</p> <ul> <li>HNSW index for vector similarity (10-100x faster than IVFFlat)</li> <li>Connection pooling (2-10 connections, right-sized)</li> <li>Prepared statements caching (100 statements)</li> </ul> <p>Caching:</p> <ul> <li>Embedding cache: LRU cache, 2000 entries</li> <li>Query results: Not cached (always fresh, semantic search)</li> </ul>"},{"location":"architecture/#scalability","title":"Scalability","text":"<ul> <li>Horizontal Scaling: Supported via load balancer (future)</li> <li>Connection Pool: Dynamic (2-10 connections)</li> <li>Database: PostgreSQL with PGVector (scales with hardware)</li> </ul>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/#docker-configuration","title":"Docker Configuration","text":"<p>Streamlit Container (<code>Dockerfile</code>):</p> <ul> <li>Base: <code>python:3.11-slim-bookworm</code></li> <li>UV: Copied from official image</li> <li>Multi-stage: Dependency installation \u2192 code copy \u2192 final image</li> <li>Size: &lt; 500MB target</li> <li>Health check: <code>/_stcore/health</code> endpoint</li> </ul> <p>API Container (<code>Dockerfile.api</code>):</p> <ul> <li>Base: <code>python:3.11-slim-bookworm</code></li> <li>FastAPI + Uvicorn</li> <li>Health check: <code>/health</code> endpoint</li> </ul> <p>Docker Compose:</p> <ul> <li>Services: Streamlit app, PostgreSQL (optional), LangFuse (optional)</li> <li>Networks: Internal network for service communication</li> <li>Volumes: Document storage, database persistence</li> </ul>"},{"location":"architecture/#environment-configuration","title":"Environment Configuration","text":"<p>Required Variables:</p> <ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key</li> <li><code>DATABASE_URL</code>: PostgreSQL connection string</li> <li><code>LANGFUSE_PUBLIC_KEY</code>: LangFuse public key (optional)</li> <li><code>LANGFUSE_SECRET_KEY</code>: LangFuse secret key (optional)</li> <li><code>LANGFUSE_BASE_URL</code>: LangFuse server URL (optional, defaults to cloud)</li> </ul> <p>Optional Variables:</p> <ul> <li><code>LLM_CHOICE</code>: LLM model (default: <code>gpt-4o-mini</code>)</li> <li><code>EMBEDDING_MODEL</code>: Embedding model (default: <code>text-embedding-3-small</code>)</li> <li><code>DEBUG_MODE</code>: Enable debug logging (default: <code>false</code>)</li> </ul> <p>Epic 3 Security Hardening (Optional):</p> <ul> <li><code>COST_DAILY_LIMIT</code>: Daily cost limit in USD (default: <code>10.00</code>)</li> <li><code>COST_HOURLY_LIMIT</code>: Hourly cost limit in USD (default: <code>2.00</code>)</li> <li><code>COST_ALERT_THRESHOLD</code>: Cost alert threshold in USD (default: <code>5.00</code>)</li> <li><code>STREAMLIT_PASSWORD_HASH</code>: SHA256 hash for Streamlit authentication (optional)</li> <li><code>REDIS_URL</code>: Redis connection URL for persistent rate limiting (optional, default: <code>redis://localhost:6379</code>)</li> </ul>"},{"location":"architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>GitHub Actions Workflow (<code>.github/workflows/ci.yml</code>):</p> <p>Il CI/CD pipeline completo \u00e8 documentato in dettaglio in: - Technical Specification: <code>docs/stories/4/tech-spec-epic-4.md</code> - Setup Guide: <code>docs/stories/4/epic-4-setup-guide.md</code></p> <p>Pipeline Components:</p> <pre><code>name: CI\n\non:\n  pull_request:\n    branches: [main, develop]\n  push:\n    branches: [main, develop]\n\njobs:\n  lint:\n    - ruff check (zero warnings)\n    - ruff format check\n  type-check:\n    - mypy (zero errors)\n  test:\n    - pytest with coverage &gt;70%\n    - Fail build if coverage &lt;70%\n    - Upload coverage report artifacts\n  build:\n    - Docker build test (Streamlit + API)\n    - Verify image sizes &lt;500MB\n  secret-scan:\n    - TruffleHog OSS scan (full git history)\n    - Fail build if secrets detected\n</code></pre> <p>Release Workflow (<code>.github/workflows/release.yml</code>):</p> <ul> <li>Trigger: Git tag creation (<code>v*.*.*</code>)</li> <li>Actions: Update CHANGELOG, create GitHub Release</li> <li>Reference: <code>docs/stories/4/epic-4-setup-guide.md</code> (Step 6)</li> </ul> <p>Security Scanning:</p> <ul> <li>TruffleHog OSS: Automatic secret scanning su ogni PR/push</li> <li>CodeRabbit: AI-powered code review automatica</li> <li>References: </li> <li>TruffleHog: https://github.com/marketplace/actions/trufflehog-oss</li> <li>CodeRabbit: https://docs.coderabbit.ai/platforms/github-com</li> </ul>"},{"location":"architecture/#development-environment","title":"Development Environment","text":""},{"location":"architecture/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: 3.11+ (3.10+ minimum)</li> <li>UV: Latest version (package manager)</li> <li>PostgreSQL: 16+ with PGVector extension</li> <li>Docker: Latest (for containerized development)</li> <li>Git: Latest (for version control)</li> </ul>"},{"location":"architecture/#setup-commands","title":"Setup Commands","text":"<pre><code># Install dependencies\nuv sync\n\n# Setup environment\ncp .env.example .env\n# Edit .env with your API keys and database URL\n\n# Initialize database\npsql $DATABASE_URL &lt; sql/optimize_index.sql\n\n# Run Streamlit app\nstreamlit run app.py\n\n# Run MCP server (for Cursor/Claude Desktop)\nuv run python -m mcp.server\n\n# Run tests\npytest --cov=core --cov=ingestion --cov=mcp --cov-report=html\n\n# Run linting\nruff check .\n\n# Run type checking\nmypy .\n</code></pre>"},{"location":"architecture/#git-workflow","title":"Git Workflow","text":"<p>Branching Strategy:</p> <ul> <li><code>main</code>: Production-ready code</li> <li><code>develop</code>: Integration branch</li> <li><code>feature/*</code>: Feature development</li> <li><code>hotfix/*</code>: Urgent production fixes</li> </ul> <p>Commit Conventions:</p> <ul> <li>Format: <code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;</code></li> <li>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>refactor</code>, <code>test</code>, <code>chore</code></li> <li>Example: <code>feat(mcp): add LangFuse tracing to query_knowledge_base</code></li> </ul> <p>Pull Request Process:</p> <ol> <li>Create feature branch from <code>develop</code></li> <li>Implement changes with tests</li> <li>Ensure CI passes (lint, type-check, test, secret-scan)</li> <li>CodeRabbit review (automatic)</li> <li>Manual review (if needed)</li> <li>Merge to <code>develop</code>, then <code>main</code></li> </ol>"},{"location":"architecture/#versionamento","title":"Versionamento","text":"<p>Semantic Versioning:</p> <ul> <li>Format: <code>MAJOR.MINOR.PATCH</code> (es. <code>0.1.0</code>)</li> <li>Current: <code>0.1.0</code> (in <code>pyproject.toml</code>)</li> <li>Changelog: <code>CHANGELOG.md</code> with Keep a Changelog format</li> <li>Tagging: Git tags <code>v0.1.0</code> for each release</li> </ul> <p>Release Process:</p> <ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code> with changes</li> <li>Create git tag: <code>git tag v0.1.0</code></li> <li>Push tag: <code>git push origin v0.1.0</code></li> <li>GitHub Actions creates release automatically</li> <li>Set <code>LANGFUSE_RELEASE</code> env var with tag</li> </ol>"},{"location":"architecture/#architecture-decision-records-adrs","title":"Architecture Decision Records (ADRs)","text":""},{"location":"architecture/#adr-001-langfuse-integration-pattern","title":"ADR-001: LangFuse Integration Pattern","text":"<p>Status: Accepted Date: 2025-11-26 Verified: 2025-11-26 Context: Need observability for MCP server operations with cost tracking and performance metrics.</p> <p>Decision: Use LangFuse decorator-based pattern (<code>@observe()</code>) with <code>langfuse.openai</code> wrapper for automatic cost tracking.</p> <p>Rationale:</p> <ul> <li>Minimal code changes required</li> <li>Automatic hierarchical trace structure</li> <li>Cost tracking built-in with current OpenAI pricing</li> <li>Graceful degradation if LangFuse unavailable</li> <li>OpenTelemetry-based (v3) enables third-party library integration</li> </ul> <p>Implementation:</p> <pre><code># Step 1: Initialize LangFuse client (once at startup)\nfrom langfuse import get_client\n\nlangfuse = get_client()  # Uses env vars: LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY\n\n# Step 2: Decorate critical functions\nfrom langfuse import observe\n\n@observe(name=\"search_knowledge_base\")\nasync def search_knowledge_base_structured(query: str, limit: int = 5):\n    # Automatic tracing: inputs, outputs, timing, errors\n    pass\n\n# Step 3: Use langfuse.openai wrapper for LLM calls\nfrom langfuse.openai import openai\n\nresponse = await openai.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": query}]\n)\n# Automatic cost tracking and token usage\n\n# Step 4: Set trace attributes (user_id, session_id)\nfrom langfuse import propagate_attributes\n\nwith propagate_attributes(user_id=\"user_123\", session_id=\"session_abc\"):\n    # All child observations inherit these attributes\n    result = await search_knowledge_base_structured(query)\n</code></pre> <p>Consequences:</p> <ul> <li>All critical functions must use <code>@observe()</code> decorator</li> <li>LangFuse SDK v3.0.0+ required (OTel-based)</li> <li>Environment variables needed: <code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_HOST</code> (optional)</li> <li>Use <code>langfuse.openai</code> wrapper instead of direct <code>openai</code> import for cost tracking</li> <li>Trace attributes must be set via <code>propagate_attributes()</code> context manager (not directly on calls)</li> </ul>"},{"location":"architecture/#adr-002-mcp-server-standalone-architecture","title":"ADR-002: MCP Server Standalone Architecture","text":"<p>Status: Accepted Date: 2025-11-26 Verified: 2025-11-26 Context: MCP server currently depends on external API server, causing reliability issues.</p> <p>Decision: Refactor MCP server to use <code>core/rag_service.py</code> directly, eliminating HTTP dependency.</p> <p>Rationale:</p> <ul> <li>Eliminates external dependency</li> <li>Reduces latency (no HTTP overhead)</li> <li>Simpler architecture, easier debugging</li> <li>Aligns with existing design (core logic decoupled)</li> </ul> <p>Implementation:</p> <pre><code># mcp/server.py\nfrom fastmcp import FastMCP\nfrom contextlib import asynccontextmanager\n\nmcp = FastMCP(\"docling-rag-agent\")\n\n@asynccontextmanager\nasync def lifespan(app: FastMCP):\n    \"\"\"FastMCP lifespan pattern for startup/shutdown.\"\"\"\n    # Startup: Initialize resources\n    from utils.db_utils import init_db_pool\n    from ingestion.embedder import init_embedder\n\n    await init_db_pool()\n    await init_embedder()\n\n    yield  # Server runs here\n\n    # Shutdown: Cleanup resources\n    from utils.db_utils import close_db_pool\n    await close_db_pool()\n\nmcp.lifespan = lifespan\n\n# Direct import from core (no HTTP)\nfrom core.rag_service import search_knowledge_base_structured\n\n@mcp.tool\nasync def query_knowledge_base(query: str, limit: int = 5):\n    \"\"\"Query knowledge base with direct function call.\"\"\"\n    try:\n        return await search_knowledge_base_structured(query, limit)\n    except Exception as e:\n        from mcp.server import ToolError\n        raise ToolError(f\"Failed to query knowledge base: {str(e)}\")\n</code></pre> <p>Breaking Changes (FastMCP 0.4.x+):</p> <ul> <li>FastMCP 0.4.x+ uses lifespan pattern instead of startup/shutdown hooks</li> <li>Tool error handling uses <code>ToolError</code> instead of generic exceptions</li> <li>Context injection via <code>Context</code> parameter (type-hinted) instead of global state</li> </ul> <p>Consequences:</p> <ul> <li>MCP server must initialize DB pool and embedder at startup via lifespan</li> <li>Requires FastMCP lifespan management pattern (<code>@asynccontextmanager</code>)</li> <li><code>client/api_client.py</code> no longer needed for MCP (kept for other use cases)</li> <li>Testing requires FastMCP Client with in-memory transport for unit tests</li> </ul>"},{"location":"architecture/#adr-003-tdd-structure-rigorosa","title":"ADR-003: TDD Structure Rigorosa","text":"<p>Status: Accepted Date: 2025-11-26 Context: Need comprehensive testing infrastructure for production-ready system.</p> <p>Decision: Implement TDD structure with <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code>, coverage &gt;70% enforcement.</p> <p>Rationale:</p> <ul> <li>Prevents regressions</li> <li>Ensures code quality</li> <li>RAGAS evaluation for RAG quality</li> <li>Playwright E2E tests for user workflows</li> </ul> <p>Consequences:</p> <ul> <li>All new code must have tests first (Red-Green-Refactor)</li> <li>CI/CD fails if coverage &lt;70%</li> <li>Golden dataset required for RAGAS evaluation (20+ pairs)</li> </ul>"},{"location":"architecture/#adr-004-git-workflow-cicd","title":"ADR-004: Git Workflow &amp; CI/CD","text":"<p>Status: Accepted Date: 2025-11-26 Context: Need automated quality gates and security scanning for production deployment.</p> <p>Decision: Git Flow + Conventional Commits + GitHub Actions CI/CD + TruffleHog secret scanning + CodeRabbit code review.</p> <p>Rationale:</p> <ul> <li>Automated quality checks prevent bad code from merging</li> <li>Secret scanning prevents credential leaks</li> <li>CodeRabbit provides AI-powered code review</li> <li>Semantic versioning enables release tracking</li> </ul> <p>Consequences:</p> <ul> <li>All commits must follow Conventional Commits format</li> <li>PRs must pass CI/CD before merging</li> <li>Secrets detected in PRs cause build failure</li> <li>CodeRabbit reviews every PR automatically</li> </ul>"},{"location":"architecture/#adr-005-prometheus-metrics-and-health-check-endpoints","title":"ADR-005: Prometheus Metrics and Health Check Endpoints","text":"<p>Status: Accepted Date: 2025-11-27 Context: Need production-grade monitoring with standard Prometheus metrics and health check endpoints for Kubernetes/Docker orchestration.</p> <p>Decision: Implement Prometheus metrics via <code>prometheus_client</code> library and JSON health check endpoint via FastAPI.</p> <p>Rationale:</p> <ul> <li>Prometheus is industry standard for container monitoring</li> <li>Health check endpoints enable Kubernetes liveness/readiness probes</li> <li>Graceful degradation maintains service availability</li> <li>Histogram buckets aligned with SLO targets for meaningful alerting</li> </ul> <p>Implementation:</p> <pre><code># docling_mcp/metrics.py\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Request counter (labeled by tool and status)\nmcp_requests_total = Counter(\n    \"mcp_requests_total\",\n    \"Total MCP requests\",\n    [\"tool_name\", \"status\"]\n)\n\n# Request duration histogram (SLO: &lt;2s p95)\nmcp_request_duration_seconds = Histogram(\n    \"mcp_request_duration_seconds\",\n    \"Request latency\",\n    [\"tool_name\"],\n    buckets=[0.1, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n)\n\n# RAG-specific histograms\nrag_embedding_time_seconds = Histogram(\n    \"rag_embedding_time_seconds\",\n    \"Embedding generation time\",\n    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 1.0]  # SLO: &lt;500ms\n)\n\nrag_db_search_time_seconds = Histogram(\n    \"rag_db_search_time_seconds\",\n    \"Database search time\",\n    buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0]  # SLO: &lt;100ms\n)\n</code></pre> <pre><code># docling_mcp/health.py\nasync def get_health_status() -&gt; HealthResponse:\n    \"\"\"\n    Status logic:\n    - ok: All services UP\n    - degraded: LangFuse DOWN (non-critical)\n    - down: Database or Embedder DOWN (critical)\n    \"\"\"\n    db_status = await check_database()\n    langfuse_status = check_langfuse()\n    embedder_status = await check_embedder()\n\n    if db_status.status == \"down\" or embedder_status.status == \"down\":\n        overall_status = \"down\"\n    elif langfuse_status.status == \"down\":\n        overall_status = \"degraded\"\n    else:\n        overall_status = \"ok\"\n\n    return HealthResponse(status=overall_status, ...)\n</code></pre> <p>Endpoints:</p> Endpoint Port Content-Type Purpose <code>/metrics</code> 8080 application/openmetrics-text Prometheus scraping <code>/health</code> 8080 application/json Kubernetes probes <p>Prometheus Configuration (recommended <code>scrape_interval</code>):</p> <ul> <li>15s: Default for real-time monitoring</li> <li>60s: Cost-sensitive deployments (reduced alert responsiveness)</li> </ul> <p>Consequences:</p> <ul> <li>HTTP server (<code>docling_mcp/http_server.py</code>) runs on port 8080 (configurable via <code>METRICS_PORT</code>)</li> <li>Metrics recording wrapped in try/except for graceful degradation</li> <li>Health check status \"degraded\" returns HTTP 200 (service still functional)</li> <li>Health check status \"down\" returns HTTP 503 (service unavailable)</li> </ul>"},{"location":"architecture/#references","title":"References","text":""},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Coding Standards: Complete code style guide, naming conventions, documentation standards, error handling patterns, and best practices based on existing codebase patterns.</li> <li>Testing Strategy: Comprehensive testing strategy with TDD workflow, test organization (unit/integration/e2e), RAGAS evaluation, and CI/CD integration.</li> <li>Unified Project Structure: Standardized directory structure, file organization rules, epic-to-directory mapping, and validation checklist.</li> <li>Development Guide: Setup instructions, development workflow, database operations, and troubleshooting.</li> <li>Epics Breakdown: Complete epic and story breakdown with acceptance criteria and technical notes.</li> <li>PRD: Product Requirements Document with functional requirements inventory.</li> </ul>"},{"location":"architecture/#external-references","title":"External References","text":"<ul> <li>LangFuse Documentation: https://langfuse.com/docs</li> <li>FastMCP Documentation: https://github.com/jlowin/fastmcp</li> <li>PydanticAI Documentation: https://ai.pydantic.dev</li> <li>Prometheus Best Practices: https://prometheus.io/docs/practices/histograms/</li> <li>PostgreSQL PGVector: https://github.com/pgvector/pgvector</li> </ul> <p>Generated by BMAD Decision Architecture Workflow v1.0 Date: 2025-11-26 Updated: 2025-11-27 For: Stefano</p>"},{"location":"coding-standards/","title":"Coding Standards - docling-rag-agent","text":"<p>Version: 1.0 Last Updated: 2025-01-27 Python Version: &gt;=3.10</p>"},{"location":"coding-standards/#overview","title":"Overview","text":"<p>Questo documento definisce gli standard di codice per il progetto <code>docling-rag-agent</code>. Gli standard riflettono le pratiche consolidate nel codice esistente e garantiscono coerenza, manutenibilit\u00e0 e qualit\u00e0 del codice.</p> <p>Principi Fondamentali:</p> <ul> <li>Type Safety: Type hints obbligatori per tutte le funzioni pubbliche</li> <li>Documentation First: Docstrings completi con formato standardizzato</li> <li>Graceful Degradation: Gestione robusta di dipendenze opzionali</li> <li>Observability: Logging strutturato e tracing integrato</li> <li>Test-Driven: Coverage minimo 70% con test organizzati rigorosamente</li> </ul>"},{"location":"coding-standards/#1-python-style-guide","title":"1. Python Style Guide","text":""},{"location":"coding-standards/#11-formattazione","title":"1.1 Formattazione","text":"<p>Line Length:</p> <ul> <li>Massimo 100 caratteri per riga (configurato in <code>pyproject.toml</code>)</li> <li>Usare parentesi per continuare su pi\u00f9 righe quando necessario</li> </ul> <p>Indentazione:</p> <ul> <li>4 spazi (no tab)</li> <li>Allineamento per parametri multipli quando necessario</li> </ul> <p>Esempio:</p> <pre><code># \u2705 Corretto\nasync def search_knowledge_base(\n    query: str,\n    limit: int = 5,\n    source_filter: Optional[str] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Search knowledge base with optional source filtering.\"\"\"\n    pass\n\n# \u274c Errato\nasync def search_knowledge_base(query: str, limit: int = 5, source_filter: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"coding-standards/#12-import-organization","title":"1.2 Import Organization","text":"<p>Ordine degli import:</p> <ol> <li>Standard library</li> <li>Third-party packages</li> <li>Local application imports</li> </ol> <p>Separazione:</p> <ul> <li>Una riga vuota tra ogni gruppo</li> <li>Import assoluti preferiti (no relative imports)</li> </ul> <p>Esempio:</p> <pre><code># Standard library\nimport logging\nimport time\nfrom typing import List, Dict, Any, Optional\nfrom contextlib import asynccontextmanager\n\n# Third-party\nfrom fastmcp import FastMCP\nfrom langfuse import observe\n\n# Local imports\nfrom core.rag_service import search_knowledge_base_structured\nfrom docling_mcp.metrics import record_request_start, record_request_end\n</code></pre>"},{"location":"coding-standards/#13-type-hints","title":"1.3 Type Hints","text":"<p>Obbligatori per:</p> <ul> <li>Tutte le funzioni pubbliche (non private)</li> <li>Parametri di funzione</li> <li>Valori di ritorno</li> <li>Variabili di classe pubbliche</li> </ul> <p>Optional e Union:</p> <ul> <li>Usare <code>Optional[T]</code> invece di <code>Union[T, None]</code></li> <li>Usare <code>Literal</code> per valori fissi (es. status codes)</li> </ul> <p>Esempio:</p> <pre><code>from typing import List, Dict, Any, Optional, Literal\n\n# \u2705 Corretto\nasync def check_database() -&gt; ServiceStatus:\n    \"\"\"Check database connectivity.\"\"\"\n    pass\n\n@dataclass\nclass HealthResponse:\n    status: Literal[\"ok\", \"degraded\", \"down\"]\n    timestamp: float\n    services: Dict[str, Dict[str, Any]]\n\n# \u274c Errato\nasync def check_database():\n    pass\n</code></pre>"},{"location":"coding-standards/#2-naming-conventions","title":"2. Naming Conventions","text":""},{"location":"coding-standards/#21-file-e-directory","title":"2.1 File e Directory","text":"<p>File Python:</p> <ul> <li><code>snake_case.py</code> (es. <code>rag_service.py</code>, <code>mcp_server.py</code>)</li> <li>Test files: <code>test_*.py</code> o <code>*_test.py</code> (es. <code>test_rag_service.py</code>)</li> </ul> <p>Directory:</p> <ul> <li><code>snake_case/</code> (es. <code>docling_mcp/</code>, <code>core/</code>, <code>ingestion/</code>)</li> </ul> <p>Esempio:</p> <pre><code>docling_mcp/\n\u251c\u2500\u2500 server.py\n\u251c\u2500\u2500 metrics.py\n\u251c\u2500\u2500 health.py\n\u2514\u2500\u2500 tools/\n    \u251c\u2500\u2500 search.py\n    \u2514\u2500\u2500 documents.py\n</code></pre>"},{"location":"coding-standards/#22-codice","title":"2.2 Codice","text":"<p>Classi:</p> <ul> <li><code>PascalCase</code> (es. <code>EmbeddingGenerator</code>, <code>DatabasePool</code>, <code>ServiceStatus</code>)</li> </ul> <p>Funzioni e Variabili:</p> <ul> <li><code>snake_case</code> (es. <code>query_knowledge_base</code>, <code>search_results</code>, <code>embedding_time</code>)</li> </ul> <p>Costanti:</p> <ul> <li><code>UPPER_SNAKE_CASE</code> (es. <code>MAX_RETRIES</code>, <code>DEFAULT_LIMIT</code>, <code>CACHE_SIZE</code>)</li> </ul> <p>Private/Internal:</p> <ul> <li>Prefisso <code>_</code> per funzioni/variabili private (es. <code>_global_embedder</code>, <code>_initialize_metrics()</code>)</li> </ul> <p>Esempio:</p> <pre><code># Costanti\nMAX_RETRIES = 3\nDEFAULT_BATCH_SIZE = 100\n\n# Classe pubblica\nclass EmbeddingGenerator:\n    # Variabile privata\n    _cache: Dict[str, List[float]]\n\n    # Metodo pubblico\n    async def embed_query(self, text: str) -&gt; List[float]:\n        pass\n\n    # Metodo privato\n    def _get_from_cache(self, text: str) -&gt; Optional[List[float]]:\n        pass\n</code></pre>"},{"location":"coding-standards/#23-database","title":"2.3 Database","text":"<p>Tabelle:</p> <ul> <li><code>snake_case</code>, plurale (es. <code>documents</code>, <code>chunks</code>)</li> </ul> <p>Colonne:</p> <ul> <li><code>snake_case</code> (es. <code>document_id</code>, <code>chunk_content</code>, <code>created_at</code>)</li> </ul> <p>Indici:</p> <ul> <li><code>idx_&lt;table&gt;_&lt;column&gt;_&lt;type&gt;</code> (es. <code>idx_chunks_embedding_hnsw</code>)</li> </ul>"},{"location":"coding-standards/#3-documentation-standards","title":"3. Documentation Standards","text":""},{"location":"coding-standards/#31-module-docstrings","title":"3.1 Module Docstrings","text":"<p>Ogni modulo deve iniziare con un docstring che descrive:</p> <ul> <li>Scopo del modulo</li> <li>Architettura/pattern utilizzati</li> <li>Componenti principali</li> </ul> <p>Esempio:</p> <pre><code>\"\"\"\nMCP Server\n==========\nExposes the RAG agent capabilities as a Model Context Protocol (MCP) server.\nCompatible with Cursor, Claude Desktop, and other MCP clients.\n\nArchitecture:\n- Standalone server with direct service integration (no HTTP proxy)\n- Uses core/rag_service.py directly for RAG operations\n- LangFuse integration for observability tracing (graceful degradation if unavailable)\n- Cost tracking via langfuse.openai wrapper in embedder (automatic token/cost calculation)\n- Prometheus metrics for performance monitoring (/metrics endpoint)\n- Health check endpoint (/health) for service status monitoring\n\"\"\"\n</code></pre>"},{"location":"coding-standards/#32-function-docstrings","title":"3.2 Function Docstrings","text":"<p>Formato standardizzato:</p> <ul> <li>Breve descrizione (prima riga)</li> <li>Sezione <code>Args:</code> per parametri</li> <li>Sezione <code>Returns:</code> per valore di ritorno</li> <li>Sezione <code>Yields:</code> per generatori</li> <li>Sezione <code>Raises:</code> per eccezioni</li> <li>Sezione <code>Note:</code> per informazioni aggiuntive</li> </ul> <p>Esempio:</p> <pre><code>async def langfuse_span(\n    name: str,\n    span_type: str = \"span\",\n    metadata: dict = None\n) -&gt; AsyncGenerator[Any, None]:\n    \"\"\"\n    Create a nested LangFuse span for cost tracking and timing breakdown.\n\n    Args:\n        name: Name of the span (e.g., \"embedding-generation\", \"vector-search\")\n        span_type: Type of span (\"span\" for general, \"generation\" for LLM calls)\n        metadata: Optional metadata to attach to the span\n\n    Yields:\n        A dict with 'span' (LangFuse span or None) and 'start_time' for timing.\n\n    Note:\n        - Gracefully degrades to no-op if LangFuse unavailable\n        - Always records timing in span metadata (duration_ms)\n        - Also records to Prometheus metrics for embedding and db_search spans\n    \"\"\"\n    pass\n</code></pre>"},{"location":"coding-standards/#33-class-docstrings","title":"3.3 Class Docstrings","text":"<p>Formato:</p> <ul> <li>Descrizione della classe</li> <li>Sezione <code>Attributes:</code> per attributi pubblici importanti</li> <li>Sezione <code>Example:</code> se utile</li> </ul> <p>Esempio:</p> <pre><code>class EmbeddingGenerator(BaseEmbedder):\n    \"\"\"\n    Generates embeddings using OpenAI compatible API.\n\n    Cost Tracking:\n        Uses langfuse.openai wrapper when available for automatic cost tracking.\n        Falls back to direct OpenAI client if LangFuse unavailable.\n\n    Attributes:\n        model_name: OpenAI model name (default: \"text-embedding-3-small\")\n        batch_size: Batch size for embedding generation (default: 100)\n        use_cache: Enable in-memory cache for embeddings (default: True)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"coding-standards/#34-inline-comments","title":"3.4 Inline Comments","text":"<p>Quando usare:</p> <ul> <li>Spiegare \"perch\u00e9\" non \"cosa\" (il codice dovrebbe essere auto-esplicativo)</li> <li>Sezioni complesse o ottimizzazioni non ovvie</li> <li>Workaround o limitazioni note</li> </ul> <p>Esempio:</p> <pre><code># OPTIMIZED: Runs in background to prevent blocking server startup (MCP handshake timeout)\nasync def initialize_global_embedder():\n    \"\"\"Initialize global embedder instance at server startup.\"\"\"\n    # Offload heavy import and creation to thread to avoid blocking asyncio loop\n    # This is critical because importing transformers/docling takes ~40s\n    _global_embedder = await asyncio.to_thread(_create_embedder_sync)\n</code></pre>"},{"location":"coding-standards/#4-error-handling","title":"4. Error Handling","text":""},{"location":"coding-standards/#41-graceful-degradation","title":"4.1 Graceful Degradation","text":"<p>Pattern obbligatorio per dipendenze opzionali:</p> <ul> <li>Try/except per import</li> <li>Fallback a implementazione no-op o alternativa</li> <li>Logging informativo</li> </ul> <p>Esempio:</p> <pre><code># LangFuse integration with graceful degradation\ntry:\n    from langfuse import observe, get_client as get_langfuse_client\n    _langfuse_available = True\nexcept ImportError:\n    _langfuse_available = False\n    get_langfuse_client = None\n    # Create no-op decorator as fallback\n    def observe(name=None, **kwargs):\n        def decorator(func):\n            return func\n        return decorator\n    logger.info(\"LangFuse SDK not installed, tracing disabled\")\n</code></pre>"},{"location":"coding-standards/#42-exception-handling","title":"4.2 Exception Handling","text":"<p>Best Practices:</p> <ul> <li>Catturare eccezioni specifiche quando possibile</li> <li>Logging con contesto (error message, stack trace se necessario)</li> <li>Re-raise dopo logging se necessario</li> <li>Usare <code>ToolError</code> per errori user-facing in MCP tools</li> </ul> <p>Esempio:</p> <pre><code>from fastmcp import ToolError\n\nasync def query_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"Query knowledge base with error handling.\"\"\"\n    try:\n        results = await search_knowledge_base_structured(query, limit)\n        return format_results(results)\n    except ValueError as e:\n        logger.error(f\"Invalid query parameters: {e}\")\n        raise ToolError(f\"Invalid query: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error in query_knowledge_base: {e}\", exc_info=True)\n        raise ToolError(\"An error occurred while querying the knowledge base\")\n</code></pre>"},{"location":"coding-standards/#43-retry-logic","title":"4.3 Retry Logic","text":"<p>Quando usare:</p> <ul> <li>Operazioni di rete (API calls, DB connections)</li> <li>Operazioni transient che possono fallire temporaneamente</li> </ul> <p>Pattern:</p> <ul> <li>Usare <code>tenacity</code> per retry con exponential backoff</li> <li>Configurare max tentativi e delay appropriati</li> </ul> <p>Esempio:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10)\n)\nasync def generate_embedding(text: str) -&gt; List[float]:\n    \"\"\"Generate embedding with retry logic for transient errors.\"\"\"\n    try:\n        response = await client.embeddings.create(\n            model=self.model_name,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        logger.warning(f\"Embedding generation failed (will retry): {e}\")\n        raise\n</code></pre>"},{"location":"coding-standards/#5-logging","title":"5. Logging","text":""},{"location":"coding-standards/#51-logger-setup","title":"5.1 Logger Setup","text":"<p>Pattern standard:</p> <ul> <li>Un logger per modulo con <code>__name__</code></li> <li>Configurazione centralizzata (non per-modulo)</li> </ul> <p>Esempio:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n# Non configurare logging qui - viene fatto a livello applicazione\n# logging.basicConfig(level=logging.INFO)  # \u274c Non fare questo\n</code></pre>"},{"location":"coding-standards/#52-log-levels","title":"5.2 Log Levels","text":"<p>DEBUG:</p> <ul> <li>Informazioni dettagliate per debugging</li> <li>Non usare in produzione</li> </ul> <p>INFO:</p> <ul> <li>Eventi significativi dell'applicazione (startup, shutdown, operazioni principali)</li> <li>Metriche importanti</li> </ul> <p>WARNING:</p> <ul> <li>Situazioni anomale ma non critiche</li> <li>Fallback a funzionalit\u00e0 alternative</li> </ul> <p>ERROR:</p> <ul> <li>Errori che impediscono un'operazione ma non crashano l'applicazione</li> <li>Include stack trace quando utile</li> </ul> <p>CRITICAL:</p> <ul> <li>Errori critici che possono causare crash</li> <li>Raramente usato</li> </ul> <p>Esempio:</p> <pre><code>logger.debug(f\"Processing query: {query[:50]}...\")  # Debug info\nlogger.info(\"Global embedder initialized successfully\")  # Significant event\nlogger.warning(\"LangFuse unavailable, continuing without tracing\")  # Degradation\nlogger.error(f\"Database connection failed: {e}\", exc_info=True)  # Error with context\n</code></pre>"},{"location":"coding-standards/#53-structured-logging","title":"5.3 Structured Logging","text":"<p>Quando possibile:</p> <ul> <li>Usare formattazione strutturata per parsing automatico</li> <li>Includere contesto rilevante (request_id, tool_name, etc.)</li> </ul> <p>Esempio:</p> <pre><code>logger.info(\n    \"MCP request completed\",\n    extra={\n        \"tool_name\": \"query_knowledge_base\",\n        \"duration_ms\": duration_ms,\n        \"status\": \"success\",\n        \"results_count\": len(results)\n    }\n)\n</code></pre>"},{"location":"coding-standards/#6-asyncawait-patterns","title":"6. Async/Await Patterns","text":""},{"location":"coding-standards/#61-async-functions","title":"6.1 Async Functions","text":"<p>Quando usare async:</p> <ul> <li>Operazioni I/O (database, API calls, file operations)</li> <li>Operazioni che possono essere bloccanti</li> </ul> <p>Quando NON usare async:</p> <ul> <li>Operazioni CPU-bound pure</li> <li>Calcoli matematici semplici</li> </ul> <p>Esempio:</p> <pre><code># \u2705 Corretto - I/O operation\nasync def search_knowledge_base(query: str) -&gt; List[Dict]:\n    async with db_pool.acquire() as conn:\n        results = await conn.fetch(query)\n    return results\n\n# \u274c Errato - CPU-bound operation\nasync def calculate_embedding_similarity(emb1: List[float], emb2: List[float]) -&gt; float:\n    # Non serve async per operazioni CPU-bound\n    return cosine_similarity(emb1, emb2)\n</code></pre>"},{"location":"coding-standards/#62-context-managers","title":"6.2 Context Managers","text":"<p>Per risorse che richiedono cleanup:</p> <ul> <li>Database connections</li> <li>File handles</li> <li>LangFuse spans</li> </ul> <p>Esempio:</p> <pre><code>from contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def langfuse_span(name: str) -&gt; AsyncGenerator[Any, None]:\n    \"\"\"Create LangFuse span with automatic cleanup.\"\"\"\n    span = create_span(name)\n    try:\n        yield span\n    finally:\n        span.end()\n\n# Usage\nasync with langfuse_span(\"embedding-generation\") as span:\n    embedding = await generate_embedding(query)\n    span.update(metadata={\"tokens\": len(query.split())})\n</code></pre>"},{"location":"coding-standards/#63-background-tasks","title":"6.3 Background Tasks","text":"<p>Per operazioni pesanti che non devono bloccare startup:</p> <ul> <li>Inizializzazione modelli pesanti</li> <li>Pre-warming cache</li> </ul> <p>Esempio:</p> <pre><code>async def initialize_global_embedder():\n    \"\"\"Initialize embedder in background to avoid blocking startup.\"\"\"\n    global _initialization_task\n\n    async def _init_task():\n        global _global_embedder\n        # Heavy operation offloaded to thread\n        _global_embedder = await asyncio.to_thread(_create_embedder_sync)\n        _embedder_ready.set()\n\n    # Start background task\n    _initialization_task = asyncio.create_task(_init_task())\n</code></pre>"},{"location":"coding-standards/#7-testing-standards","title":"7. Testing Standards","text":""},{"location":"coding-standards/#71-test-organization","title":"7.1 Test Organization","text":"<p>Struttura directory:</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u251c\u2500\u2500 unit/                # Unit tests (&gt;70% coverage)\n\u2502   \u251c\u2500\u2500 test_rag_service.py\n\u2502   \u2514\u2500\u2500 test_embedder.py\n\u251c\u2500\u2500 integration/         # Integration tests\n\u2502   \u251c\u2500\u2500 test_mcp_server.py\n\u2502   \u2514\u2500\u2500 test_observability_endpoints.py\n\u251c\u2500\u2500 e2e/                # End-to-end tests\n\u2502   \u2514\u2500\u2500 test_streamlit_workflow.py\n\u2514\u2500\u2500 fixtures/           # Test fixtures + golden dataset\n    \u2514\u2500\u2500 golden_dataset.json\n</code></pre>"},{"location":"coding-standards/#72-test-naming","title":"7.2 Test Naming","text":"<p>File:</p> <ul> <li><code>test_*.py</code> o <code>*_test.py</code></li> </ul> <p>Funzioni:</p> <ul> <li><code>test_&lt;functionality&gt;_&lt;condition&gt;_&lt;expected_result&gt;</code></li> <li>Usare nomi descrittivi</li> </ul> <p>Esempio:</p> <pre><code># \u2705 Corretto\ndef test_query_knowledge_base_with_valid_query_returns_results():\n    pass\n\ndef test_query_knowledge_base_with_empty_query_raises_error():\n    pass\n\n# \u274c Errato\ndef test_query():\n    pass\n</code></pre>"},{"location":"coding-standards/#73-test-structure","title":"7.3 Test Structure","text":"<p>Pattern AAA (Arrange-Act-Assert):</p> <ul> <li>Arrange: Setup test data</li> <li>Act: Execute function under test</li> <li>Assert: Verify results</li> </ul> <p>Esempio:</p> <pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_search_knowledge_base_returns_formatted_results():\n    # Arrange\n    query = \"test query\"\n    limit = 5\n    mock_results = [\n        {\"content\": \"result 1\", \"source\": \"doc1\"},\n        {\"content\": \"result 2\", \"source\": \"doc2\"}\n    ]\n\n    with patch('core.rag_service.search_knowledge_base_structured') as mock_search:\n        mock_search.return_value = mock_results\n\n        # Act\n        result = await query_knowledge_base(query, limit)\n\n        # Assert\n        assert \"result 1\" in result\n        assert \"doc1\" in result\n        mock_search.assert_called_once_with(query, limit)\n</code></pre>"},{"location":"coding-standards/#74-coverage-requirements","title":"7.4 Coverage Requirements","text":"<p>Minimo:</p> <ul> <li>70% coverage per moduli core</li> <li>80% coverage per moduli critici (observability, error handling)</li> </ul> <p>Verifica:</p> <pre><code>pytest --cov=core --cov=docling_mcp --cov-report=term-missing\n</code></pre>"},{"location":"coding-standards/#8-observability-patterns","title":"8. Observability Patterns","text":""},{"location":"coding-standards/#81-langfuse-integration","title":"8.1 LangFuse Integration","text":"<p>Pattern decorator:</p> <ul> <li>Usare <code>@observe()</code> per funzioni critiche</li> <li>Nested spans per operazioni child</li> </ul> <p>Esempio:</p> <pre><code>from langfuse import observe\n\n@observe(name=\"query_knowledge_base\")\nasync def query_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"Query knowledge base with LangFuse tracing.\"\"\"\n    # Nested span per embedding\n    async with langfuse_span(\"embedding-generation\") as span_ctx:\n        embedding = await generate_query_embedding(query)\n        span_ctx[\"span\"].update(metadata={\"tokens\": len(query.split())})\n\n    # Nested span per DB search\n    async with langfuse_span(\"vector-search\") as span_ctx:\n        results = await search_with_embedding(embedding, limit)\n        span_ctx[\"span\"].update(metadata={\"results_count\": len(results)})\n\n    return format_results(results)\n</code></pre>"},{"location":"coding-standards/#82-prometheus-metrics","title":"8.2 Prometheus Metrics","text":"<p>Pattern:</p> <ul> <li>Inizializzazione lazy con graceful degradation</li> <li>Recording functions per metriche</li> </ul> <p>Esempio:</p> <pre><code>from docling_mcp.metrics import record_request_start, record_request_end\n\n@mcp.tool()\nasync def query_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"Query knowledge base with Prometheus metrics.\"\"\"\n    tool_name = \"query_knowledge_base\"\n    start_time = record_request_start(tool_name)\n    status = \"success\"\n\n    try:\n        results = await search_knowledge_base_structured(query, limit)\n        return format_results(results)\n    except Exception as e:\n        status = \"error\"\n        raise\n    finally:\n        record_request_end(tool_name, start_time, status)\n</code></pre>"},{"location":"coding-standards/#83-health-checks","title":"8.3 Health Checks","text":"<p>Pattern:</p> <ul> <li>Status logic chiaro (ok/degraded/down)</li> <li>Service checks individuali</li> <li>Timing information</li> </ul> <p>Esempio:</p> <pre><code>@dataclass\nclass HealthResponse:\n    status: Literal[\"ok\", \"degraded\", \"down\"]\n    timestamp: float\n    services: Dict[str, Dict[str, Any]]\n\nasync def check_health() -&gt; HealthResponse:\n    \"\"\"Check system health with service status.\"\"\"\n    db_status = await check_database()\n    langfuse_status = await check_langfuse()\n    embedder_status = await check_embedder()\n\n    # Determine overall status\n    if db_status.status == \"down\":\n        overall_status = \"down\"\n    elif langfuse_status.status == \"down\":\n        overall_status = \"degraded\"  # LangFuse optional\n    else:\n        overall_status = \"ok\"\n\n    return HealthResponse(\n        status=overall_status,\n        timestamp=time.time(),\n        services={\n            \"database\": asdict(db_status),\n            \"langfuse\": asdict(langfuse_status),\n            \"embedder\": asdict(embedder_status)\n        }\n    )\n</code></pre>"},{"location":"coding-standards/#9-code-organization","title":"9. Code Organization","text":""},{"location":"coding-standards/#91-module-structure","title":"9.1 Module Structure","text":"<p>Ordine standard:</p> <ol> <li>Module docstring</li> <li>Imports (standard, third-party, local)</li> <li>Constants</li> <li>Global variables (con prefisso <code>_</code> se privati)</li> <li>Classes</li> <li>Functions</li> <li>Module-level code (se necessario)</li> </ol> <p>Esempio:</p> <pre><code>\"\"\"\nModule description.\n\"\"\"\n\n# Standard library\nimport logging\nfrom typing import List, Optional\n\n# Third-party\nfrom fastmcp import FastMCP\n\n# Local\nfrom core.rag_service import search_knowledge_base_structured\n\n# Constants\nDEFAULT_LIMIT = 5\nMAX_RETRIES = 3\n\n# Global variables\nlogger = logging.getLogger(__name__)\n_global_state = None\n\n# Classes\nclass ServiceClass:\n    pass\n\n# Functions\nasync def public_function():\n    pass\n\ndef _private_function():\n    pass\n</code></pre>"},{"location":"coding-standards/#92-separation-of-concerns","title":"9.2 Separation of Concerns","text":"<p>Principi:</p> <ul> <li>Business logic separata da I/O (es. <code>core/rag_service.py</code> decoupled)</li> <li>Observability separata da business logic (decorators, metrics)</li> <li>Configuration separata da implementation</li> </ul> <p>Esempio:</p> <pre><code># \u2705 Corretto - Business logic separata\n# core/rag_service.py\nasync def search_knowledge_base_structured(query: str, limit: int) -&gt; List[Dict]:\n    \"\"\"Pure business logic, no I/O concerns.\"\"\"\n    pass\n\n# docling_mcp/server.py - I/O layer\n@mcp.tool()\n@observe(name=\"query_knowledge_base\")\nasync def query_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"MCP tool with observability, calls business logic.\"\"\"\n    results = await search_knowledge_base_structured(query, limit)\n    return format_results(results)\n\n# \u274c Errato - Business logic mescolata con I/O\nasync def query_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    # Business logic\n    embedding = await generate_embedding(query)\n    # I/O operations\n    async with db_pool.acquire() as conn:\n        results = await conn.fetch(...)\n    # Formatting\n    return format_results(results)\n</code></pre>"},{"location":"coding-standards/#10-performance-considerations","title":"10. Performance Considerations","text":""},{"location":"coding-standards/#101-resource-management","title":"10.1 Resource Management","text":"<p>Connection Pools:</p> <ul> <li>Inizializzare una volta, riutilizzare</li> <li>Configurare size appropriato (min/max)</li> </ul> <p>Esempio:</p> <pre><code># \u2705 Corretto - Global pool\ndb_pool = DatabasePool()\nawait db_pool.initialize()  # Once at startup\n\nasync def search_db(query: str):\n    async with db_pool.acquire() as conn:\n        return await conn.fetch(query)\n\n# \u274c Errato - New connection per query\nasync def search_db(query: str):\n    conn = await asyncpg.connect(DATABASE_URL)\n    try:\n        return await conn.fetch(query)\n    finally:\n        await conn.close()\n</code></pre>"},{"location":"coding-standards/#102-caching","title":"10.2 Caching","text":"<p>Quando usare:</p> <ul> <li>Operazioni costose ripetute (embedding generation)</li> <li>Dati che cambiano raramente</li> </ul> <p>Pattern:</p> <ul> <li>LRU cache con size limitato</li> <li>Cache key basata su input</li> </ul> <p>Esempio:</p> <pre><code>class EmbeddingCache:\n    \"\"\"Simple in-memory cache for embeddings.\"\"\"\n    def __init__(self, max_size: int = 1000):\n        self.cache: Dict[str, List[float]] = {}\n        self.max_size = max_size\n\n    def get(self, text: str) -&gt; Optional[List[float]]:\n        return self.cache.get(text)\n\n    def set(self, text: str, embedding: List[float]):\n        if len(self.cache) &gt;= self.max_size:\n            # Simple eviction: remove first key (FIFO-ish)\n            first_key = next(iter(self.cache))\n            del self.cache[first_key]\n        self.cache[text] = embedding\n</code></pre>"},{"location":"coding-standards/#103-background-initialization","title":"10.3 Background Initialization","text":"<p>Per operazioni pesanti:</p> <ul> <li>Non bloccare startup</li> <li>Pre-warm risorse</li> </ul> <p>Esempio:</p> <pre><code># Initialize in background to avoid blocking MCP handshake\n_initialization_task = asyncio.create_task(initialize_global_embedder())\n\n# Check readiness before use\nif not _embedder_ready.is_set():\n    await _embedder_ready.wait()\n</code></pre>"},{"location":"coding-standards/#11-security-best-practices","title":"11. Security Best Practices","text":""},{"location":"coding-standards/#111-secrets-management","title":"11.1 Secrets Management","text":"<p>Pattern:</p> <ul> <li>Environment variables per secrets</li> <li>Mai hardcode secrets nel codice</li> <li>Usare <code>.env</code> file per sviluppo locale</li> </ul> <p>Esempio:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# \u2705 Corretto\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n\n# \u274c Errato\napi_key = \"sk-...\"  # Never hardcode secrets\n</code></pre>"},{"location":"coding-standards/#112-input-validation","title":"11.2 Input Validation","text":"<p>Pattern:</p> <ul> <li>Validare input all'ingresso</li> <li>Usare Pydantic models quando possibile</li> <li>Sanitizzare user input</li> </ul> <p>Esempio:</p> <pre><code>from pydantic import BaseModel, Field, validator\n\nclass QueryRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=1000)\n    limit: int = Field(default=5, ge=1, le=100)\n\n    @validator('query')\n    def validate_query(cls, v):\n        if not v.strip():\n            raise ValueError(\"Query cannot be empty\")\n        return v.strip()\n</code></pre>"},{"location":"coding-standards/#113-error-messages","title":"11.3 Error Messages","text":"<p>Pattern:</p> <ul> <li>Non esporre informazioni sensibili in errori user-facing</li> <li>Log dettagli completi, messaggi user-friendly</li> </ul> <p>Esempio:</p> <pre><code>try:\n    result = await process_request(request)\nexcept DatabaseError as e:\n    # Log completo per debugging\n    logger.error(f\"Database error: {e}\", exc_info=True)\n    # Messaggio user-friendly\n    raise ToolError(\"An error occurred processing your request\")\n</code></pre>"},{"location":"coding-standards/#12-configuration-management","title":"12. Configuration Management","text":""},{"location":"coding-standards/#121-environment-variables","title":"12.1 Environment Variables","text":"<p>Pattern:</p> <ul> <li>Usare <code>python-dotenv</code> per sviluppo</li> <li>Validare variabili richieste all'avvio</li> <li>Fornire valori di default quando appropriato</li> </ul> <p>Esempio:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Required variables\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nif not DATABASE_URL:\n    raise ValueError(\"DATABASE_URL environment variable required\")\n\n# Optional variables with defaults\nMETRICS_PORT = int(os.getenv(\"METRICS_PORT\", \"8080\"))\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n</code></pre>"},{"location":"coding-standards/#122-configuration-classes","title":"12.2 Configuration Classes","text":"<p>Pattern:</p> <ul> <li>Usare dataclasses o Pydantic models per config</li> <li>Validazione automatica</li> <li>Type hints completi</li> </ul> <p>Esempio:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass ServerConfig:\n    host: str = \"0.0.0.0\"\n    port: int = 8080\n    database_url: str = \"\"\n    langfuse_public_key: Optional[str] = None\n    langfuse_secret_key: Optional[str] = None\n\n    def __post_init__(self):\n        if not self.database_url:\n            raise ValueError(\"database_url is required\")\n</code></pre>"},{"location":"coding-standards/#13-code-review-checklist","title":"13. Code Review Checklist","text":"<p>Prima di sottomettere codice per review, verificare:</p> <ul> <li>[ ] Type hints completi per funzioni pubbliche</li> <li>[ ] Docstrings con formato standardizzato</li> <li>[ ] Error handling appropriato (graceful degradation se necessario)</li> <li>[ ] Logging con livelli corretti</li> <li>[ ] Test aggiunti/modificati per nuove funzionalit\u00e0</li> <li>[ ] Coverage mantenuto sopra 70%</li> <li>[ ] Nessun secret hardcoded</li> <li>[ ] Input validation dove necessario</li> <li>[ ] Performance considerations (caching, pooling) se applicabile</li> <li>[ ] Observability integrata (LangFuse, Prometheus) per operazioni critiche</li> </ul>"},{"location":"coding-standards/#14-tools-and-linting","title":"14. Tools and Linting","text":""},{"location":"coding-standards/#141-ruff","title":"14.1 Ruff","text":"<p>Configurazione in <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\nline-length = 100\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\"]\nignore = [\"E501\"]  # Line too long (handled by formatter)\n</code></pre> <p>Uso:</p> <pre><code>ruff check .\nruff check --fix .\n</code></pre>"},{"location":"coding-standards/#142-mypy","title":"14.2 MyPy","text":"<p>Configurazione:</p> <pre><code>[tool.mypy]\npython_version = \"3.10\"\nwarn_return_any = true\nwarn_unused_configs = true\nignore_missing_imports = true\n</code></pre> <p>Uso:</p> <pre><code>mypy core/ docling_mcp/\n</code></pre>"},{"location":"coding-standards/#143-black-optional","title":"14.3 Black (Optional)","text":"<p>Configurazione:</p> <pre><code>[tool.black]\nline-length = 100\ntarget-version = [\"py310\", \"py311\"]\n</code></pre> <p>Uso:</p> <pre><code>black .\n</code></pre>"},{"location":"coding-standards/#15-references","title":"15. References","text":""},{"location":"coding-standards/#internal-documentation","title":"Internal Documentation","text":"<ul> <li>Architecture: System architecture, design decisions, and integration patterns</li> <li>Unified Project Structure: Standardized directory structure and file organization rules</li> <li>Testing Strategy: Complete testing strategy with TDD workflow and test organization</li> <li>Development Guide: Setup instructions and development workflow</li> <li>Epic Breakdown: Complete epic and story breakdown</li> <li>Tech Specs: Technical specifications for each epic</li> </ul>"},{"location":"coding-standards/#external-references","title":"External References","text":"<ul> <li>Python Style Guide: PEP 8 (https://pep8.org/)</li> <li>Type Hints: PEP 484 (https://peps.python.org/pep-0484/)</li> <li>Async/Await: PEP 492 (https://peps.python.org/pep-0492/)</li> <li>Ruff Documentation: https://docs.astral.sh/ruff/</li> <li>MyPy Documentation: https://mypy.readthedocs.io/</li> <li>Black Documentation: https://black.readthedocs.io/</li> </ul>"},{"location":"coding-standards/#changelog","title":"Changelog","text":"<ul> <li>2025-01-27: Initial version based on existing codebase patterns</li> </ul>"},{"location":"development-guide/","title":"Guida allo Sviluppo","text":"<p>Questa guida fornisce informazioni dettagliate sull'architettura del progetto, pattern di sviluppo con FastMCP, testing con PydanticAI, e walkthrough dell'implementazione.</p>"},{"location":"development-guide/#indice","title":"Indice","text":"<ul> <li>Struttura del Progetto</li> <li>Pattern FastMCP</li> <li>Testing con PydanticAI</li> <li>Walkthrough Implementazione</li> </ul>"},{"location":"development-guide/#struttura-del-progetto","title":"Struttura del Progetto","text":""},{"location":"development-guide/#organizzazione-directory","title":"Organizzazione Directory","text":"<pre><code>docling-rag-agent/\n\u251c\u2500\u2500 app.py                         # Interfaccia web Streamlit\n\u251c\u2500\u2500 mcp_server.py                  # Entry point MCP + HTTP observability\n\u251c\u2500\u2500 docling_mcp/                   # MCP server module\n\u2502   \u251c\u2500\u2500 server.py                  # FastMCP instance con tools\n\u2502   \u251c\u2500\u2500 lifespan.py                # Lifecycle risorse (DB, embedder)\n\u2502   \u251c\u2500\u2500 http_server.py             # HTTP server per /health e /metrics\n\u2502   \u251c\u2500\u2500 health.py                  # Health check logic\n\u2502   \u2514\u2500\u2500 metrics.py                 # Prometheus metrics\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 agent.py                   # PydanticAI agent wrapper\n\u2502   \u2514\u2500\u2500 rag_service.py             # Core RAG logic (decoupled)\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 main.py                    # FastAPI REST API\n\u2502   \u2514\u2500\u2500 models.py                  # Modelli API\n\u251c\u2500\u2500 client/\n\u2502   \u2514\u2500\u2500 api_client.py              # Client API per chiamate esterne\n\u251c\u2500\u2500 ingestion/\n\u2502   \u251c\u2500\u2500 ingest.py                  # Pipeline ingestione documenti\n\u2502   \u251c\u2500\u2500 embedder.py                # Generazione embedding con caching\n\u2502   \u2514\u2500\u2500 chunker.py                 # Chunking intelligente (Docling HybridChunker)\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 providers.py               # Configurazione modelli/client OpenAI\n\u2502   \u251c\u2500\u2500 db_utils.py                # Connection pooling ottimizzato\n\u2502   \u2514\u2500\u2500 models.py                  # Modelli Pydantic per validazione\n\u251c\u2500\u2500 sql/\n\u2502   \u2514\u2500\u2500 optimize_index.sql         # Schema completo + HNSW index ottimizzato\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 verification/              # Script di verifica e validazione\n\u2502   \u2502   \u251c\u2500\u2500 verify_api.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_api_endpoints.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_mcp_setup.py\n\u2502   \u2502   \u2514\u2500\u2500 optimize_database.py\n\u2502   \u251c\u2500\u2500 validation/                # Script validazione struttura\n\u2502   \u2502   \u251c\u2500\u2500 validate_structure.py\n\u2502   \u2502   \u2514\u2500\u2500 validate_imports.py\n\u2502   \u251c\u2500\u2500 testing/                   # Script test utilities (non pytest)\n\u2502   \u2502   \u251c\u2500\u2500 test_cost_tracking.py\n\u2502   \u2502   \u251c\u2500\u2500 test_e2e_langfuse_timing.py\n\u2502   \u2502   \u2514\u2500\u2500 test_mcp_performance.py\n\u2502   \u2514\u2500\u2500 debug/                     # Script debug\n\u2502       \u2514\u2500\u2500 debug_mcp_tools.py\n\u251c\u2500\u2500 guide/                         # Documentazione progetto\n\u251c\u2500\u2500 docs/                          # Documentazione BMAD workflow\n\u251c\u2500\u2500 documents/                     # Documenti per ingestione\n\u2514\u2500\u2500 tests/                         # Test suite (pytest)\n    \u251c\u2500\u2500 unit/                      # Unit tests\n    \u251c\u2500\u2500 integration/               # Integration tests\n    \u2514\u2500\u2500 e2e/                       # End-to-end tests\n</code></pre>"},{"location":"development-guide/#responsabilita-moduli","title":"Responsabilit\u00e0 Moduli","text":"Directory Responsabilit\u00e0 <code>core/</code> Business logic principale, agent RAG, servizio ricerca <code>api/</code> REST API per accesso programmatico <code>docling_mcp/</code> Server MCP per integrazione Cursor IDE <code>client/</code> Client API per chiamate esterne <code>ingestion/</code> Pipeline ingestione documenti, chunking, embedding <code>utils/</code> Utility condivise: configurazione, connessione DB, modelli <code>scripts/verification/</code> Script verifica: API, MCP setup, ottimizzazione DB <code>scripts/validation/</code> Script validazione: struttura progetto, imports Python <code>scripts/testing/</code> Script test utilities (non pytest): performance, cost tracking <code>scripts/debug/</code> Script debug: MCP tools debugging <code>guide/</code> Documentazione progetto (troubleshooting, development guide) <code>docs/</code> Documentazione workflow BMAD"},{"location":"development-guide/#pattern-fastmcp","title":"Pattern FastMCP","text":""},{"location":"development-guide/#testabilita-unitaria-con-fastmcp-2x","title":"Testabilit\u00e0 Unitaria con FastMCP 2.x","text":"<p>In FastMCP 2.x, i decoratori <code>@mcp.tool</code> e <code>@mcp.resource</code> trasformano le funzioni in oggetti (<code>FunctionTool</code>, <code>FunctionResource</code>).</p> <pre><code># FastMCP 2.x - NON funziona\n@mcp.tool\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\nprint(add(1, 2))  # TypeError: 'FunctionTool' object is not callable\n</code></pre>"},{"location":"development-guide/#soluzioni-per-testing","title":"Soluzioni per Testing","text":""},{"location":"development-guide/#a-accesso-alla-funzione-originale-tramite-fn","title":"A) Accesso alla Funzione Originale tramite <code>.fn</code>","text":"<pre><code># Accesso diretto alla funzione wrappata\nresult = add.fn(1, 2)  # Funziona\n</code></pre>"},{"location":"development-guide/#b-approccio-raccomandato-test-via-client","title":"B) Approccio Raccomandato: Test via Client","text":"<pre><code>import pytest\nfrom fastmcp.client import Client\nfrom my_project.main import mcp\n\n@pytest.fixture\nasync def main_mcp_client():\n    async with Client(transport=mcp) as mcp_client:\n        yield mcp_client\n\n@pytest.mark.parametrize(\"a, b, expected\", [(1, 2, 3), (2, 3, 5)])\nasync def test_add(a: int, b: int, expected: int, main_mcp_client: Client):\n    result = await main_mcp_client.call_tool(\n        name=\"add\",\n        arguments={\"x\": a, \"y\": b}\n    )\n    assert result.data == expected\n</code></pre> <p>Configurazione pytest richiesta in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\n</code></pre>"},{"location":"development-guide/#risorse-dinamiche-vs-tools","title":"Risorse Dinamiche vs Tools","text":"Concetto Semantica Resources Rappresentano dati che un server MCP vuole rendere disponibili ai client (file, schemi DB, info app) Tools Rappresentano operazioni dinamiche che possono modificare lo stato o interagire con sistemi esterni"},{"location":"development-guide/#quando-usare-resources","title":"Quando Usare Resources","text":"<p>Una lista di documenti generata dinamicamente da query DB pu\u00f2 essere esposta come Resource se:</p> <ol> <li>\u00c8 read-only</li> <li>Non ha side-effects</li> <li>Rappresenta \"lo stato corrente\" di una collezione</li> </ol> <pre><code>@mcp.resource(\"documents://{document_id}\")\ndef get_document(document_id: str) -&gt; dict:\n    return db.query(f\"SELECT * FROM docs WHERE id = {document_id}\")\n</code></pre>"},{"location":"development-guide/#quando-usare-tools","title":"Quando Usare Tools","text":"<p>Preferire Tool se:</p> <ul> <li>L'operazione richiede parametri di filtro complessi</li> <li>L'operazione ha effetti collaterali</li> <li>Vuoi che l'LLM possa \"invocare\" l'operazione attivamente</li> </ul> <p>Nota Critica: Dalla documentazione Cursor: \"Resources are not yet supported in Cursor.\" Se il client \u00e8 Cursor, usare Tools \u00e8 l'unica opzione funzionante.</p>"},{"location":"development-guide/#gestione-errori-e-propagazione-al-client","title":"Gestione Errori e Propagazione al Client","text":""},{"location":"development-guide/#meccanismo-di-default","title":"Meccanismo di Default","text":"<p>FastMCP converte automaticamente le eccezioni in risposte MCP error:</p> <pre><code>from fastmcp import FastMCP\nfrom fastmcp.exceptions import ToolError\n\nmcp = FastMCP(name=\"Server\", mask_error_details=True)\n\n@mcp.tool\ndef divide(a: float, b: float) -&gt; float:\n    if b == 0:\n        # Questo messaggio arriva SEMPRE al client\n        raise ToolError(\"Division by zero is not allowed.\")\n\n    try:\n        result = a / b\n        if result &gt; 1000000:\n            # Con mask_error_details=True, questo diventa messaggio generico\n            raise ValueError(\"Result too large\")\n        return result\n    except Exception as e:\n        raise ToolError(\"Calculation failed\") from e\n</code></pre>"},{"location":"development-guide/#comportamento-con-mask_error_details","title":"Comportamento con <code>mask_error_details</code>","text":"<code>mask_error_details</code> <code>ToolError</code> Altre Eccezioni <code>False</code> (default) Messaggio visibile Messaggio visibile <code>True</code> Messaggio visibile Messaggio generico mascherato"},{"location":"development-guide/#logging-verso-il-client","title":"Logging verso il Client","text":"<p>Usare il <code>Context</code> per inviare log strutturati al client MCP:</p> <pre><code>from fastmcp import FastMCP, Context\n\n@mcp.tool\nasync def risky_operation(ctx: Context) -&gt; dict:\n    try:\n        await ctx.info(\"Starting operation\")\n        result = await db.execute(...)\n        await ctx.info(\"Operation completed\")\n        return result\n    except DatabaseError as e:\n        await ctx.error(f\"Database error: {e}\")\n        raise ToolError(\"Database operation failed\")\n</code></pre>"},{"location":"development-guide/#codici-errore-mcp-standard","title":"Codici Errore MCP Standard","text":"<p>Per errori critici (DB down), usare i codici JSON-RPC standard:</p> <ul> <li><code>-32002</code>: Resource not found</li> <li><code>-32602</code>: Invalid params</li> <li><code>-32603</code>: Internal error</li> </ul>"},{"location":"development-guide/#raccomandazioni-fastmcp","title":"Raccomandazioni FastMCP","text":"<ol> <li>Testing: Usare <code>.fn</code> per unit test rapidi, Client fixture per integration test completi</li> <li>Resources vs Tools: Dato che Cursor non supporta Resources, convertire <code>documents://list</code> in un Tool <code>list_documents()</code></li> <li>Errori: Usare <code>ToolError</code> per messaggi espliciti, <code>mask_error_details=True</code> in produzione, Context logging per debug</li> </ol>"},{"location":"development-guide/#testing-con-pydanticai","title":"Testing con PydanticAI","text":""},{"location":"development-guide/#panoramica","title":"Panoramica","text":"<p>Testing in PydanticAI segue patterns standard Python con strumenti specializzati per evitare chiamate reali a LLM durante i test.</p>"},{"location":"development-guide/#stack-testing-raccomandato","title":"Stack Testing Raccomandato","text":"Tool Scopo <code>pytest</code> Test harness <code>inline-snapshot</code> Assertions lunghe <code>dirty-equals</code> Comparazione strutture dati complesse <code>TestModel</code> Mock model per sostituire LLM reali <code>FunctionModel</code> Mock model con logica custom <code>Agent.override()</code> Mock model/dependencies/toolsets <code>ALLOW_MODEL_REQUESTS</code> Blocca chiamate accidentali a LLM"},{"location":"development-guide/#testmodel","title":"TestModel","text":"<p><code>TestModel</code> \u00e8 un mock model per testing che:</p> <ul> <li>Chiama automaticamente tutti i tool dell'agent</li> <li>Genera risposte deterministiche (testo o structured output)</li> <li>NON usa ML/AI - solo codice Python procedurale</li> <li>Genera dati validi che soddisfano JSON schema dei tool registrati</li> </ul> <pre><code>from pydantic_ai.models.test import TestModel\n\n# Basic usage\ntest_model = TestModel()\n\n# Con custom output text\ntest_model = TestModel(custom_output_text='Sunny')\n</code></pre> <p>Limitazioni:</p> <ul> <li>Dati generati sono \"fake\" e non semanticamente corretti</li> <li>Usa hardcoded values (es. date nel passato)</li> <li>Per testing realistico, usa <code>FunctionModel</code> con custom logic</li> </ul>"},{"location":"development-guide/#agentoverride","title":"Agent.override()","text":"<pre><code>import pytest\nfrom pydantic_ai.models.test import TestModel\nfrom weather_app import weather_agent\n\nasync def test_forecast():\n    # Override model con TestModel\n    with weather_agent.override(model=TestModel()):\n        prompt = 'What will the weather be like in London on 2024-11-28?'\n        result = await weather_agent.run(prompt)\n\n    # Assert sul risultato\n    assert result.data == expected_data\n</code></pre>"},{"location":"development-guide/#pytest-fixture-pattern","title":"Pytest Fixture Pattern","text":"<pre><code># test_agent.py\nimport pytest\nfrom pydantic_ai.models.test import TestModel\nfrom weather_app import weather_agent\n\n@pytest.fixture\ndef override_weather_agent():\n    with weather_agent.override(model=TestModel()):\n        yield\n\nasync def test_forecast(override_weather_agent: None):\n    # TestModel gi\u00e0 attivo qui\n    result = await weather_agent.run('test prompt')\n    assert result.data == expected\n</code></pre>"},{"location":"development-guide/#capture_run_messages","title":"capture_run_messages()","text":"<p>Catturare e ispezionare messaggi scambiati tra agent e model:</p> <pre><code>from pydantic_ai import capture_run_messages\n\nwith capture_run_messages() as messages:\n    with weather_agent.override(model=TestModel()):\n        result = await weather_agent.run('test prompt')\n\n# Inspect tool calls\nassert messages[1].parts[0].tool_name == 'weather_forecast'\nassert messages[1].parts[0].args == {'location': 'London'}\n</code></pre>"},{"location":"development-guide/#allow_model_requests","title":"ALLOW_MODEL_REQUESTS","text":"<p>Safety measure per bloccare globalmente richieste a modelli non-test:</p> <pre><code>from pydantic_ai import models\n\n# A livello modulo (top del file test)\nmodels.ALLOW_MODEL_REQUESTS = False\n</code></pre>"},{"location":"development-guide/#pattern-conftestpy","title":"Pattern conftest.py","text":"<pre><code># conftest.py\nimport pytest\nfrom pydantic_ai import models\n\n@pytest.fixture(autouse=True)\ndef disable_real_model_requests():\n    \"\"\"Disable real LLM requests globally for all tests.\"\"\"\n    original = models.ALLOW_MODEL_REQUESTS\n    models.ALLOW_MODEL_REQUESTS = False\n    yield\n    models.ALLOW_MODEL_REQUESTS = original\n</code></pre>"},{"location":"development-guide/#functionmodel-testing-avanzato","title":"FunctionModel - Testing Avanzato","text":"<p>Per test che richiedono valori specifici o logica custom:</p> <pre><code>import re\nfrom pydantic_ai import ModelMessage, ModelResponse, TextPart, ToolCallPart\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\ndef call_weather_forecast(\n    messages: list[ModelMessage],\n    info: AgentInfo\n) -&gt; ModelResponse:\n    \"\"\"Custom function per generare tool calls realistici.\"\"\"\n    if len(messages) == 1:\n        # First call: extract date from prompt\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n\n        # Call weather_forecast tool con date estratta\n        return ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={'location': 'London', 'date': m.group()}\n                )\n            ]\n        )\n    else:\n        # Second call: return final response\n        return ModelResponse(\n            parts=[TextPart(content='Forecast result')]\n        )\n\nasync def test_forecast_future():\n    from weather_app import weather_agent\n\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):\n        result = await weather_agent.run('Weather for 2024-12-25?')\n\n    # Ora il tool viene chiamato con date futura!\n    assert '2024-12-25' in result.data\n</code></pre>"},{"location":"development-guide/#testing-checklist","title":"Testing Checklist","text":"<pre><code>\u2705 Setup\n- [ ] ALLOW_MODEL_REQUESTS = False\n- [ ] pytest.mark.anyio per async tests\n- [ ] TestModel override configurato\n\n\u2705 Test Coverage\n- [ ] Agent logic (TestModel)\n- [ ] Tool calls (capture_run_messages)\n- [ ] Tool integration (FunctionModel)\n- [ ] Error handling\n- [ ] Dependency mocking\n\n\u2705 Assertions\n- [ ] Response data validation\n- [ ] Tool call parameters\n- [ ] Message exchange sequence\n- [ ] Side effects (DB writes, etc.)\n</code></pre>"},{"location":"development-guide/#gotchaswarnings-comuni","title":"Gotchas/Warnings Comuni","text":""},{"location":"development-guide/#1-testmodel-hardcoded-values","title":"1. TestModel Hardcoded Values","text":"<pre><code># \u274c TestModel genera date nel passato\n# Tool chiamato con: {'date': '2020-01-01'}\n\n# \u2705 Usa FunctionModel per date specifiche\ndef custom_model(...):\n    return ToolCallPart(args={'date': '2024-12-25'})\n</code></pre>"},{"location":"development-guide/#2-allow_model_requests-dimenticato","title":"2. ALLOW_MODEL_REQUESTS Dimenticato","text":"<pre><code># \u274c Senza safety flag\nasync def test_agent():\n    # Se override fallisce, chiama OpenAI API! \ud83d\udcb8\n\n# \u2705 Con safety flag\nmodels.ALLOW_MODEL_REQUESTS = False\nasync def test_agent():\n    # Fallisce subito se override mancante\n</code></pre>"},{"location":"development-guide/#3-capture_run_messages-scope","title":"3. capture_run_messages Scope","text":"<pre><code># \u274c Cattura fuori scope\nwith capture_run_messages() as messages:\n    pass\nawait agent.run('test')  # Non catturato!\n\n# \u2705 Cattura dentro scope\nwith capture_run_messages() as messages:\n    await agent.run('test')  # \u2705 Catturato\n</code></pre>"},{"location":"development-guide/#4-pytest-async-setup","title":"4. Pytest Async Setup","text":"<pre><code># \u274c Senza anyio\nasync def test_agent():  # Non eseguito!\n    pass\n\n# \u2705 Con anyio\npytestmark = pytest.mark.anyio\nasync def test_agent():  # \u2705 Eseguito\n    pass\n</code></pre>"},{"location":"development-guide/#link-documentazione-ufficiale","title":"Link Documentazione Ufficiale","text":"<p>PydanticAI Testing Documentation:</p> <ul> <li>Main: https://ai.pydantic.dev/testing/</li> <li>TestModel: https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel</li> <li>FunctionModel: https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel</li> <li>Agent.override: https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.override</li> <li>capture_run_messages: https://ai.pydantic.dev/api/messages/#pydantic_ai.capture_run_messages</li> <li>ALLOW_MODEL_REQUESTS: https://ai.pydantic.dev/api/models/#pydantic_ai.models.ALLOW_MODEL_REQUESTS</li> </ul> <p>Testing Tools:</p> <ul> <li>pytest: https://docs.pytest.org/en/stable/</li> <li>inline-snapshot: https://15r10nk.github.io/inline-snapshot/latest/</li> <li>dirty-equals: https://dirty-equals.helpmanual.io/latest/</li> <li>anyio: https://anyio.readthedocs.io/en/stable/</li> </ul>"},{"location":"development-guide/#walkthrough-implementazione","title":"Walkthrough Implementazione","text":""},{"location":"development-guide/#refactoring-e-implementazione-mcp-server","title":"Refactoring e Implementazione MCP Server","text":"<p>Questo progetto \u00e8 stato refactorizzato per supportare sia Streamlit che MCP (Model Context Protocol).</p>"},{"location":"development-guide/#1-core-logic-decoupling","title":"1. Core Logic Decoupling","text":"<p>La logica core RAG \u00e8 stata estratta in un package <code>core</code> separato:</p> <ul> <li><code>core/rag_service.py</code>: Contiene la logica pura per la ricerca nella knowledge base, indipendente da qualsiasi framework agent.</li> <li><code>core/agent.py</code>: Wrappa il <code>rag_service</code> per uso con PydanticAI (usato da Streamlit).</li> </ul>"},{"location":"development-guide/#2-mcp-server-implementation","title":"2. MCP Server Implementation","text":"<p>Il modulo <code>docling_mcp/</code> usa la libreria <code>FastMCP</code>:</p> <ul> <li><code>docling_mcp/server.py</code>: FastMCP instance con tool registration</li> <li><code>docling_mcp/lifespan.py</code>: Gestisce il lifecycle delle risorse (DB pool, embedder)</li> <li><code>docling_mcp/tools/</code>: Tools organizzati per dominio (search.py, documents.py, overview.py)</li> </ul> <p>Tools disponibili:</p> <ul> <li><code>query_knowledge_base</code>: Ricerca semantica nella knowledge base</li> <li><code>ask_knowledge_base</code>: Domande con risposta formattata</li> <li><code>list_knowledge_base_documents</code>: Lista documenti</li> <li><code>get_knowledge_base_document</code>: Dettagli documento per ID</li> <li><code>get_knowledge_base_overview</code>: Statistiche knowledge base</li> </ul>"},{"location":"development-guide/#3-application-updates","title":"3. Application Updates","text":"<ul> <li><code>app.py</code>: Aggiornato per importare l'agent da <code>core.agent</code></li> <li><code>pyproject.toml</code>: Aggiunta dipendenza <code>fastmcp</code></li> <li><code>rag_agent.py</code>: Eliminato (sostituito da <code>core/agent.py</code>)</li> </ul>"},{"location":"development-guide/#come-usare","title":"Come Usare","text":""},{"location":"development-guide/#running-the-mcp-server-cursor-claude-desktop","title":"Running the MCP Server (Cursor / Claude Desktop)","text":"<ol> <li>Assicurati di avere le dipendenze installate:</li> </ol> <p><code>bash    uv sync</code></p> <ol> <li>Configura il tuo editor.</li> </ol> <p>Per Cursor (<code>~/.cursor/mcp.json</code> su Windows <code>%USERPROFILE%\\.cursor\\mcp.json</code>):</p> <p><code>json    {      \"docling-rag\": {        \"command\": \"uv\",        \"args\": [          \"run\",          \"--project\",          \"C:/path/to/docling-rag-agent\",          \"python\",          \"C:/path/to/docling-rag-agent/mcp_server.py\"        ]      }    }</code></p> <p>Nota importante: Il parametro <code>--project</code> \u00e8 obbligatorio per garantire che <code>uv</code> utilizzi il virtual environment corretto del progetto. Senza questo wrapper, l'MCP potrebbe fallire a trovare le dipendenze.</p> <p>Per Claude Desktop: Copia la stessa configurazione nel file di configurazione di Claude Desktop.</p> <ol> <li>Verifica Observability: Dopo l'avvio del server MCP, gli endpoint di osservabilit\u00e0 saranno disponibili:</li> <li>Health check: http://localhost:8080/health</li> <li>Prometheus metrics: http://localhost:8080/metrics</li> <li>API docs: http://localhost:8080/docs</li> </ol>"},{"location":"development-guide/#running-the-streamlit-app","title":"Running the Streamlit App","text":"<p>Il workflow esistente rimane invariato:</p> <pre><code>streamlit run app.py\n</code></pre>"},{"location":"development-guide/#verifica","title":"Verifica","text":"<ul> <li>Streamlit: L'app dovrebbe funzionare esattamente come prima, poich\u00e9 l'interfaccia <code>core.agent</code> corrisponde al vecchio <code>rag_agent</code>.</li> <li>MCP: Il server espone <code>query_knowledge_base</code> che accetta <code>query</code>, <code>limit</code>, e <code>source_filter</code>.</li> </ul>"},{"location":"development-guide/#best-practices-generali","title":"Best Practices Generali","text":""},{"location":"development-guide/#code-quality","title":"Code Quality","text":"<ol> <li>Type Hints: Usa sempre type hints per parametri e return types</li> <li>Docstrings: Documenta tutte le funzioni pubbliche con docstrings</li> <li>Error Handling: Cattura eccezioni specifiche, non generiche</li> <li>Logging: Usa logging strutturato per debug e monitoring</li> </ol>"},{"location":"development-guide/#performance","title":"Performance","text":"<ol> <li>Connection Pooling: Usa il connection pool per database</li> <li>Embedding Cache: L'embedder ha cache LRU per query frequenti</li> <li>HNSW Index: Usa HNSW invece di IVFFlat per ricerche vettoriali</li> </ol>"},{"location":"development-guide/#testing","title":"Testing","text":"<ol> <li>Unit Tests: Testa business logic con mock models</li> <li>Integration Tests: Testa interazioni componenti con FunctionModel</li> <li>ALLOW_MODEL_REQUESTS=False: Sempre attivo per evitare chiamate API accidentali</li> </ol>"},{"location":"epics/","title":"docling-rag-agent - Epic Breakdown","text":"<p>Author: Stefano Date: 2025-11-24 Updated: 2025-11-26 Project Level: Brownfield Enhancement Target Scale: Production-Ready MVP</p>"},{"location":"epics/#overview","title":"Overview","text":"<p>This document provides the complete epic and story breakdown for <code>docling-rag-agent</code>, decomposing the requirements from the PRD into implementable stories. The primary goal is to transform the system into a production-ready RAG agent with comprehensive LangFuse monitoring, cost tracking, and GitHub deployment readiness.</p> <p>Living Document Notice: This is the initial version. It will be updated after Architecture workflow adds technical implementation details.</p>"},{"location":"epics/#functional-requirements-inventory","title":"Functional Requirements Inventory","text":""},{"location":"epics/#core-rag-capabilities","title":"Core RAG Capabilities","text":"<ul> <li>FR1: Ingestione documenti multi-formato via Docling</li> <li>FR2: Generazione embeddings vettoriali (1536 dim)</li> <li>FR3: Storage PostgreSQL + PGVector</li> <li>FR4: Ricerca semantica (cosine similarity)</li> <li>FR5: Risposte RAG con citazioni fonti</li> <li>FR6: Filtraggio per fonte documentale</li> </ul>"},{"location":"epics/#mcp-server-observability","title":"MCP Server Observability","text":"<ul> <li>FR7: Tracking chiamate <code>query_knowledge_base</code></li> <li>FR8: Calcolo costo per query</li> <li>FR9: Trace LangFuse per operazioni MCP</li> <li>FR10: Breakdown timing (embedding, DB, LLM)</li> <li>FR11: Endpoint <code>/metrics</code> real-time</li> <li>FR12: Logging errori con stack trace</li> </ul>"},{"location":"epics/#mcp-server-architecture-fix","title":"MCP Server Architecture &amp; Fix","text":"<ul> <li>FR12.1: MCP server usa direttamente <code>core/rag_service.py</code> senza dipendenza API esterna</li> <li>FR12.2: MCP server standalone senza <code>api/main.py</code> richiesto</li> <li>FR12.3: MCP server organizzato in modulo <code>mcp/</code> con tools separati per dominio</li> <li>FR12.4: MCP server implementa pattern FastMCP nativi (lifespan, context injection)</li> <li>FR12.5: Tutti i tool e prompt MCP funzionano correttamente senza errori</li> <li>FR12.6: Gestione errori con messaggi informativi e graceful degradation</li> </ul>"},{"location":"epics/#streamlit-ui-observability","title":"Streamlit UI Observability","text":"<ul> <li>FR13: Session tracking con session_id</li> <li>FR14: Registrazione query utente</li> <li>FR15: Calcolo costi per sessione</li> <li>FR16: Statistiche sidebar</li> </ul>"},{"location":"epics/#cost-tracking-analytics","title":"Cost Tracking &amp; Analytics","text":"<ul> <li>FR17: Calcolo costo per token</li> <li>FR18: Aggregazione costi (daily/monthly)</li> <li>FR19: Export report CSV/JSON</li> <li>FR20: Dashboard LangFuse</li> </ul>"},{"location":"epics/#production-infrastructure","title":"Production Infrastructure","text":"<ul> <li>FR21: Linting (ruff) senza warning</li> <li>FR22: Type checking (mypy) senza errori</li> <li>FR23: Health check endpoints</li> <li>FR24: Docker deployment</li> <li>FR25: GitHub Actions CI/CD</li> </ul>"},{"location":"epics/#documentation-developer-experience","title":"Documentation &amp; Developer Experience","text":"<ul> <li>FR26: README con setup &lt; 5 min</li> <li>FR27: API documentation auto-generata</li> <li>FR28: Guida monitoring setup</li> <li>FR29: Esempi query con costi</li> <li>FR30: Badge GitHub</li> <li>FR30.1: Documentazione centralizzata in <code>docs/</code> senza file markdown sparsi</li> <li>FR30.2: Troubleshooting guide completa per MCP server</li> <li>FR30.3: Guida struttura progetto e organizzazione codice</li> </ul>"},{"location":"epics/#testing-quality-assurance-tdd","title":"Testing &amp; Quality Assurance (TDD)","text":"<ul> <li>FR31: Unit tests con coverage &gt; 70%</li> <li>FR32: PydanticAI TestModel per LLM mocking</li> <li>FR33: RAGAS evaluation suite</li> <li>FR34: Playwright E2E tests</li> <li>FR35: Integration tests MCP</li> <li>FR36: Tests in CI/CD</li> <li>FR37: Coverage report pubblicato</li> <li>FR38: Test fixtures database</li> <li>FR39: Golden dataset RAGAS (20+ pairs)</li> <li>FR40: Test results in LangFuse</li> </ul>"},{"location":"epics/#tdd-structure-organization","title":"TDD Structure &amp; Organization","text":"<ul> <li>FR41: Test suite organizzata rigorosamente in <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code></li> <li>FR42: Test fixtures in <code>tests/fixtures/</code> con golden dataset per RAGAS</li> <li>FR43: Test seguono pattern Red-Green-Refactor rigoroso (test prima del codice)</li> <li>FR44: Coverage report generato automaticamente in CI/CD con threshold &gt; 70%</li> </ul>"},{"location":"epics/#project-structure-organization","title":"Project Structure &amp; Organization","text":"<ul> <li>FR45: Progetto segue struttura directory rigorosa senza file sparsi in root</li> <li>FR46: Tutti i file markdown in <code>docs/</code> (eccetto README.md root)</li> <li>FR47: Tutti gli script organizzati in <code>scripts/</code> con sottodirectory per categoria</li> <li>FR48: Codice organizzato per responsabilit\u00e0 (mcp/, core/, ingestion/, utils/, api/)</li> <li>FR49: Zero file temporanei o di debug in root directory</li> </ul>"},{"location":"epics/#fr-coverage-map","title":"FR Coverage Map","text":"Epic Stories FRs Covered Epic 1 1.1, 1.2, 1.3, 1.4 FR1-FR6, FR26-FR30, FR30.1-FR30.3 Epic 2 2.1, 2.2, 2.3, 2.4, 2.5 FR7-FR12, FR12.1-FR12.6, FR17-FR20 Epic 3 3.1, 3.2 FR13-FR16 Epic 4 4.1, 4.2, 4.3 FR21-FR25 Epic 5 5.1, 5.2, 5.3, 5.4 FR31-FR40, FR41-FR44 Epic 6 6.1, 6.2 FR45-FR49"},{"location":"epics/#epic-1-core-rag-baseline-documentation","title":"Epic 1: Core RAG Baseline &amp; Documentation","text":"<p>Goal: Stabilire la baseline documentale del sistema esistente e creare fondamenta per monitoring. Questo epic fornisce l'infrastruttura di base (documentazione + API reference) necessaria per gli epic successivi.</p> <p>Why Foundation: Senza documentazione completa e API reference, l'implementazione di monitoring (Epic 2) sarebbe difficile da validare. Questo epic crea la \"single source of truth\" per il sistema.</p>"},{"location":"epics/#story-11-document-current-architecture","title":"Story 1.1: Document Current Architecture","text":"<p>As a developer, I want comprehensive documentation of the existing RAG architecture, So that I can understand the system before adding monitoring.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the current codebase, When I read <code>docs/architecture.md</code>, Then it accurately reflects all components (core, ingestion, utils, MCP, Streamlit)</li> <li>Given the architecture doc, When I review data flows, Then I see complete diagrams for ingestion and query pipelines</li> <li>Given the architecture doc, When I check component descriptions, Then each module has clear responsibilities documented</li> </ul> <p>Prerequisites: None (foundation story)</p> <p>Technical Notes: Update existing <code>docs/architecture.md</code> to reflect current state post-audio removal. Include MCP server architecture.</p>"},{"location":"epics/#story-12-generate-api-reference-documentation","title":"Story 1.2: Generate API Reference Documentation","text":"<p>As a developer, I want auto-generated API documentation for all public functions, So that I can quickly understand how to use and extend the system.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the codebase, When I run documentation generation, Then all public functions in <code>core/</code>, <code>ingestion/</code>, <code>utils/</code> have docstrings</li> <li>Given the generated docs, When I search for a function, Then I find parameters, return types, and usage examples</li> <li>Given the API docs, When I deploy them, Then they are accessible via GitHub Pages or local server</li> </ul> <p>Prerequisites: Story 1.1 (architecture understanding)</p> <p>Technical Notes: Use Sphinx or MkDocs. Configure auto-build in GitHub Actions.</p>"},{"location":"epics/#story-13-create-production-ready-readme","title":"Story 1.3: Create Production-Ready README","text":"<p>As a new developer, I want to setup the project locally in &lt; 5 minutes, So that I can start contributing immediately.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the README, When I follow setup instructions, Then I have a working local environment in &lt; 5 minutes</li> <li>Given the README, When I check prerequisites, Then all required tools are listed with version numbers</li> <li>Given the README, When I view the top, Then I see GitHub badges (build status, coverage, version)</li> </ul> <p>Prerequisites: Story 1.1, 1.2 (docs complete)</p> <p>Technical Notes: Include quick start, Docker setup, troubleshooting section. Add shields.io badges.</p>"},{"location":"epics/#story-14-centralize-documentation-and-add-troubleshooting-guide","title":"Story 1.4: Centralize Documentation and Add Troubleshooting Guide","text":"<p>As a developer, I want all documentation centralized in <code>docs/</code> with a complete troubleshooting guide, So that I can quickly find information and resolve issues.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the project root, When I check for markdown files, Then all <code>.md</code> files are in <code>docs/</code> (except README.md)</li> <li>Given the documentation, When I look for troubleshooting, Then I find complete troubleshooting guide for MCP server issues</li> <li>Given the documentation, When I check project structure, Then I find guide explaining directory organization and code structure</li> <li>Given scattered documentation files, When I review them, Then they are integrated into appropriate guides in <code>docs/</code></li> </ul> <p>Prerequisites: Story 1.1, 1.2, 1.3 (baseline docs complete)</p> <p>Technical Notes:</p> <ul> <li>Move all root-level <code>.md</code> files (except README.md) to <code>guide/</code></li> <li>Integrate content from: <code>flusso-mcp-tool.md</code>, <code>mat-FastMCP-e-architecture.md</code>, <code>MCP_TROUBLESHOOTING.md</code>, <code>pydantic_ai_testing_reference.md</code>, <code>walkthrough.md</code></li> <li>Create <code>guide/troubleshooting-guide.md</code> with MCP server troubleshooting section</li> <li>Add project structure guide to <code>guide/development-guide.md</code></li> <li>Note: <code>docs/</code> directory is reserved for BMAD workflow documentation</li> </ul> <p>FRs Covered: FR30.1, FR30.2, FR30.3</p>"},{"location":"epics/#epic-2-mcp-server-observability-langfuse","title":"Epic 2: MCP Server Observability (LangFuse)","text":"<p>Goal: Implementare monitoring completo per MCP server usando LangFuse, con cost tracking granulare e performance metrics real-time.</p> <p>Why Critical: MCP server \u00e8 l'interfaccia primaria per workflow production. Senza monitoring, impossibile ottimizzare costi o diagnosticare problemi.</p>"},{"location":"epics/#story-21-integrate-langfuse-sdk","title":"Story 2.1: Integrate LangFuse SDK","text":"<p>As a developer, I want LangFuse integrated in the MCP server, So that all operations are automatically traced.</p> <p>Acceptance Criteria:</p> <ul> <li>Given <code>mcp_server.py</code>, When I start the server, Then LangFuse client is initialized with API key from env</li> <li>Given a query, When <code>query_knowledge_base</code> is called, Then a new trace is created in LangFuse</li> <li>Given LangFuse dashboard, When I view traces, Then I see all MCP queries with timestamps</li> </ul> <p>Prerequisites: Story 1.1 (architecture understanding)</p> <p>Technical Notes: Use <code>langfuse</code> Python SDK. Configure via <code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_HOST</code> env vars.</p>"},{"location":"epics/#story-22-implement-cost-tracking","title":"Story 2.2: Implement Cost Tracking","text":"<p>As a product owner, I want to know the exact cost of each MCP query, So that I can budget and optimize spending.</p> <p>Acceptance Criteria:</p> <ul> <li>Given a query, When embeddings are generated, Then input tokens are counted and cost calculated</li> <li>Given a query, When LLM generates response, Then input/output tokens are counted and cost calculated</li> <li>Given LangFuse trace, When I view it, Then I see total cost breakdown (embedding + generation)</li> </ul> <p>Prerequisites: Story 2.1 (LangFuse integrated)</p> <p>Technical Notes: Use OpenAI pricing: <code>text-embedding-3-small</code> = $0.00002/1K tokens, <code>gpt-4o-mini</code> = $0.00015/1K input, $0.0006/1K output.</p>"},{"location":"epics/#story-23-add-performance-metrics","title":"Story 2.3: Add Performance Metrics","text":"<p>As a developer, I want detailed timing breakdown for each query, So that I can identify performance bottlenecks.</p> <p>Acceptance Criteria:</p> <ul> <li>Given a query, When it completes, Then I see timing for: embedding_time, db_search_time, llm_generation_time</li> <li>Given LangFuse trace, When I view spans, Then each component (embedder, DB, LLM) has separate span with duration</li> <li>Given metrics endpoint, When I query <code>/metrics</code>, Then I see Prometheus-format metrics (latency histograms, request count)</li> </ul> <p>Prerequisites: Story 2.1 (LangFuse integrated)</p> <p>Technical Notes: Use LangFuse spans for hierarchical timing. Add <code>prometheus_client</code> for <code>/metrics</code> endpoint.</p>"},{"location":"epics/#story-24-create-langfuse-dashboard","title":"Story 2.4: Create LangFuse Dashboard","text":"<p>As a product owner, I want a real-time dashboard showing MCP performance and costs, So that I can monitor the system without technical knowledge.</p> <p>Acceptance Criteria:</p> <ul> <li>Given LangFuse UI, When I open the dashboard, Then I see: total queries, avg latency, total cost (today/week/month)</li> <li>Given the dashboard, When I filter by date range, Then I see cost trends over time</li> <li>Given the dashboard, When I click a trace, Then I see full query details (input, output, cost, timing)</li> </ul> <p>Prerequisites: Story 2.1, 2.2, 2.3 (all monitoring implemented)</p> <p>Technical Notes: Configure LangFuse dashboard views. Create custom charts for cost trends.</p>"},{"location":"epics/#story-25-refactor-mcp-server-architecture-standalone","title":"Story 2.5: Refactor MCP Server Architecture (Standalone)","text":"<p>As a developer, I want the MCP server to work standalone without requiring an external API server, So that it's simpler to deploy and debug.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the MCP server, When I start it, Then it works without <code>api/main.py</code> running</li> <li>Given the codebase, When I check MCP server structure, Then it's organized in <code>mcp/</code> module with tools separated by domain (search, documents, overview)</li> <li>Given the MCP server, When I inspect it, Then it uses <code>core/rag_service.py</code> directly instead of <code>client/api_client.py</code></li> <li>Given the MCP server, When I check implementation, Then it uses FastMCP native patterns (lifespan management, context injection)</li> <li>Given all MCP tools, When I test them, Then <code>query_knowledge_base</code>, <code>list_knowledge_base_documents</code>, <code>get_knowledge_base_document</code>, <code>get_knowledge_base_overview</code>, and <code>ask_knowledge_base</code> all work correctly without errors</li> <li>Given an error occurs, When the MCP server handles it, Then it provides informative error messages and graceful degradation</li> </ul> <p>Prerequisites: Story 1.1 (architecture understanding), Story 2.1 (LangFuse integration for tracing refactoring)</p> <p>Technical Notes:</p> <ul> <li>Create <code>mcp/</code> directory structure:</li> <li><code>mcp/server.py</code> - FastMCP instance</li> <li><code>mcp/tools/search.py</code> - query_knowledge_base, ask_knowledge_base</li> <li><code>mcp/tools/documents.py</code> - list_knowledge_base_documents, get_knowledge_base_document</li> <li><code>mcp/tools/overview.py</code> - get_knowledge_base_overview</li> <li><code>mcp/lifespan.py</code> - Server lifecycle management</li> <li>Remove dependency on <code>client/api_client.py</code></li> <li>Use direct calls to <code>core/rag_service.py</code> functions</li> <li>Implement FastMCP lifespan pattern for resource initialization</li> <li>Add comprehensive error handling with informative messages</li> </ul> <p>FRs Covered: FR12.1, FR12.2, FR12.3, FR12.4, FR12.5, FR12.6</p>"},{"location":"epics/#epic-3-streamlit-ui-observability","title":"Epic 3: Streamlit UI Observability","text":"<p>Goal: Estendere monitoring a Streamlit UI con session tracking e cost visibility per utenti. Include security hardening opzionale per prevenire cost explosion e abuse.</p> <p>Security Hardening: Epic 3 include protezioni multi-layer documentate in <code>docs/stories/3/epic-3-security-hardening-guide.md</code>:</p> <ul> <li>Cost protection con threshold enforcement (priorit\u00e0 ALTA)</li> <li>Rate limiting per prevenzione abuse (priorit\u00e0 MEDIA)</li> <li>Network security (VPN/IP whitelist) (priorit\u00e0 ALTA)</li> <li>Streamlit authentication opzionale (priorit\u00e0 BASSA)</li> </ul> <p>Documentazione Completa:</p> <ul> <li>Tech Spec: <code>docs/stories/3/tech-spec-epic-3.md</code></li> <li>Security Hardening Guide: <code>docs/stories/3/epic-3-security-hardening-guide.md</code></li> <li>Setup Guide: <code>docs/stories/3/epic-3-setup-guide.md</code></li> </ul>"},{"location":"epics/#story-31-implement-session-tracking","title":"Story 3.1: Implement Session Tracking","text":"<p>As a Streamlit user, I want to see my session statistics in the sidebar, So that I know how many queries I've made and their total cost.</p> <p>Acceptance Criteria:</p> <ul> <li>Given a Streamlit session, When I open the app, Then a unique session_id is generated</li> <li>Given a query, When I submit it, Then it's logged with session_id, timestamp, and cost</li> <li>Given the sidebar, When I view it, Then I see: query count, total cost, avg latency for current session</li> <li>Given PostgreSQL, When session is initialized, Then record is created in <code>sessions</code> table with RLS protection</li> <li>Given a query, When it's processed, Then it's logged in <code>query_logs</code> table with cost and latency</li> </ul> <p>Prerequisites: Story 2.2 (cost tracking logic)</p> <p>Technical Notes:</p> <ul> <li>Use <code>st.session_state</code> for session_id persistence</li> <li>Store session data in PostgreSQL (<code>sessions</code> and <code>query_logs</code> tables)</li> <li>RLS enabled on all tables (service_role only access)</li> <li>Modules: <code>utils/session_manager.py</code> for session management</li> </ul> <p>Security Considerations:</p> <ul> <li>RLS Supabase protects database access</li> <li>Cost monitoring optional via <code>utils/cost_monitor.py</code> (see Security Hardening Guide)</li> </ul>"},{"location":"epics/#story-32-add-langfuse-tracing-to-streamlit","title":"Story 3.2: Add LangFuse Tracing to Streamlit","text":"<p>As a developer, I want Streamlit queries traced in LangFuse, So that I can compare MCP and UI performance.</p> <p>Acceptance Criteria:</p> <ul> <li>Given a Streamlit query, When it's processed, Then a LangFuse trace is created with session_id</li> <li>Given LangFuse dashboard, When I filter by source, Then I can separate MCP vs Streamlit queries</li> <li>Given a trace, When I view metadata, Then I see: user_agent, session_id, query_text</li> <li>Given LangFuse context injection, When trace is created, Then session_id propagates to all nested spans</li> </ul> <p>Prerequisites: Story 2.1 (LangFuse SDK), Story 3.1 (session tracking)</p> <p>Technical Notes:</p> <ul> <li>Tag traces with <code>source: streamlit</code> metadata</li> <li>Module: <code>utils/langfuse_streamlit.py</code> for context injection</li> <li>Graceful degradation if LangFuse unavailable</li> </ul>"},{"location":"epics/#epic-4-production-infrastructure-cicd","title":"Epic 4: Production Infrastructure &amp; CI/CD","text":"<p>Goal: Preparare il sistema per deployment GitHub con CI/CD, health checks, e quality gates.</p>"},{"location":"epics/#story-41-setup-github-actions-cicd","title":"Story 4.1: Setup GitHub Actions CI/CD","text":"<p>As a developer, I want automated testing and linting on every push, So that code quality is maintained automatically.</p> <p>Acceptance Criteria:</p> <ul> <li>Given a push to main, When GitHub Actions runs, Then all tests pass</li> <li>Given the CI pipeline, When it runs, Then ruff linting passes with zero warnings</li> <li>Given the CI pipeline, When it runs, Then mypy type checking passes with zero errors</li> </ul> <p>Prerequisites: Story 1.3 (README with setup)</p> <p>Technical Notes: Create <code>.github/workflows/ci.yml</code>. Run: <code>ruff check</code>, <code>mypy</code>, <code>pytest</code>.</p>"},{"location":"epics/#story-42-add-health-check-endpoints","title":"Story 4.2: Add Health Check Endpoints","text":"<p>As a DevOps engineer, I want health check endpoints for all services, So that I can monitor system status in production.</p> <p>Acceptance Criteria:</p> <ul> <li>Given MCP server, When I query <code>/health</code>, Then I get JSON response with status: ok/degraded/down</li> <li>Given Streamlit app, When I query <code>/_stcore/health</code>, Then I get 200 OK</li> <li>Given PostgreSQL, When health check runs, Then connection is verified</li> </ul> <p>Prerequisites: None (infrastructure story)</p> <p>Technical Notes: Add FastAPI <code>/health</code> endpoint to MCP server. Check DB connection, LangFuse connectivity.</p>"},{"location":"epics/#story-43-optimize-docker-images","title":"Story 4.3: Optimize Docker Images","text":"<p>As a DevOps engineer, I want Docker images &lt; 500MB, So that deployment is fast and cost-effective.</p> <p>Acceptance Criteria:</p> <ul> <li>Given Dockerfile, When I build it, Then final image size is &lt; 500MB</li> <li>Given docker-compose, When I run it, Then all services start in &lt; 30 seconds</li> <li>Given the images, When I inspect layers, Then I see multi-stage build optimization</li> </ul> <p>Prerequisites: Story 4.2 (health checks for validation)</p> <p>Technical Notes: Use multi-stage builds. Base image: <code>python:3.11-slim</code>. Remove build dependencies in final stage.</p>"},{"location":"epics/#epic-5-testing-quality-assurance-tdd","title":"Epic 5: Testing &amp; Quality Assurance (TDD)","text":"<p>Goal: Implementare Test-Driven Development con suite completa di unit tests, RAG evaluation, e E2E tests per garantire qualit\u00e0 production-ready.</p> <p>Why Critical: Senza testing robusto, impossibile validare monitoring accuracy, prevent regressions, o garantire RAG quality. TDD assicura che ogni feature sia testabile e testata.</p>"},{"location":"epics/#story-51-setup-testing-infrastructure-with-tdd-structure","title":"Story 5.1: Setup Testing Infrastructure with TDD Structure","text":"<p>As a developer, I want a complete testing infrastructure with rigorous TDD structure and pytest fixtures, So that I can write and run tests efficiently following Red-Green-Refactor pattern.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the project, When I run <code>pytest</code>, Then all tests are discovered and executed</li> <li>Given <code>tests/</code> directory, When I inspect it, Then I see rigorous organization: <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code>, <code>tests/fixtures/</code></li> <li>Given <code>tests/fixtures/</code>, When I check it, Then I see golden dataset for RAGAS evaluation (20+ query-answer pairs)</li> <li>Given pytest config, When I check it, Then I see async support, coverage tracking with threshold &gt; 70%, and markers configured</li> <li>Given CI/CD pipeline, When it runs, Then coverage report is generated automatically and build fails if coverage &lt; 70%</li> <li>Given test workflow, When I follow TDD, Then I write test first (Red), implement code (Green), then refactor (Refactor)</li> </ul> <p>Prerequisites: None (foundation story)</p> <p>Technical Notes:</p> <ul> <li>Install <code>pytest</code>, <code>pytest-asyncio</code>, <code>pytest-cov</code>, <code>pytest-mock</code></li> <li>Create <code>conftest.py</code> with shared fixtures</li> <li>Organize tests rigorously: <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code>, <code>tests/fixtures/</code></li> <li>Create <code>tests/fixtures/golden_dataset.json</code> with 20+ query-answer pairs for RAGAS</li> <li>Configure pytest.ini with coverage threshold &gt; 70%</li> <li>Setup CI/CD to generate coverage report and fail build if threshold not met</li> </ul> <p>FRs Covered: FR31, FR38, FR39, FR41, FR42, FR43, FR44</p>"},{"location":"epics/#story-52-implement-unit-tests-with-tdd","title":"Story 5.2: Implement Unit Tests with TDD","text":"<p>As a developer, I want unit tests for all core modules using PydanticAI TestModel, So that I can validate logic without API costs.</p> <p>Acceptance Criteria:</p> <ul> <li>Given <code>core/rag_service.py</code>, When I run unit tests, Then all functions are tested with mocked LLM</li> <li>Given <code>ingestion/embedder.py</code>, When I run tests, Then embedding logic is validated with TestModel</li> <li>Given coverage report, When I check it, Then core modules have &gt; 70% coverage</li> </ul> <p>Prerequisites: Story 5.1 (testing infrastructure)</p> <p>Technical Notes: Use <code>PydanticAI.TestModel</code> for LLM mocking. Set <code>ALLOW_MODEL_REQUESTS=False</code> in tests.</p>"},{"location":"epics/#story-53-implement-ragas-evaluation-suite","title":"Story 5.3: Implement RAGAS Evaluation Suite","text":"<p>As a product owner, I want RAGAS metrics to validate RAG quality, So that I can ensure high-quality responses.</p> <p>Acceptance Criteria:</p> <ul> <li>Given golden dataset (20+ query-answer pairs), When I run RAGAS eval, Then I see faithfulness, relevancy, precision, recall scores</li> <li>Given RAGAS results, When I check thresholds, Then faithfulness &gt; 0.85 and relevancy &gt; 0.80</li> <li>Given LangFuse, When I view eval results, Then I see RAGAS metrics tracked over time</li> </ul> <p>Prerequisites: Story 2.1 (LangFuse integration)</p> <p>Technical Notes: Install <code>ragas</code>. Create <code>tests/golden_dataset.json</code>. Integrate with LangFuse for tracking.</p>"},{"location":"epics/#story-54-implement-playwright-e2e-tests","title":"Story 5.4: Implement Playwright E2E Tests","text":"<p>As a QA engineer, I want E2E tests for critical Streamlit workflows, So that I can validate user experience.</p> <p>Acceptance Criteria:</p> <ul> <li>Given Streamlit app running, When Playwright test runs, Then it simulates user query and validates response</li> <li>Given E2E test, When it completes, Then I see screenshot/video recording for debugging</li> <li>Given CI/CD, When tests run, Then E2E tests execute in headless mode</li> </ul> <p>Prerequisites: Story 3.1 (Streamlit session tracking)</p> <p>Technical Notes: Install <code>playwright</code>. Create <code>tests/e2e/test_streamlit_workflow.py</code>. Use <code>data-testid</code> selectors.</p>"},{"location":"epics/#epic-6-project-structure-refactoring-organization","title":"Epic 6: Project Structure Refactoring &amp; Organization","text":"<p>Goal: Implementare struttura directory rigorosa eliminando file sparsi e organizzando codice per responsabilit\u00e0, seguendo best practices FastMCP e TDD.</p> <p>Why Critical: Struttura rigorosa \u00e8 fondamentale per mantenibilit\u00e0, testabilit\u00e0, e scalabilit\u00e0. File sparsi creano confusione e rendono difficile onboarding nuovi sviluppatori.</p>"},{"location":"epics/#story-61-reorganize-project-structure","title":"Story 6.1: Reorganize Project Structure","text":"<p>As a developer, I want a rigorous directory structure without scattered files in root, So that the project is maintainable and easy to navigate.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the project root, When I check for files, Then no markdown files exist except README.md</li> <li>Given the project root, When I check for Python files, Then no temporary or debug files exist (temp_query.py, debug_mcp_tools.py removed or moved)</li> <li>Given the scripts directory, When I inspect it, Then scripts are organized in subdirectories: <code>scripts/verification/</code>, <code>scripts/debug/</code></li> <li>Given the codebase, When I check organization, Then code is organized by responsibility: <code>mcp/</code>, <code>core/</code>, <code>ingestion/</code>, <code>utils/</code>, <code>api/</code></li> <li>Given the MCP server, When I check location, Then it's in <code>mcp/</code> module, not root</li> </ul> <p>Prerequisites: Story 1.4 (documentation centralized), Story 2.5 (MCP server refactored)</p> <p>Technical Notes:</p> <ul> <li>Remove <code>temp_query.py</code> from root</li> <li>Move <code>debug_mcp_tools.py</code> to <code>scripts/debug/</code> or remove</li> <li>Move <code>mcp_server.py</code> to <code>mcp/server.py</code> (if not already done in Story 2.5)</li> <li>Reorganize scripts: <code>scripts/verify_*.py</code> \u2192 <code>scripts/verification/</code></li> <li>Move <code>scripts/test_mcp_performance.py</code> to <code>tests/performance/</code> or keep in scripts if it's a utility</li> <li>Ensure zero temporary files in root</li> </ul> <p>FRs Covered: FR45, FR46, FR47, FR48, FR49</p>"},{"location":"epics/#story-62-clean-up-and-validate-structure","title":"Story 6.2: Clean Up and Validate Structure","text":"<p>As a developer, I want to verify that the project structure is rigorous and complete, So that I can confidently proceed with implementation.</p> <p>Acceptance Criteria:</p> <ul> <li>Given the project structure, When I validate it, Then all files are in appropriate directories</li> <li>Given the root directory, When I check it, Then only essential files exist (README.md, pyproject.toml, docker-compose.yml, app.py entry point)</li> <li>Given the documentation, When I check structure guide, Then it accurately reflects the new organization</li> <li>Given the codebase, When I check imports, Then all imports work correctly after reorganization</li> </ul> <p>Prerequisites: Story 6.1 (structure reorganized)</p> <p>Technical Notes:</p> <ul> <li>Run import validation: <code>python -m py_compile</code> on all modules</li> <li>Update any hardcoded paths in scripts/docs</li> <li>Verify Docker builds still work</li> <li>Update CI/CD paths if needed</li> </ul> <p>FRs Covered: FR45, FR48, FR49</p>"},{"location":"epics/#summary","title":"Summary","text":"<p>This breakdown defines 6 Epics and 20 Stories to execute the production-ready transformation.</p> <p>Epic Sequence:</p> <ol> <li>Epic 1 (Foundation) \u2192 Establishes documentation baseline</li> <li>Epic 2 (MCP Monitoring + Architecture Fix) \u2192 Core observability + standalone MCP server</li> <li>Epic 3 (Streamlit Monitoring) \u2192 Extends monitoring to UI</li> <li>Epic 4 (Production Infra) \u2192 Deployment readiness</li> <li>Epic 5 (Testing &amp; QA) \u2192 TDD implementation with rigorous structure</li> <li>Epic 6 (Project Structure) \u2192 Rigorous organization and cleanup</li> </ol> <p>Total FRs Covered: 49/49 (100%)</p> <p>Estimated Timeline: 7-9 weeks (assuming 1 developer, part-time)</p> <p>For implementation: Use the <code>create-story</code> workflow to generate individual story implementation plans from this epic breakdown.</p> <p>This document will be updated after Architecture workflow to incorporate technical design decisions.</p>"},{"location":"getting-started/","title":"Docling RAG Agent","text":"<p>Un agente intelligente basato su Streamlit che fornisce accesso conversazionale a una knowledge base archiviata in PostgreSQL con PGVector. Utilizza RAG (Retrieval Augmented Generation) per cercare nei documenti embedded e fornire risposte contestuali e accurate con citazioni delle fonti. Supporta molteplici formati di documenti.</p>"},{"location":"getting-started/#nuovo-a-docling","title":"\ud83c\udf93 Nuovo a Docling?","text":"<p>Inizia dai tutorial! Consulta la cartella <code>docling_basics/</code> nella root del progetto per esempi progressivi che insegnano i fondamenti di Docling:</p> <ol> <li>Conversione PDF Semplice - Elaborazione base dei documenti</li> <li>Supporto Formati Multipli - Gestione PDF, Word, PowerPoint</li> <li>Chunking Ibrido - Chunking intelligente per sistemi RAG</li> </ol> <p>Questi tutorial forniscono le basi per comprendere come funziona questo agente RAG completo.</p>"},{"location":"getting-started/#funzionalita","title":"Funzionalit\u00e0","text":"<ul> <li>\ud83d\udcac Interfaccia chat web interattiva con Streamlit</li> <li>\ud83d\udd0d Ricerca semantica attraverso documenti vettoriali embedded</li> <li>\ud83d\udcda Risposte context-aware usando pipeline RAG</li> <li>\ud83c\udfaf Citazione delle fonti per tutte le informazioni fornite</li> <li>\ud83d\udd04 Output di testo in streaming real-time mentre arrivano i token</li> <li>\ud83d\udcbe PostgreSQL/PGVector per storage scalabile della conoscenza</li> <li>\ud83e\udde0 Cronologia conversazione mantenuta tra i turni</li> </ul>"},{"location":"getting-started/#prerequisiti","title":"Prerequisiti","text":"Tool Versione Link Installazione Python 3.10+ (3.11 raccomandato) python.org/downloads UV 0.9.13+ (latest raccomandato) docs.astral.sh/uv PostgreSQL 16+ con PGVector postgresql.org Docker latest (opzionale) docker.com <p>API Keys richieste:</p> <ul> <li><code>OPENAI_API_KEY</code> - OpenAI API key per embeddings e LLM (ottienila qui)</li> </ul> <p>Provider Database supportati:</p> <ul> <li>PostgreSQL self-hosted con estensione PGVector</li> <li>Supabase (managed PostgreSQL)</li> <li>Neon (serverless PostgreSQL)</li> </ul>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>Tempo totale stimato: ~3-4 minuti (esclusa installazione prerequisiti)</p>"},{"location":"getting-started/#1-clona-e-installa-dipendenze-30-secondi","title":"1. Clona e Installa Dipendenze (~30 secondi)","text":"<pre><code># Clona il repository\ngit clone https://github.com/stefanopiga/Agent-RAG.git\ncd Agent-RAG\n\n# Installa le dipendenze usando UV\nuv sync\n</code></pre>"},{"location":"getting-started/#2-configura-le-variabili-dambiente-30-secondi","title":"2. Configura le Variabili d'Ambiente (~30 secondi)","text":"<pre><code>cp .env.example .env\n</code></pre> <p>Modifica <code>.env</code> con le tue credenziali:</p> <pre><code># RICHIESTO: Connessione database PostgreSQL con PGVector\nDATABASE_URL=postgresql://user:password@localhost:5432/dbname\n\n# RICHIESTO: OpenAI API key\nOPENAI_API_KEY=sk-...\n\n# OPZIONALE: Modello LLM (default: gpt-4o-mini)\nLLM_CHOICE=gpt-4o-mini\n\n# OPZIONALE: Modello embedding (default: text-embedding-3-small)\nEMBEDDING_MODEL=text-embedding-3-small\n\n# OPZIONALE: LangFuse per observability/tracing (MCP server)\nLANGFUSE_PUBLIC_KEY=pk-lf-...\nLANGFUSE_SECRET_KEY=sk-lf-...\nLANGFUSE_BASE_URL=https://cloud.langfuse.com\n</code></pre> <p>Esempi DATABASE_URL per provider:</p> <ul> <li>Self-hosted: <code>postgresql://user:password@localhost:5432/dbname</code></li> <li>Supabase: <code>postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:5432/postgres</code></li> <li>Neon: <code>postgresql://[user]:[password]@[endpoint].neon.tech/[dbname]</code></li> </ul>"},{"location":"getting-started/#3-configura-il-database-1-minuto","title":"3. Configura il Database (~1 minuto)","text":"<p>Fresh install:</p> <pre><code># Esegui lo schema completo (include HNSW index ottimizzato)\npsql $DATABASE_URL &lt; sql/optimize_index.sql\n</code></pre> <p>Per Supabase/Neon: Usa l'editor SQL integrato e incolla il contenuto di <code>sql/optimize_index.sql</code>.</p> <p>Upgrade database esistente:</p> <pre><code>python scripts/optimize_database.py --apply\n</code></pre> <p>Il file SQL (<code>sql/optimize_index.sql</code>) crea:</p> <ul> <li>\u2705 Estensioni richieste (<code>vector</code>, <code>uuid-ossp</code>, <code>pg_trgm</code>)</li> <li>\u2705 Tabella <code>documents</code> per memorizzare documenti originali con metadati</li> <li>\u2705 Tabella <code>chunks</code> per chunk di testo con embeddings a 1536 dimensioni</li> <li>\u2705 Funzione <code>match_chunks()</code> per ricerca di similarit\u00e0 vettoriale</li> <li>\u2705 HNSW index ottimizzato (10-100x pi\u00f9 veloce del vecchio IVFFlat)</li> <li>\u2705 Performance indexes per filtering e source queries</li> </ul> <p>Epic 3 - Session Tracking (Opzionale):</p> <p>Per abilitare session tracking e cost visibility nella Streamlit UI:</p> <pre><code># Esegui schema Epic 3 (crea tabelle sessions e query_logs)\n# Per Supabase: Usa SQL Editor e incolla contenuto di sql/epic-3-sessions-schema.sql\npsql $DATABASE_URL &lt; sql/epic-3-sessions-schema.sql\n</code></pre> <p>Questo crea:</p> <ul> <li>\u2705 Tabella <code>sessions</code> per session tracking con statistiche (query_count, total_cost, avg_latency)</li> <li>\u2705 Tabella <code>query_logs</code> per logging query con costi e timing</li> <li>\u2705 RLS policies <code>service_role</code> only (protezione completa)</li> <li>\u2705 Indexes per performance (session_id, timestamp lookups)</li> </ul> <p>Vantaggi HNSW:</p> <ul> <li>\ud83d\ude80 50-80% pi\u00f9 veloce nelle ricerche vettoriali</li> <li>\ud83d\udcc8 Performance consistente al crescere del dataset</li> <li>\ud83c\udfaf Ottimale per &lt;1M vectors (case d'uso RAG tipici)</li> </ul>"},{"location":"getting-started/#4-ingerisci-i-documenti-1-2-minuti-per-setup-iniziale","title":"4. Ingerisci i Documenti (~1-2 minuti per setup iniziale)","text":"<p>Aggiungi i tuoi documenti nella cartella <code>documents/</code>. Supportati molteplici formati tramite Docling:</p> <p>Formati Supportati:</p> <ul> <li>\ud83d\udcc4 PDF (<code>.pdf</code>)</li> <li>\ud83d\udcdd Word (<code>.docx</code>, <code>.doc</code>)</li> <li>\ud83d\udcca PowerPoint (<code>.pptx</code>, <code>.ppt</code>)</li> <li>\ud83d\udcc8 Excel (<code>.xlsx</code>, <code>.xls</code>)</li> <li>\ud83c\udf10 HTML (<code>.html</code>, <code>.htm</code>)</li> <li>\ud83d\udccb Markdown (<code>.md</code>, <code>.markdown</code>)</li> <li>\ud83d\udcc3 Testo (<code>.txt</code>)</li> </ul> <pre><code># Ingerisci tutti i documenti supportati nella cartella documents/\n# NOTA: Di default, questo CANCELLA i dati esistenti prima dell'ingestione\nuv run python -m ingestion.ingest --documents documents/\n\n# Regola la dimensione dei chunk (default: 1000)\nuv run python -m ingestion.ingest --documents documents/ --chunk-size 800\n</code></pre> <p>\u26a0\ufe0f Importante: Il processo di ingestione cancella automaticamente tutti i documenti e chunk esistenti dal database prima di aggiungere nuovi documenti. Questo assicura uno stato pulito e previene duplicati.</p> <p>La pipeline di ingestione:</p> <ol> <li>Auto-rileva il tipo di file e usa Docling per PDF, documenti Office e HTML</li> <li>Converte in Markdown per elaborazione consistente</li> <li>Divide in chunk semantici con dimensione configurabile</li> <li>Genera embeddings usando OpenAI</li> <li>Memorizza in PostgreSQL con PGVector per ricerca di similarit\u00e0</li> </ol> <p>\ud83d\udca1 Best Practice - Organizzazione Documenti:</p> <p>Organizza i documenti in sottocartelle per sfruttare il filtraggio per fonte:</p> <pre><code>documents/\n\u251c\u2500\u2500 docling/\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u2514\u2500\u2500 architecture.md\n\u2502   \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 langfuse-docs/\n\u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u251c\u2500\u2500 cloud.md\n\u2502   \u2502   \u2514\u2500\u2500 docker.md\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u2514\u2500\u2500 tracing.md\n\u2502   \u2514\u2500\u2500 guides/\n\u2514\u2500\u2500 company-docs/\n    \u251c\u2500\u2500 policies.pdf\n    \u2514\u2500\u2500 procedures.docx\n</code></pre> <p>Vantaggi:</p> <ul> <li>\u2705 Citazioni precise con percorso completo</li> <li>\u2705 Filtraggio per documentazione specifica (\"cerca solo in Docling docs\")</li> <li>\u2705 Facile manutenzione e aggiornamenti</li> <li>\u2705 Separazione logica tra diverse fonti di conoscenza</li> </ul>"},{"location":"getting-started/#5-esegui-lapplicazione-10-secondi","title":"5. Esegui l'Applicazione (~10 secondi)","text":"<pre><code>streamlit run app.py\n</code></pre> <p>L'applicazione si aprir\u00e0 automaticamente nel browser: <code>http://localhost:8501</code></p> <p>Funzionalit\u00e0:</p> <ul> <li>\ud83d\udcac Interfaccia chat con cronologia messaggi</li> <li>\ud83d\udcca Pulsante cancella chat per resettare la conversazione</li> <li>\ud83c\udfa8 UI moderna costruita con Streamlit</li> <li>\ud83d\udd0d Streaming real-time delle risposte</li> <li>\ud83d\udcda Stato della knowledge base nella sidebar</li> </ul> <p>Esempio di interazione:</p> <pre><code>Tu: Quali argomenti sono trattati nella knowledge base?\nAssistant: Sulla base della knowledge base, gli argomenti principali includono...\n\n[Fonte: company-overview.md]\nContenuto dal documento...\n</code></pre>"},{"location":"getting-started/#architettura","title":"Architettura","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Streamlit  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  RAG Agent   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 PostgreSQL  \u2502\n\u2502     UI      \u2502     \u2502 (PydanticAI) \u2502     \u2502  PGVector   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  OpenAI  \u2502  \u2502  OpenAI  \u2502\n              \u2502   LLM    \u2502  \u2502Embeddings\u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/#componenti-chiave","title":"Componenti Chiave","text":""},{"location":"getting-started/#rag-agent","title":"RAG Agent","text":"<p>L'agente principale (<code>rag_agent.py</code>) che:</p> <ul> <li>Gestisce le connessioni al database con connection pooling</li> <li>Fornisce il tool search_knowledge_base per operazioni RAG</li> <li>Si integra con l'interfaccia Streamlit tramite app.py</li> <li>Traccia la cronologia della conversazione per il contesto</li> </ul>"},{"location":"getting-started/#tool-search_knowledge_base","title":"Tool search_knowledge_base","text":"<p>Funzione tool registrata con l'agente che:</p> <ul> <li>Genera query embeddings usando OpenAI</li> <li>Effettua ricerche usando similarit\u00e0 coseno PGVector</li> <li>Restituisce i top-k chunk pi\u00f9 rilevanti</li> <li>Formatta i risultati con citazioni delle fonti</li> <li>Supporta filtraggio per fonte per ricerche mirate in documentazioni specifiche</li> </ul> <p>Capacit\u00e0 di Filtraggio Avanzato: L'agente pu\u00f2 filtrare automaticamente le ricerche per fonte specifica quando richiesto dall'utente.</p> <p>Esempi di query con filtraggio:</p> <ul> <li>\"Come fare il deploy di Docling usando solo la documentazione Docling?\"</li> <li>\"Spiega il tracing secondo Langfuse docs, non guardare altre fonti\"</li> <li>\"Deployment in Langfuse\" \u2192 filtra automaticamente su <code>langfuse-docs</code></li> </ul> <p>Esempio definizione tool:</p> <pre><code>async def search_knowledge_base(\n    ctx: RunContext[None],\n    query: str,\n    limit: int = 5,\n    source_filter: str | None = None  # Filtro opzionale per fonte\n) -&gt; str:\n    \"\"\"Cerca nella knowledge base usando similarit\u00e0 semantica.\n\n    Args:\n        query: Query di ricerca\n        limit: Numero massimo risultati\n        source_filter: Filtro opzionale (es: \"docling\", \"langfuse-docs\")\n    \"\"\"\n    # Genera embedding per la query\n    # Cerca in PostgreSQL con PGVector\n    # Applica filtro fonte se specificato\n    # Formatta e restituisce i risultati\n</code></pre>"},{"location":"getting-started/#schema-database","title":"Schema Database","text":"<ul> <li> <p><code>documents</code>: Memorizza documenti originali con metadati</p> </li> <li> <p><code>id</code>, <code>title</code>, <code>source</code>, <code>content</code>, <code>metadata</code>, <code>created_at</code>, <code>updated_at</code></p> </li> <li> <p><code>chunks</code>: Memorizza chunk di testo con embeddings vettoriali</p> </li> <li> <p><code>id</code>, <code>document_id</code>, <code>content</code>, <code>embedding</code> (vector(1536)), <code>chunk_index</code>, <code>metadata</code>, <code>token_count</code></p> </li> <li> <p><code>match_chunks()</code>: Funzione PostgreSQL per ricerca similarit\u00e0 vettoriale</p> </li> <li>Usa similarit\u00e0 coseno (<code>1 - (embedding &lt;=&gt; query_embedding)</code>)</li> <li>Restituisce chunk con punteggi di similarit\u00e0 sopra la soglia</li> </ul>"},{"location":"getting-started/#mcp-server-per-cursor-ide","title":"\ud83d\ude80 MCP Server per Cursor IDE","text":"<p>Il progetto include un Model Context Protocol (MCP) server ottimizzato per integrazione con Cursor IDE.</p> <p>Setup MCP:</p> <ol> <li>Configura <code>.cursor/mcp.json</code>:</li> </ol> <pre><code>{\n  \"mcpServers\": {\n    \"docling-rag\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--project\",\n        \"/path/to/docling-rag-agent\",\n        \"python\",\n        \"-m\",\n        \"docling_mcp.server\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/path/to/docling-rag-agent\"\n      }\n    }\n  }\n}\n</code></pre> <ol> <li> <p>Restart Cursor - MCP server si avvia automaticamente</p> </li> <li> <p>Usa il tool in Cursor:</p> </li> </ol> <pre><code>Chiedi: \"What is Docling?\"\nIl MCP tool cercher\u00e0 nella knowledge base e risponder\u00e0 con context\n</code></pre> <p>Performance MCP:</p> <ul> <li>\u2705 Global embedder instance: -70% latency (eliminato overhead 300-500ms)</li> <li>\u2705 HNSW index: -61% query time (1395ms avg, 237ms con cache)</li> <li>\u2705 Connection pool ottimizzato: -20% overhead</li> <li>\u2705 Timing instrumentation per monitoring</li> </ul> <p>Documentazione completa:</p> <ul> <li><code>docs/performance-optimization-guide.md</code> - Guida tecnica dettagliata</li> <li><code>docs/optimization-summary.md</code> - Analisi risultati</li> <li><code>docs/optimization-deployment.md</code> - Deploy guide</li> </ul>"},{"location":"getting-started/#code-quality-cicd","title":"\ud83e\udd16 Code Quality &amp; CI/CD","text":""},{"location":"getting-started/#github-mcp-server-development-standard","title":"GitHub MCP Server (Development Standard)","text":"<p>Il progetto utilizza il GitHub MCP Server come standard per operazioni GitHub automatizzate in Cursor IDE.</p> <p>Configurazione <code>.cursor/mcp.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"&lt;your-token&gt;\"\n      }\n    }\n  }\n}\n</code></pre> <p>Funzionalit\u00e0 disponibili:</p> <ul> <li>Branch management: <code>create_branch</code>, <code>list_branches</code></li> <li>File operations: <code>push_files</code>, <code>create_or_update_file</code></li> <li>Pull requests: <code>create_pull_request</code>, <code>merge_pull_request</code></li> <li>Issues: <code>list_issues</code>, <code>issue_write</code>, <code>add_issue_comment</code></li> <li>Code search: <code>search_code</code>, <code>search_repositories</code></li> </ul>"},{"location":"getting-started/#coderabbit-integration","title":"CodeRabbit Integration","text":"<p>Il repository utilizza CodeRabbit per code review AI-powered automatica su ogni pull request.</p> <p>Funzionalit\u00e0:</p> <ul> <li>\u2705 Review automatica su ogni PR</li> <li>\ud83d\udcdd Suggerimenti inline e best practices</li> <li>\ud83d\udd12 Security analysis automatica</li> <li>\ud83d\udcca High-level summary delle modifiche</li> </ul>"},{"location":"getting-started/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>GitHub Actions esegue automaticamente su ogni PR e push a <code>main</code>/<code>develop</code>:</p> Job Descrizione Tool lint Linting e format check Ruff type-check Type checking statico Mypy test Unit/integration tests con coverage &gt;70% Pytest build Docker image build validation (&lt;500MB) Docker Buildx secret-scan Secret scanning preventivo TruffleHog OSS"},{"location":"getting-started/#developer-resources","title":"\ud83d\udcda Developer Resources","text":""},{"location":"getting-started/#documentation","title":"Documentation","text":"<ul> <li>Architecture: Complete system architecture, design decisions, and component descriptions</li> <li>Coding Standards: Code style guide, naming conventions, documentation standards, and best practices</li> <li>Testing Strategy: TDD workflow, test organization, RAGAS evaluation, and CI/CD integration</li> <li>Development Guide: Setup instructions, development workflow, and troubleshooting</li> <li>Documentation Index: Complete documentation index with quick links and navigation</li> </ul>"},{"location":"getting-started/#quick-links","title":"Quick Links","text":"<ul> <li>Code Style: See Coding Standards for Python style guide, type hints, error handling patterns</li> <li>Writing Tests: See Testing Strategy for TDD approach, test organization, coverage requirements</li> <li>Project Structure: See Unified Project Structure for directory organization, file placement rules, epic mapping</li> <li>Architecture Patterns: See Architecture for design decisions, integration patterns, observability</li> </ul>"},{"location":"getting-started/#langfuse-observability","title":"\ud83d\udcca LangFuse Observability","text":"<p>Il MCP server include integrazione con LangFuse per tracciamento e osservabilit\u00e0 delle chiamate AI.</p>"},{"location":"getting-started/#setup-langfuse","title":"Setup LangFuse","text":"<ol> <li> <p>Crea un account LangFuse:</p> </li> <li> <p>Cloud: cloud.langfuse.com</p> </li> <li> <p>Self-hosted: docs.langfuse.com/self-hosting</p> </li> <li> <p>Configura le variabili d'ambiente:</p> </li> </ol> <pre><code># LangFuse Configuration (opzionale)\nLANGFUSE_PUBLIC_KEY=pk-lf-...\nLANGFUSE_SECRET_KEY=sk-lf-...\nLANGFUSE_BASE_URL=https://cloud.langfuse.com  # Default, oppure URL self-hosted\n</code></pre> <ol> <li>Restart il server MCP - Le tracce appariranno automaticamente in LangFuse</li> </ol>"},{"location":"getting-started/#funzionalita-tracing","title":"Funzionalit\u00e0 Tracing","text":"<ul> <li>\ud83d\udd0d Tracciamento automatico di tutte le chiamate MCP tools</li> <li>\ud83d\udcca Metadata dettagliati: tool_name, query, limit, source</li> <li>\u23f1\ufe0f Timing e performance per ogni operazione</li> <li>\ud83d\udd04 Graceful degradation: il sistema funziona anche senza LangFuse</li> </ul>"},{"location":"getting-started/#cost-tracking-story-22","title":"Cost Tracking (Story 2.2)","text":"<p>Il sistema traccia automaticamente i costi di utilizzo API OpenAI:</p> <ul> <li>\ud83d\udcb0 Embedding cost tracking: Costo per generazione embedding via <code>langfuse.openai</code> wrapper</li> <li>\ud83d\udcc8 Token counting automatico: Input tokens per embedding, input/output tokens per LLM</li> <li>\ud83d\udcb5 Pricing aggiornato: LangFuse SDK aggiorna automaticamente i prezzi OpenAI</li> <li>\ud83d\udcca Cost breakdown: Visualizzazione costi separati per embedding e LLM generation</li> </ul> <p>Modelli e Pricing Tracciati:</p> Modello Tipo Costo <code>text-embedding-3-small</code> Embedding $0.00002/1K tokens <code>gpt-4o-mini</code> LLM (input) $0.00015/1K tokens <code>gpt-4o-mini</code> LLM (output) $0.0006/1K tokens <p>Come funziona:</p> <ol> <li>Il wrapper <code>langfuse.openai</code> sostituisce il client OpenAI diretto</li> <li>Ogni chiamata API viene automaticamente tracciata con token count</li> <li>I costi vengono calcolati usando i prezzi OpenAI correnti</li> <li>Il cost breakdown \u00e8 visibile nel dashboard LangFuse per ogni trace</li> </ol> <p>Nota: Il cost tracking \u00e8 automatico e non richiede codice aggiuntivo. Se LangFuse non \u00e8 configurato, il sistema funziona normalmente senza tracking (graceful degradation).</p>"},{"location":"getting-started/#performance-metrics-story-23","title":"Performance Metrics (Story 2.3)","text":"<p>Il sistema espone metriche Prometheus per il monitoraggio delle performance tramite un server HTTP dedicato.</p> <p>Endpoint disponibili:</p> Endpoint Formato Descrizione <code>/metrics</code> Prometheus Metriche in formato OpenMetrics (text/plain) <code>/health</code> JSON Health check con status servizi (ok/degraded/down) <p>Metriche Prometheus:</p> Metrica Tipo Descrizione <code>mcp_requests_total</code> Counter Totale richieste MCP (label: tool, status) <code>mcp_request_duration_seconds</code> Histogram Latenza richieste (buckets SLO-aligned) <code>rag_embedding_time_seconds</code> Histogram Tempo generazione embedding <code>rag_db_search_time_seconds</code> Histogram Tempo ricerca database <code>rag_llm_generation_time_seconds</code> Histogram Tempo generazione LLM <code>mcp_active_requests</code> Gauge Richieste attive concorrenti <p>Configurazione Prometheus (<code>prometheus.yml</code>):</p> <pre><code>scrape_configs:\n  - job_name: \"docling-rag-agent\"\n    scrape_interval: 15s # Default raccomandato (60s per cost-sensitive)\n    static_configs:\n      - targets: [\"localhost:8080\"]\n</code></pre> <p>Avvio server metriche:</p> <pre><code># Avvia il server HTTP per metriche e health check (porta 8080)\nuv run python -m docling_mcp.http_server\n\n# Oppure con porta personalizzata\nMETRICS_PORT=9090 uv run python -m docling_mcp.http_server\n</code></pre> <p>Health Check Response:</p> <pre><code>{\n  \"status\": \"ok\",\n  \"timestamp\": 1732700400.123,\n  \"services\": {\n    \"database\": {\n      \"status\": \"up\",\n      \"message\": \"PostgreSQL connection successful\",\n      \"latency_ms\": 5.2\n    },\n    \"langfuse\": {\n      \"status\": \"up\",\n      \"message\": \"LangFuse client initialized\",\n      \"latency_ms\": 0.1\n    },\n    \"embedder\": {\n      \"status\": \"up\",\n      \"message\": \"Embedder initialized and ready\",\n      \"latency_ms\": 0.5\n    }\n  }\n}\n</code></pre> <p>Status Logic:</p> <ul> <li><code>ok</code>: Tutti i servizi operativi</li> <li><code>degraded</code>: LangFuse non disponibile (graceful degradation, MCP funziona)</li> <li><code>down</code>: Database o embedder non disponibili (servizi critici)</li> </ul>"},{"location":"getting-started/#visualizzazione-tracce","title":"Visualizzazione Tracce","text":"<p>Nel dashboard LangFuse vedrai tracce per ogni chiamata tool:</p> Tool Metadata Tracciati <code>query_knowledge_base</code> query, limit, source_filter, source=\"mcp\" <code>ask_knowledge_base</code> question, limit, source=\"mcp\" <code>list_knowledge_base_documents</code> limit, offset, source=\"mcp\" <code>get_knowledge_base_document</code> document_id, source=\"mcp\" <code>get_knowledge_base_overview</code> source=\"mcp\" <p>Nota: Se le variabili LangFuse non sono configurate, il server funziona normalmente senza tracing (graceful degradation).</p>"},{"location":"getting-started/#langfuse-dashboard-story-24","title":"LangFuse Dashboard (Story 2.4)","text":"<p>Il dashboard LangFuse fornisce una visualizzazione real-time delle performance e dei costi del MCP server.</p> <p>Metriche Dashboard:</p> Metrica Descrizione Fonte Total Queries Numero totale richieste Conteggio trace Avg Latency Latenza media Campo <code>latency</code> trace Total Cost Costo totale (embedding + LLM) Token usage automatico <p>Visualizzazioni Disponibili:</p> <ul> <li>Cost Trends: Grafico costi nel tempo (daily/weekly/monthly)</li> <li>Latency Distribution: Distribuzione latenza per tool</li> <li>Trace Detail View: Input, output, cost breakdown, timing breakdown, nested spans</li> </ul> <p>Trace Detail View:</p> <p>Ogni trace mostra:</p> <ul> <li>Input query text</li> <li>Output response</li> <li>Cost breakdown: <code>embedding_cost</code> + <code>llm_generation_cost</code></li> <li>Timing breakdown: <code>embedding_time</code>, <code>db_search_time</code>, <code>llm_generation_time</code></li> <li>Nested spans: <code>embedding-generation</code>, <code>vector-search</code>, <code>llm-generation</code></li> </ul> <p>Custom Charts:</p> <p>LangFuse supporta chart personalizzati configurabili con:</p> <ul> <li>Dimensioni: <code>time</code>, <code>tool_name</code>, <code>metadata.source</code></li> <li>Metriche: <code>count</code>, <code>totalCost</code>, <code>avgLatency</code>, <code>p95Latency</code></li> </ul> <p>Guida Completa: Consulta LangFuse Dashboard Guide per la configurazione dettagliata del dashboard.</p>"},{"location":"getting-started/#ottimizzazione-delle-prestazioni","title":"Ottimizzazione delle Prestazioni","text":""},{"location":"getting-started/#performance-optimizations-2025-11","title":"\ud83c\udfaf Performance Optimizations (2025-11)","text":"<p>Il sistema \u00e8 stato ottimizzato per performance reattive:</p> <p>Ottimizzazioni Applicate:</p> <ol> <li>HNSW Vector Index (Critical)</li> <li>Upgrade da IVFFlat (<code>lists=1</code>) a HNSW</li> <li>10-100x pi\u00f9 veloce per ricerche vettoriali</li> <li>Performance consistente al crescere del dataset</li> <li> <p>Global Embedder Instance</p> </li> <li> <p>Embedder singleton con cache persistente</p> </li> <li>Eliminato overhead 300-500ms per query</li> <li> <p>Cache hit rate: 66%</p> </li> <li> <p>Connection Pool Ottimizzato</p> </li> </ol> <p><code>python    db_pool = await asyncpg.create_pool(        DATABASE_URL,        min_size=2,              # Ottimizzato per MCP burst traffic        max_size=10,             # Sufficient per concurrent queries        statement_cache_size=100 # Prepared statements abilitati    )</code></p> <ol> <li>Performance Instrumentation</li> <li>Timing breakdown dettagliato per componente</li> <li>Monitoring logs: <code>embedding_ms | db_ms | total_ms</code></li> </ol> <p>Performance Baseline:</p> Metrica Before After Improvement Avg Query 3563ms 1395ms -61% Cached Query 298ms 237ms -20% DB Search 100-300ms 20-60ms -75% <p>Performance Tools:</p> <pre><code># Verifica status DB index\npython scripts/optimize_database.py --check\n\n# Applica ottimizzazioni (upgrade HNSW index)\npython scripts/optimize_database.py --apply\n\n# Test performance\npython scripts/test_mcp_performance.py\n</code></pre>"},{"location":"getting-started/#cache-embedding","title":"Cache Embedding","text":"<p>L'embedder include caching LRU (2000 entries) per query frequenti, riducendo chiamate OpenAI API del 60-70%.</p>"},{"location":"getting-started/#risposte-in-streaming","title":"Risposte in Streaming","text":"<p>Lo streaming token-per-token fornisce feedback immediato:</p> <pre><code>async with agent.run_stream(user_input, message_history=history) as result:\n    async for text in result.stream_text(delta=False):\n        print(f\"\\rAssistant: {text}\", end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/#funzionalita-avanzate","title":"Funzionalit\u00e0 Avanzate","text":""},{"location":"getting-started/#filtraggio-per-fonte-documentale","title":"Filtraggio per Fonte Documentale","text":"<p>L'agente supporta il filtraggio intelligente delle ricerche per fonte specifica, permettendo di isolare documentazioni diverse.</p> <p>Come funziona: L'utente pu\u00f2 richiedere informazioni da fonti specifiche e l'agente applica automaticamente il filtro appropriato.</p> <p>Esempi di query:</p> Query Utente Comportamento Agente \"Come fare il deploy?\" Cerca in tutta la knowledge base \"Come fare il deploy di Docling usando solo Docling docs?\" Filtra automaticamente su <code>source_filter=\"docling\"</code> \"Spiega il tracing in Langfuse, non guardare altre fonti\" Filtra su <code>source_filter=\"langfuse-docs\"</code> \"Deployment secondo Langfuse deployment docs\" Filtra su <code>source_filter=\"langfuse-docs/deployment\"</code> <p>Vantaggi:</p> <ul> <li>\ud83c\udfaf Risposte precise da fonti specifiche</li> <li>\ud83d\udeab Zero contaminazione tra documentazioni diverse</li> <li>\ud83e\udd16 Completamente automatico - l'agente riconosce l'intento</li> <li>\ud83d\udcc1 Pattern flessibile - supporta percorsi parziali</li> </ul> <p>Query SQL con filtro:</p> <pre><code>SELECT c.content, d.title, d.source\nFROM chunks c\nJOIN documents d ON c.document_id = d.id\nWHERE d.source ILIKE '%docling%'  -- Filtro applicato\nORDER BY c.embedding &lt;=&gt; query_embedding\n</code></pre>"},{"location":"getting-started/#deploy-docker","title":"Deploy Docker","text":""},{"location":"getting-started/#prerequisiti-docker","title":"Prerequisiti Docker","text":"<ul> <li>Docker Desktop 24.0+ (download)</li> <li>Docker Compose v2.0+ (incluso in Docker Desktop)</li> </ul>"},{"location":"getting-started/#quick-start-docker-2-minuti","title":"Quick Start Docker (~2 minuti)","text":"<p>Avviare l'applicazione completa:</p> <pre><code># Avvia tutti i servizi\ndocker-compose up --build -d\n\n# Visualizza i log\ndocker-compose logs -f rag-agent\n</code></pre> <p>L'applicazione sar\u00e0 disponibile su <code>http://localhost:8501</code></p> <p>Ingestione documenti con Docker:</p> <pre><code># Default: CANCELLA il DB e reingerisce tutto (raccomandato)\ndocker-compose --profile ingestion up ingestion\n\n# AGGIUNGERE documenti senza cancellare (pu\u00f2 creare duplicati)\n# Usa il doppio slash per Windows + Git Bash\ndocker-compose run --rm ingestion uv run python -m ingestion.ingest --documents //app/documents --no-clean\n\n# Con opzioni personalizzate\ndocker-compose run --rm ingestion uv run python -m ingestion.ingest \\\n  --documents /app/documents \\\n  --chunk-size 800 \\\n  --verbose\n</code></pre> <p>\u26a0\ufe0f Nota: Il comando default cancella automaticamente tutti i dati esistenti prima dell'ingestione per evitare duplicati. Usa <code>--no-clean</code> solo se vuoi aggiungere nuovi documenti senza toccare quelli esistenti.</p> <p>Comandi utili:</p> <pre><code># Ferma tutti i servizi\ndocker-compose down\n\n# Ricostruisci le immagini dopo modifiche al codice\ndocker-compose build\n\n# Visualizza i container attivi\ndocker-compose ps\n\n# Accedi al container per debugging\ndocker-compose exec rag-agent bash\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#problemi-comuni","title":"Problemi Comuni","text":"Problema Causa Soluzione <code>connection refused</code> Database non raggiungibile Verifica <code>DATABASE_URL</code> e che PostgreSQL sia attivo <code>extension \"vector\" not found</code> PGVector non installato Esegui <code>CREATE EXTENSION vector;</code> o usa immagine Docker <code>pgvector/pgvector:pg16</code> <code>OPENAI_API_KEY not set</code> Variabile mancante Aggiungi <code>OPENAI_API_KEY</code> nel file <code>.env</code> <code>uv: command not found</code> UV non installato Installa: <code>curl -LsSf https://astral.sh/uv/install.sh \\| sh</code> <code>Python version mismatch</code> Python &lt; 3.10 Aggiorna Python a 3.10+ <code>ImportError: docling</code> Dipendenze mancanti Esegui <code>uv sync</code>"},{"location":"getting-started/#connessione-database","title":"Connessione Database","text":"<pre><code># Verifica connessione\npsql $DATABASE_URL -c \"SELECT 1\"\n\n# Verifica estensione PGVector\npsql $DATABASE_URL -c \"SELECT * FROM pg_extension WHERE extname = 'vector'\"\n\n# Test performance index\npython scripts/optimize_database.py --check\n</code></pre>"},{"location":"getting-started/#reset-ambiente","title":"Reset Ambiente","text":"<pre><code># Rimuovi cache e reinstalla\nrm -rf .venv\nuv sync\n\n# Reset database (ATTENZIONE: cancella tutti i dati)\npsql $DATABASE_URL &lt; sql/optimize_index.sql\n</code></pre>"},{"location":"getting-started/#riferimento-api","title":"Riferimento API","text":""},{"location":"getting-started/#tool-search_knowledge_base_1","title":"Tool search_knowledge_base","text":"<pre><code>async def search_knowledge_base(\n    ctx: RunContext[None],\n    query: str,\n    limit: int = 5,\n    source_filter: str | None = None\n) -&gt; str:\n    \"\"\"\n    Cerca nella knowledge base usando similarit\u00e0 semantica.\n\n    Args:\n        query: La query di ricerca per trovare informazioni rilevanti\n        limit: Numero massimo di risultati da restituire (default: 5)\n        source_filter: Filtro opzionale per cercare solo in fonti specifiche\n                      Esempi: \"docling\", \"langfuse-docs\", \"langfuse-docs/deployment\"\n                      Se fornito, verranno cercati solo documenti il cui percorso\n                      sorgente contiene questa stringa\n\n    Returns:\n        Risultati di ricerca formattati con citazioni delle fonti\n    \"\"\"\n</code></pre> <p>Esempi di utilizzo del filtro:</p> <pre><code># Cerca in tutta la knowledge base\nawait search_knowledge_base(query=\"come fare il deploy\")\n\n# Cerca solo nella documentazione Docling\nawait search_knowledge_base(\n    query=\"come fare il deploy\",\n    source_filter=\"docling\"\n)\n\n# Cerca solo nella sezione deployment di Langfuse\nawait search_knowledge_base(\n    query=\"deployment cloud\",\n    source_filter=\"langfuse-docs/deployment\"\n)\n</code></pre> <p>Come funziona il filtro:</p> <ul> <li>L'agente riconosce automaticamente quando l'utente specifica una fonte</li> <li>Applica il filtro usando pattern matching case-insensitive</li> <li>Utile per separare documentazioni diverse (Docling, Langfuse, ecc.)</li> <li>Previene contaminazione cross-documentazione nelle risposte</li> </ul>"},{"location":"getting-started/#funzioni-database","title":"Funzioni Database","text":"<pre><code>-- Ricerca similarit\u00e0 vettoriale\nSELECT * FROM match_chunks(\n    query_embedding::vector(1536),\n    match_count INT,\n    similarity_threshold FLOAT DEFAULT 0.7\n)\n</code></pre> <p>Restituisce chunk con:</p> <ul> <li><code>id</code>: UUID del chunk</li> <li><code>content</code>: Contenuto testuale</li> <li><code>embedding</code>: Embedding vettoriale</li> <li><code>similarity</code>: Punteggio similarit\u00e0 coseno (0-1)</li> <li><code>document_title</code>: Titolo documento sorgente</li> <li><code>document_source</code>: Percorso documento sorgente</li> </ul>"},{"location":"getting-started/#documentazione","title":"\ud83d\udcda Documentazione","text":"<p>La documentazione completa del progetto, inclusa l'API reference, \u00e8 disponibile in <code>guide/</code>.</p>"},{"location":"getting-started/#visualizzazione-locale","title":"Visualizzazione Locale","text":"<p>Puoi avviare un server locale per navigare la documentazione:</p> <pre><code># Avvia il server di documentazione\nuv run mkdocs serve\n</code></pre> <p>La documentazione sar\u00e0 accessibile a <code>http://127.0.0.1:8000</code>.</p>"},{"location":"getting-started/#generazione-statica","title":"Generazione Statica","text":"<p>Per generare la documentazione statica (HTML):</p> <pre><code># Genera la documentazione in site/\nuv run mkdocs build\n</code></pre>"},{"location":"getting-started/#github-pages","title":"GitHub Pages","text":"<p>La documentazione viene automaticamente pubblicata su GitHub Pages ad ogni push sul branch <code>main</code>.</p>"},{"location":"getting-started/#struttura-progetto","title":"Struttura Progetto","text":"<pre><code>docling-rag-agent/\n\u251c\u2500\u2500 app.py                         # Interfaccia web Streamlit\n\u251c\u2500\u2500 docling_mcp/                   # MCP server per Cursor IDE (standalone)\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 agent.py                   # PydanticAI agent wrapper\n\u2502   \u2514\u2500\u2500 rag_service.py             # Core RAG logic (decoupled)\n\u251c\u2500\u2500 ingestion/\n\u2502   \u251c\u2500\u2500 ingest.py                  # Pipeline ingestione documenti\n\u2502   \u251c\u2500\u2500 embedder.py                # Generazione embedding con caching\n\u2502   \u2514\u2500\u2500 chunker.py                 # Chunking intelligente (Docling HybridChunker)\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 providers.py               # Configurazione modelli/client OpenAI\n\u2502   \u251c\u2500\u2500 db_utils.py                # Connection pooling ottimizzato\n\u2502   \u2514\u2500\u2500 models.py                  # Modelli Pydantic per validazione\n\u251c\u2500\u2500 sql/\n\u2502   \u2514\u2500\u2500 optimize_index.sql         # Schema completo + HNSW index ottimizzato\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 optimize_database.py       # Tool gestione index e performance\n\u2502   \u2514\u2500\u2500 test_mcp_performance.py    # Performance test suite\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 optimization-summary.md    # Analisi performance optimization\n\u2502   \u251c\u2500\u2500 performance-optimization-guide.md\n\u2502   \u2514\u2500\u2500 optimization-deployment.md\n\u251c\u2500\u2500 documents/                     # Documenti per ingestione\n\u251c\u2500\u2500 pyproject.toml                 # Dipendenze progetto (uv)\n\u251c\u2500\u2500 .env.example                   # Template variabili d'ambiente\n\u2514\u2500\u2500 README.md                      # Questo file\n</code></pre>"},{"location":"langfuse-dashboard-guide/","title":"Guida Configurazione LangFuse Dashboard","text":"<p>Questa guida descrive come configurare e utilizzare il dashboard LangFuse per monitorare le performance e i costi del Docling RAG Agent MCP Server.</p>"},{"location":"langfuse-dashboard-guide/#prerequisiti","title":"Prerequisiti","text":"<ol> <li>LangFuse Account: Cloud (cloud.langfuse.com) o self-hosted</li> <li>Variabili d'ambiente configurate:    <code>env    LANGFUSE_PUBLIC_KEY=pk-lf-...    LANGFUSE_SECRET_KEY=sk-lf-...    LANGFUSE_BASE_URL=https://cloud.langfuse.com  # opzionale</code></li> <li>MCP Server avviato con LangFuse abilitato</li> </ol>"},{"location":"langfuse-dashboard-guide/#accesso-al-dashboard","title":"Accesso al Dashboard","text":"<ol> <li>Accedi a cloud.langfuse.com o al tuo server self-hosted</li> <li>Seleziona il progetto \"docling-rag-agent\"</li> <li>Naviga alla sezione Dashboard nel menu laterale</li> </ol>"},{"location":"langfuse-dashboard-guide/#task-1-configurazione-dashboard-views-ac-1","title":"Task 1: Configurazione Dashboard Views (AC #1)","text":""},{"location":"langfuse-dashboard-guide/#metriche-chiave-disponibili","title":"Metriche Chiave Disponibili","text":"<p>Il dashboard LangFuse mostra automaticamente le seguenti metriche:</p> Metrica Descrizione Fonte Total Queries Numero totale di trace create Conteggio automatico trace Avg Latency Latenza media delle richieste Campo <code>latency</code> del trace Total Cost Costo totale embedding + LLM Calcolato da token usage"},{"location":"langfuse-dashboard-guide/#configurazione-time-period-filters","title":"Configurazione Time Period Filters","text":"<ol> <li>Nella barra superiore del dashboard, clicca sul selettore temporale</li> <li>Seleziona il periodo desiderato:</li> <li>Today: Ultime 24 ore</li> <li>Last 7 days: Ultima settimana</li> <li>Last 30 days: Ultimo mese</li> <li>Custom: Range personalizzato</li> </ol>"},{"location":"langfuse-dashboard-guide/#verifica-metriche","title":"Verifica Metriche","text":"<p>Le metriche sono calcolate automaticamente dai trace creati dal MCP server:</p> <pre><code>Tool chiamato \u2192 Trace creato \u2192 Metriche aggregate\n</code></pre> <p>Per ogni tool MCP (<code>query_knowledge_base</code>, <code>ask_knowledge_base</code>, etc.) viene creato un trace con:</p> <ul> <li><code>name</code>: Nome del tool (es. \"query_knowledge_base\")</li> <li><code>metadata</code>: { tool_name, query, limit, source_filter, source: \"mcp\" }</li> <li><code>latency</code>: Durata totale in millisecondi</li> <li><code>cost</code>: Calcolato automaticamente da token usage</li> </ul>"},{"location":"langfuse-dashboard-guide/#task-2-cost-trends-visualization-ac-2","title":"Task 2: Cost Trends Visualization (AC #2)","text":""},{"location":"langfuse-dashboard-guide/#visualizzazione-trend-costi","title":"Visualizzazione Trend Costi","text":"<ol> <li>Vai a Dashboard \u2192 Analytics</li> <li>Seleziona la visualizzazione Cost over Time</li> <li>Configura il grafico:</li> <li>X-axis: Time (daily/weekly/monthly aggregation)</li> <li>Y-axis: Total Cost (USD)</li> <li>Group by: Optional (tool_name per breakdown)</li> </ol>"},{"location":"langfuse-dashboard-guide/#filtri-data-range","title":"Filtri Data Range","text":"<p>Il filtro data range \u00e8 disponibile in tutte le visualizzazioni:</p> <ol> <li>Clicca sul selettore data in alto a destra</li> <li>Seleziona range predefinito o personalizzato</li> <li>Il grafico si aggiorna automaticamente</li> </ol>"},{"location":"langfuse-dashboard-guide/#aggregazione-costi","title":"Aggregazione Costi","text":"<p>I costi sono aggregati da:</p> <ul> <li>Embedding cost: Calcolato da <code>text-embedding-3-small</code> ($0.00002/1K tokens)</li> <li>LLM generation cost: Calcolato da <code>gpt-4o-mini</code> ($0.00015/1K input, $0.0006/1K output)</li> </ul> <p>Il costo viene tracciato automaticamente tramite il wrapper <code>langfuse.openai</code> nell'embedder.</p>"},{"location":"langfuse-dashboard-guide/#task-3-trace-detail-view-ac-3","title":"Task 3: Trace Detail View (AC #3)","text":""},{"location":"langfuse-dashboard-guide/#accesso-ai-dettagli-trace","title":"Accesso ai Dettagli Trace","text":"<ol> <li>Vai a Traces nel menu laterale</li> <li>Clicca su un trace per vedere i dettagli</li> </ol>"},{"location":"langfuse-dashboard-guide/#informazioni-visualizzate","title":"Informazioni Visualizzate","text":"<p>Ogni trace mostra:</p>"},{"location":"langfuse-dashboard-guide/#input-query-text","title":"Input Query Text","text":"<pre><code>metadata.query: \"Come configurare LangFuse?\"\n</code></pre>"},{"location":"langfuse-dashboard-guide/#output-response","title":"Output Response","text":"<pre><code>output: \"[Source: langfuse-docs/setup]\\nPer configurare LangFuse...\"\n</code></pre>"},{"location":"langfuse-dashboard-guide/#cost-breakdown","title":"Cost Breakdown","text":"<pre><code>cost:\n  embedding_cost: $0.000012  (embedding-generation span)\n  llm_generation_cost: $0.000045  (se presente span llm-generation)\n  total: $0.000057\n</code></pre>"},{"location":"langfuse-dashboard-guide/#timing-breakdown","title":"Timing Breakdown","text":"<pre><code>latency: 1234ms (totale)\nspans:\n  - embedding-generation: 234ms (metadata.duration_ms)\n  - vector-search: 45ms (metadata.duration_ms)\n</code></pre>"},{"location":"langfuse-dashboard-guide/#nested-spans","title":"Nested Spans","text":"<p>I trace contengono span gerarchici:</p> <pre><code>query_knowledge_base (root trace)\n\u251c\u2500\u2500 embedding-generation (span)\n\u2502   \u251c\u2500\u2500 duration_ms: 234\n\u2502   \u251c\u2500\u2500 model: text-embedding-3-small\n\u2502   \u2514\u2500\u2500 embedding_dim: 1536\n\u2514\u2500\u2500 vector-search (span)\n    \u251c\u2500\u2500 duration_ms: 45\n    \u251c\u2500\u2500 limit: 5\n    \u2514\u2500\u2500 results_count: 3\n</code></pre>"},{"location":"langfuse-dashboard-guide/#navigazione-spans","title":"Navigazione Spans","text":"<ol> <li>Nel detail view del trace, scorri verso il basso</li> <li>La sezione Observations mostra tutti gli span</li> <li>Clicca su uno span per vedere i dettagli specifici</li> </ol>"},{"location":"langfuse-dashboard-guide/#task-4-custom-charts-configuration-ac-4","title":"Task 4: Custom Charts Configuration (AC #4)","text":""},{"location":"langfuse-dashboard-guide/#creazione-custom-chart","title":"Creazione Custom Chart","text":"<ol> <li>Vai a Dashboard \u2192 clicca + Add Chart</li> <li>Configura i parametri:</li> </ol>"},{"location":"langfuse-dashboard-guide/#chart-per-cost-trends-raccomandato","title":"Chart per Cost Trends (Raccomandato)","text":"Parametro Valore Chart Type Line Chart Metric Total Cost Dimension Time (day/week/month) Filter Optional: tool_name = \"query_knowledge_base\""},{"location":"langfuse-dashboard-guide/#chart-per-latency-distribution","title":"Chart per Latency Distribution","text":"Parametro Valore Chart Type Bar Chart Metric Average Latency Dimension tool_name Filter Nessuno"},{"location":"langfuse-dashboard-guide/#salvataggio-custom-charts","title":"Salvataggio Custom Charts","text":"<ol> <li>Dopo aver configurato il chart, clicca Save</li> <li>Il chart viene aggiunto al dashboard</li> <li>I chart sono persistenti e accessibili in tutte le sessioni</li> </ol>"},{"location":"langfuse-dashboard-guide/#dimensioni-disponibili","title":"Dimensioni Disponibili","text":"Dimensione Uso <code>time</code> Aggregazione temporale <code>name</code> Nome del trace (tool_name) <code>user_id</code> ID utente (se configurato) <code>session_id</code> ID sessione (se configurato) <code>metadata.source</code> Filtro per \"mcp\" <code>metadata.tool_name</code> Filtro per tool specifico"},{"location":"langfuse-dashboard-guide/#metriche-disponibili","title":"Metriche Disponibili","text":"Metrica Descrizione <code>count</code> Numero di trace <code>totalCost</code> Costo totale <code>avgLatency</code> Latenza media <code>p50Latency</code> Latenza percentile 50 <code>p95Latency</code> Latenza percentile 95 <code>p99Latency</code> Latenza percentile 99"},{"location":"langfuse-dashboard-guide/#integrazione-con-prometheus","title":"Integrazione con Prometheus","text":"<p>Per un monitoring completo, combina LangFuse con Prometheus:</p>"},{"location":"langfuse-dashboard-guide/#langfuse-dashboard","title":"LangFuse Dashboard","text":"<ul> <li>Business metrics: costi, query trends, qualit\u00e0</li> <li>Trace details: debugging, analisi</li> </ul>"},{"location":"langfuse-dashboard-guide/#prometheus-metrics","title":"Prometheus Metrics","text":"<ul> <li>Infrastructure metrics: latenza, throughput, errori</li> <li>Alerting: SLO violations</li> </ul>"},{"location":"langfuse-dashboard-guide/#endpoint-prometheus","title":"Endpoint Prometheus","text":"<pre><code># Avvia HTTP server per metriche\nuv run python -m docling_mcp.http_server\n\n# Scrape metriche\ncurl http://localhost:8080/metrics\n</code></pre>"},{"location":"langfuse-dashboard-guide/#metriche-prometheus-correlate","title":"Metriche Prometheus Correlate","text":"LangFuse Prometheus Total Cost - (solo LangFuse) Avg Latency <code>mcp_request_duration_seconds</code> Embedding Time <code>rag_embedding_time_seconds</code> DB Search Time <code>rag_db_search_time_seconds</code>"},{"location":"langfuse-dashboard-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"langfuse-dashboard-guide/#trace-non-visibili","title":"Trace Non Visibili","text":"<ol> <li>Verifica variabili d'ambiente <code>LANGFUSE_PUBLIC_KEY</code> e <code>LANGFUSE_SECRET_KEY</code></li> <li>Controlla che il server MCP sia avviato</li> <li>Esegui una query di test e attendi 1-2 minuti per la propagazione</li> </ol>"},{"location":"langfuse-dashboard-guide/#costi-non-calcolati","title":"Costi Non Calcolati","text":"<ol> <li>Verifica che l'embedder usi <code>langfuse.openai</code> wrapper</li> <li>Controlla che il modello sia supportato per il pricing (text-embedding-3-small, gpt-4o-mini)</li> <li>Verifica che i trace abbiano token usage popolato</li> </ol>"},{"location":"langfuse-dashboard-guide/#span-timing-non-visibile","title":"Span Timing Non Visibile","text":"<ol> <li>Verifica che gli span abbiano <code>metadata.duration_ms</code> popolato</li> <li>Controlla i log del server MCP per errori span creation</li> <li>Usa LangFuse debug mode per dettagli aggiuntivi</li> </ol>"},{"location":"langfuse-dashboard-guide/#riferimenti","title":"Riferimenti","text":"<ul> <li>LangFuse Documentation</li> <li>LangFuse Custom Dashboards</li> <li>Architecture ADR-001</li> <li>MCP Server Implementation</li> </ul>"},{"location":"prd/","title":"docling-rag-agent - Product Requirements Document","text":"<p>Author: Stefano Date: 2025-11-24 Updated: 2025-11-26 Version: 2.1 Project Type: Backend RAG Application (Brownfield Enhancement)</p>"},{"location":"prd/#executive-summary","title":"Executive Summary","text":"<p>Il progetto <code>docling-rag-agent</code> evolve da prototipo funzionale a sistema production-ready enterprise-grade con monitoring completo, cost tracking, e deployment automatizzato su GitHub. L'obiettivo primario \u00e8 implementare observability nativa per MCP Server tramite LangFuse, fornendo visibilit\u00e0 totale su performance, costi, e utilizzo del sistema RAG in produzione.</p>"},{"location":"prd/#what-makes-this-special","title":"What Makes This Special","text":"<p>Un RAG agent production-ready con:</p> <ul> <li>LangFuse integration nativa per MCP Server observability</li> <li>Cost tracking granulare per ogni query (embedding + LLM tokens)</li> <li>Real-time monitoring dashboard con breakdown latency per componente</li> <li>GitHub-ready deployment con CI/CD, health checks, e documentazione enterprise</li> </ul>"},{"location":"prd/#project-classification","title":"Project Classification","text":"<p>Technical Type: Backend RAG Application (Brownfield Enhancement) Domain: Knowledge Management + Developer Tools Complexity: Medium-High Track: BMad Method (Brownfield)</p>"},{"location":"prd/#success-criteria","title":"Success Criteria","text":""},{"location":"prd/#1-mcp-monitoring-coverage","title":"1. MCP Monitoring Coverage","text":"<ul> <li>\u2705 100% delle chiamate <code>query_knowledge_base</code> tracciate con timing completo</li> <li>\u2705 Dashboard LangFuse real-time con metriche chiave</li> <li>\u2705 Alert automatici per errori o latency anomale (&gt; 3s)</li> </ul>"},{"location":"prd/#2-cost-tracking-accuracy","title":"2. Cost Tracking Accuracy","text":"<ul> <li>\u2705 Costo per sessione MCP calcolato con precisione (embedding + LLM tokens)</li> <li>\u2705 Breakdown costi: embedding_cost + db_cost + generation_cost</li> <li>\u2705 Export mensile costi in CSV per analisi budget</li> </ul>"},{"location":"prd/#3-production-readiness","title":"3. Production Readiness","text":"<ul> <li>\u2705 Zero warning nei linter (ruff, mypy)</li> <li>\u2705 100% documentazione API aggiornata</li> <li>\u2705 GitHub Actions CI/CD funzionante (test + lint + build)</li> <li>\u2705 Docker images ottimizzate (&lt; 500MB)</li> </ul>"},{"location":"prd/#4-developer-experience","title":"4. Developer Experience","text":"<ul> <li>\u2705 Setup locale completato in &lt; 5 minuti</li> <li>\u2705 Monitoring dashboard accessibile via browser</li> <li>\u2705 Logs strutturati e ricercabili (JSON format)</li> </ul>"},{"location":"prd/#5-performance-targets","title":"5. Performance Targets","text":"<ul> <li>\u2705 Latency media query MCP &lt; 2 secondi (95th percentile)</li> <li>\u2705 Embedding generation &lt; 500ms per batch</li> <li>\u2705 DB vector search &lt; 100ms per query</li> </ul>"},{"location":"prd/#product-scope","title":"Product Scope","text":""},{"location":"prd/#mvp-minimum-viable-product","title":"MVP - Minimum Viable Product","text":"<p>Epic 1: Core RAG Baseline Documentation</p> <ul> <li>Documentazione completa architettura esistente</li> <li>API reference auto-generato</li> <li>Deployment guide aggiornato</li> </ul> <p>Epic 2: MCP Server Observability (LangFuse)</p> <ul> <li>LangFuse integration per MCP server</li> <li>Cost tracking per <code>query_knowledge_base</code></li> <li>Performance metrics (latency, tokens, DB time)</li> <li>Real-time dashboard LangFuse</li> </ul> <p>Epic 3: Streamlit UI Observability</p> <ul> <li>Session tracking con session_id univoco</li> <li>Cost tracking per sessione utente</li> <li>Sidebar con statistiche sessione corrente</li> </ul> <p>Epic 4: Production Infrastructure</p> <ul> <li>GitHub Actions CI/CD pipeline</li> <li>Docker optimization (multi-stage builds)</li> <li>Health checks per tutti i servizi</li> <li>Linting &amp; type checking automatizzato</li> </ul> <p>Epic 5: Testing &amp; Quality Assurance (TDD)</p> <ul> <li>Test infrastructure con pytest e fixtures</li> <li>Unit tests con PydanticAI TestModel</li> <li>RAGAS evaluation suite</li> <li>Playwright E2E tests</li> <li>TDD structure rigorosa (Red-Green-Refactor)</li> </ul> <p>Epic 6: Project Structure Refactoring &amp; Organization</p> <ul> <li>Eliminazione file sparsi in root</li> <li>Centralizzazione documentazione in <code>docs/</code></li> <li>Riorganizzazione scripts in sottodirectory</li> <li>Struttura modulo <code>mcp/</code> con tools separati per dominio</li> <li>Validazione struttura rigorosa</li> </ul>"},{"location":"prd/#growth-features-post-mvp","title":"Growth Features (Post-MVP)","text":"<ul> <li>Advanced Analytics: Trend analysis, cost forecasting</li> <li>Multi-user Monitoring: Per-user cost tracking</li> <li>Alert System: Slack/Email notifications per anomalie</li> <li>Document Management UI: Upload/delete via Streamlit</li> <li>Custom Dashboards: Grafana integration</li> </ul>"},{"location":"prd/#vision-future","title":"Vision (Future)","text":"<ul> <li>Auto-scaling: Basato su load metrics</li> <li>A/B Testing: Confronto modelli LLM (GPT-4 vs GPT-4o-mini)</li> <li>Multi-tenancy: Isolamento dati per team</li> <li>GraphRAG: Knowledge graph integration</li> </ul>"},{"location":"prd/#functional-requirements","title":"Functional Requirements","text":""},{"location":"prd/#core-rag-capabilities-baseline","title":"Core RAG Capabilities (Baseline)","text":"<p>FR1: Il sistema DEVE supportare ingestione di documenti PDF, DOCX, PPTX, XLSX, HTML, MD, TXT via Docling FR2: Il sistema DEVE generare embeddings vettoriali (1536 dimensioni) per ogni chunk di documento FR3: Il sistema DEVE memorizzare documenti e chunks in PostgreSQL con estensione PGVector FR4: Il sistema DEVE eseguire ricerca semantica usando similarit\u00e0 coseno FR5: Il sistema DEVE fornire risposte RAG con citazioni delle fonti originali FR6: Il sistema DEVE supportare filtraggio per fonte documentale (source_filter)</p>"},{"location":"prd/#mcp-server-observability","title":"MCP Server Observability","text":"<p>FR7: Il sistema DEVE tracciare ogni chiamata <code>query_knowledge_base</code> con timestamp, latency, e risultato FR8: Il sistema DEVE calcolare il costo per ogni query MCP (embedding tokens + LLM tokens) FR9: Il sistema DEVE inviare trace completi a LangFuse per ogni operazione MCP FR10: Il sistema DEVE registrare breakdown timing: embedding_time, db_search_time, llm_generation_time FR11: Il sistema DEVE esporre metriche real-time via endpoint <code>/metrics</code> (Prometheus format) FR12: Il sistema DEVE loggare errori MCP con stack trace completo e context</p>"},{"location":"prd/#mcp-server-architecture-fix","title":"MCP Server Architecture &amp; Fix","text":"<p>FR12.1: Il MCP server DEVE usare direttamente <code>core/rag_service.py</code> senza dipendenza da API server esterno FR12.2: Il MCP server DEVE essere standalone e funzionare senza <code>api/main.py</code> in esecuzione FR12.3: Il MCP server DEVE essere organizzato in modulo <code>mcp/</code> con tools separati per dominio (search, documents, overview) FR12.4: Il MCP server DEVE implementare pattern FastMCP nativi (lifespan management, context injection) FR12.5: Il MCP server DEVE garantire che tutti i tool (<code>query_knowledge_base</code>, <code>list_knowledge_base_documents</code>, <code>get_knowledge_base_document</code>, <code>get_knowledge_base_overview</code>) e prompt (<code>ask_knowledge_base</code>) funzionino correttamente senza errori FR12.6: Il MCP server DEVE gestire errori con messaggi informativi e graceful degradation</p>"},{"location":"prd/#streamlit-ui-observability","title":"Streamlit UI Observability","text":"<p>FR13: Il sistema DEVE tracciare sessioni utente Streamlit con session_id univoco FR14: Il sistema DEVE registrare ogni query utente con timestamp, input, e risposta FR15: Il sistema DEVE calcolare costi cumulativi per sessione Streamlit FR16: Il sistema DEVE mostrare statistiche sessione corrente nella sidebar (query count, total cost, avg latency)</p>"},{"location":"prd/#cost-tracking-analytics","title":"Cost Tracking &amp; Analytics","text":"<p>FR17: Il sistema DEVE calcolare costo per token (input/output) basato su pricing OpenAI corrente FR18: Il sistema DEVE aggregare costi giornalieri, settimanali, e mensili FR19: Il sistema DEVE esportare report costi in formato CSV e JSON FR20: Il sistema DEVE mostrare dashboard LangFuse con metriche chiave (cost, latency, throughput)</p>"},{"location":"prd/#production-infrastructure","title":"Production Infrastructure","text":"<p>FR21: Il sistema DEVE passare linting (ruff) senza warning FR22: Il sistema DEVE passare type checking (mypy) senza errori FR23: Il sistema DEVE avere health check endpoint (<code>/health</code>) per ogni servizio (MCP, Streamlit, DB) FR24: Il sistema DEVE supportare deployment Docker con docker-compose FR25: Il sistema DEVE avere GitHub Actions per CI/CD (test + lint + build + deploy)</p>"},{"location":"prd/#documentation-developer-experience","title":"Documentation &amp; Developer Experience","text":"<p>FR26: Il sistema DEVE avere README con setup instructions completabili in &lt; 5 minuti FR27: Il sistema DEVE avere API documentation auto-generata (Sphinx o MkDocs) FR28: Il sistema DEVE avere guida monitoring setup (LangFuse configuration step-by-step) FR29: Il sistema DEVE avere esempi di query con costi stimati e performance attese FR30: Il sistema DEVE avere badge GitHub (build status, coverage, version, license) FR30.1: Documentazione DEVE essere centralizzata in <code>docs/</code> senza file markdown sparsi FR30.2: Documentazione DEVE includere troubleshooting guide completa per MCP server FR30.3: Documentazione DEVE includere guida struttura progetto e organizzazione codice</p>"},{"location":"prd/#testing-quality-assurance-tdd","title":"Testing &amp; Quality Assurance (TDD)","text":"<p>FR31: Il sistema DEVE avere unit tests per tutti i moduli core (core/, ingestion/, utils/) con coverage &gt; 70% FR32: Il sistema DEVE usare PydanticAI TestModel per mock LLM responses durante unit testing FR33: Il sistema DEVE avere RAGAS evaluation suite per validare qualit\u00e0 RAG (faithfulness, relevancy) FR34: Il sistema DEVE avere Playwright E2E tests per workflow Streamlit critici FR35: Il sistema DEVE avere integration tests per MCP server endpoints FR36: Il sistema DEVE eseguire tests automaticamente in CI/CD pipeline FR37: Il sistema DEVE generare coverage report e pubblicarlo su GitHub FR38: Il sistema DEVE avere test fixtures per database (setup/teardown automatico) FR39: Il sistema DEVE avere golden dataset per RAGAS evaluation (min 20 query-answer pairs) FR40: Il sistema DEVE loggare test results in LangFuse per tracking qualit\u00e0 nel tempo</p>"},{"location":"prd/#tdd-structure-organization","title":"TDD Structure &amp; Organization","text":"<p>FR41: Test suite DEVE essere organizzata rigorosamente in <code>tests/unit/</code>, <code>tests/integration/</code>, <code>tests/e2e/</code> FR42: Test fixtures DEVE essere in <code>tests/fixtures/</code> con golden dataset per RAGAS FR43: Test DEVE seguire pattern Red-Green-Refactor rigoroso (test prima del codice) FR44: Coverage report DEVE essere generato automaticamente in CI/CD con threshold &gt; 70%</p>"},{"location":"prd/#project-structure-organization","title":"Project Structure &amp; Organization","text":"<p>FR45: Il progetto DEVE seguire struttura directory rigorosa senza file sparsi in root FR46: Tutti i file markdown DEVE essere in <code>docs/</code> (eccetto README.md root) FR47: Tutti gli script DEVE essere organizzati in <code>scripts/</code> con sottodirectory per categoria FR48: Il codice DEVE essere organizzato per responsabilit\u00e0 (mcp/, core/, ingestion/, utils/, api/) FR49: Zero file temporanei o di debug in root directory</p>"},{"location":"prd/#non-functional-requirements","title":"Non-Functional Requirements","text":""},{"location":"prd/#performance","title":"Performance","text":"<p>NFR-P1: Latency media query MCP &lt; 2 secondi (95th percentile) NFR-P2: Embedding generation &lt; 500ms per batch (100 chunks) NFR-P3: DB vector search &lt; 100ms per query (con HNSW index) NFR-P4: Throughput MCP server: 50 query/secondo con degradazione &lt; 10%</p>"},{"location":"prd/#scalability","title":"Scalability","text":"<p>NFR-S1: Supporto 50 query concorrenti MCP senza degradazione performance NFR-S2: Connection pool PostgreSQL: 10-50 connessioni dinamiche NFR-S3: Horizontal scaling supportato via load balancer (future)</p>"},{"location":"prd/#reliability","title":"Reliability","text":"<p>NFR-R1: Uptime 99.5% per MCP server (downtime pianificato escluso) NFR-R2: Retry automatico per fallimenti OpenAI API (max 3 tentativi con exponential backoff) NFR-R3: Graceful degradation: sistema continua a funzionare se LangFuse non disponibile</p>"},{"location":"prd/#maintainability","title":"Maintainability","text":"<p>NFR-M1: Code coverage &gt; 70% per core modules (core/, ingestion/, utils/) NFR-M2: Documentazione inline (docstrings) per tutte le funzioni pubbliche NFR-M3: Logging strutturato (JSON) per facile parsing e analisi</p>"},{"location":"prd/#security","title":"Security","text":"<p>NFR-SEC1: API keys memorizzate in variabili ambiente (no hardcoding) NFR-SEC2: LangFuse API key protetta e non loggata NFR-SEC3: PostgreSQL connection string criptata in produzione</p>"},{"location":"prd/#testing-quality","title":"Testing &amp; Quality","text":"<p>NFR-T1: Test suite execution time &lt; 5 minuti (unit tests) NFR-T2: E2E tests execution time &lt; 10 minuti NFR-T3: RAGAS faithfulness score &gt; 0.85 per golden dataset NFR-T4: RAGAS answer relevancy score &gt; 0.80 per golden dataset NFR-T5: Test coverage mantenuto &gt; 70% in CI/CD (fail build se &lt; 70%)</p>"},{"location":"prd/#domain-specific-requirements","title":"Domain-Specific Requirements","text":""},{"location":"prd/#langfuse-integration","title":"LangFuse Integration","text":"<p>Domain Context: LangFuse \u00e8 la piattaforma standard per LLM observability. Richiede:</p> <ul> <li>Trace gerarchici (query \u2192 embedding \u2192 retrieval \u2192 generation)</li> <li>Metadata ricchi (model, tokens, cost)</li> <li>Session grouping per analisi utente</li> </ul> <p>Requirements:</p> <ul> <li>Trace completi per ogni operazione RAG</li> <li>Cost calculation accurato basato su pricing OpenAI</li> <li>Session tracking per analisi comportamentale</li> <li>LangFuse client inizializzato via environment variables (<code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_BASE_URL</code>)</li> <li>Decorator <code>@observe()</code> per auto-tracing funzioni critiche</li> <li>Cost tracking automatico per OpenAI calls (drop-in replacement <code>langfuse.openai</code>)</li> <li>Graceful degradation se LangFuse non disponibile (sistema continua a funzionare)</li> </ul>"},{"location":"prd/#mcp-protocol-compliance","title":"MCP Protocol Compliance","text":"<p>Domain Context: Model Context Protocol richiede:</p> <ul> <li>Tool registration standardizzato</li> <li>Error handling robusto</li> <li>Logging non invasivo (no stdout pollution)</li> </ul> <p>Requirements:</p> <ul> <li>MCP server conforme a spec FastMCP</li> <li>Logging via stderr o file dedicato</li> <li>Health check non bloccante</li> </ul>"},{"location":"prd/#references","title":"References","text":"<ul> <li>Project Documentation Index</li> <li>Architecture Documentation</li> <li>API &amp; Data Analysis</li> <li>Technical Requirements Analysis</li> <li>LangFuse Documentation</li> <li>LangFuse Python SDK Setup</li> <li>FastMCP Specification</li> <li>FastMCP Best Practices</li> <li>PydanticAI Testing Guide</li> <li>RAGAS Documentation</li> </ul> <p>This PRD defines the roadmap for transforming docling-rag-agent into a production-ready, enterprise-grade RAG system with comprehensive observability and cost tracking.</p> <p>Created through collaborative discovery between Stefano and BMAD PM Agent.</p>"},{"location":"testing-strategy/","title":"Testing Strategy - docling-rag-agent","text":"<p>Version: 1.0 Last Updated: 2025-01-27 Python Version: &gt;=3.10</p>"},{"location":"testing-strategy/#overview","title":"Overview","text":"<p>Questo documento definisce la strategia di testing per il progetto <code>docling-rag-agent</code>. La strategia segue principi Test-Driven Development (TDD) con organizzazione rigorosa dei test, coverage minimo del 70%, e integrazione completa con CI/CD.</p> <p>Principi Fondamentali: - TDD First: Scrivere test prima del codice (Red-Green-Refactor) - Coverage Enforcement: Coverage minimo 70% per moduli core, 80% per moduli critici - Test Organization: Organizzazione rigorosa in unit/integration/e2e - Quality Metrics: RAGAS evaluation per validare qualit\u00e0 RAG - Observability: Test integrati con LangFuse per tracking metriche</p>"},{"location":"testing-strategy/#1-test-driven-development-tdd","title":"1. Test-Driven Development (TDD)","text":""},{"location":"testing-strategy/#11-tdd-workflow","title":"1.1 TDD Workflow","text":"<p>Pattern Red-Green-Refactor:</p> <ol> <li>Red: Scrivere test che fallisce (definisce comportamento desiderato)</li> <li>Green: Scrivere codice minimo per far passare il test</li> <li>Refactor: Migliorare codice mantenendo test verde</li> </ol> <p>Esempio:</p> <pre><code># Step 1: Red - Test che fallisce\ndef test_search_knowledge_base_returns_formatted_results():\n    \"\"\"Test that search returns formatted results.\"\"\"\n    result = search_knowledge_base(\"test query\", limit=5)\n    assert \"test query\" in result\n    assert len(result.split(\"\\n\")) &gt; 0\n\n# Step 2: Green - Implementazione minima\ndef search_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"Search knowledge base.\"\"\"\n    return f\"Results for: {query}\"\n\n# Step 3: Refactor - Migliorare implementazione\ndef search_knowledge_base(query: str, limit: int = 5) -&gt; str:\n    \"\"\"Search knowledge base with proper formatting.\"\"\"\n    results = await search_knowledge_base_structured(query, limit)\n    return format_results(results)\n</code></pre>"},{"location":"testing-strategy/#12-tdd-benefits","title":"1.2 TDD Benefits","text":"<p>Per questo progetto: - Regression Prevention: Test catturano regressioni prima del deploy - Design Guidance: Test guidano design API pulite e testabili - Documentation: Test servono come documentazione eseguibile - Confidence: Refactoring sicuro con suite test completa</p>"},{"location":"testing-strategy/#2-test-organization","title":"2. Test Organization","text":""},{"location":"testing-strategy/#21-directory-structure","title":"2.1 Directory Structure","text":"<p>Struttura rigorosa:</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures and configuration\n\u251c\u2500\u2500 README.md                # Test documentation\n\u251c\u2500\u2500 unit/                    # Unit tests (&gt;70% coverage)\n\u2502   \u251c\u2500\u2500 test_rag_service.py\n\u2502   \u251c\u2500\u2500 test_embedder.py\n\u2502   \u251c\u2500\u2500 test_langfuse_integration.py\n\u2502   \u251c\u2500\u2500 test_performance_metrics.py\n\u2502   \u2514\u2500\u2500 test_mcp_server_validation.py\n\u251c\u2500\u2500 integration/             # Integration tests\n\u2502   \u251c\u2500\u2500 test_mcp_server_integration.py\n\u2502   \u2514\u2500\u2500 test_observability_endpoints.py\n\u251c\u2500\u2500 e2e/                    # End-to-end tests (Playwright)\n\u2502   \u2514\u2500\u2500 test_streamlit_workflow.py\n\u2514\u2500\u2500 fixtures/               # Test fixtures + golden dataset\n    \u2514\u2500\u2500 golden_dataset.json  # 20+ query-answer pairs (RAGAS)\n</code></pre>"},{"location":"testing-strategy/#22-test-file-naming","title":"2.2 Test File Naming","text":"<p>Convenzioni: - File: <code>test_*.py</code> o <code>*_test.py</code> - Classe test: <code>Test&lt;ComponentName&gt;</code> (es. <code>TestPrometheusMetrics</code>) - Funzione test: <code>test_&lt;functionality&gt;_&lt;condition&gt;_&lt;expected_result&gt;</code></p> <p>Esempio:</p> <pre><code># tests/unit/test_rag_service.py\nclass TestRAGService:\n    \"\"\"Test RAG service functionality.\"\"\"\n\n    def test_search_knowledge_base_with_valid_query_returns_results(self):\n        \"\"\"Test search returns results for valid query.\"\"\"\n        pass\n\n    def test_search_knowledge_base_with_empty_query_raises_error(self):\n        \"\"\"Test search raises error for empty query.\"\"\"\n        pass\n</code></pre>"},{"location":"testing-strategy/#23-test-categories","title":"2.3 Test Categories","text":"<p>Unit Tests: - Testano singole funzioni/moduli in isolamento - Mock di dipendenze esterne (DB, API, LangFuse) - Coverage target: &gt;70% per moduli core</p> <p>Integration Tests: - Testano interazione tra componenti - Usano test database o mock services - Coverage target: &gt;25% delle interazioni critiche</p> <p>E2E Tests: - Testano workflow completi end-to-end - Usano servizi reali (test environment) - Coverage target: &gt;15% dei workflow critici</p>"},{"location":"testing-strategy/#3-unit-testing","title":"3. Unit Testing","text":""},{"location":"testing-strategy/#31-unit-test-structure","title":"3.1 Unit Test Structure","text":"<p>Pattern AAA (Arrange-Act-Assert):</p> <pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\n@pytest.mark.asyncio\nasync def test_search_knowledge_base_returns_formatted_results():\n    # Arrange: Setup test data and mocks\n    query = \"test query\"\n    limit = 5\n    mock_results = [\n        {\"content\": \"result 1\", \"source\": \"doc1\"},\n        {\"content\": \"result 2\", \"source\": \"doc2\"}\n    ]\n\n    with patch('core.rag_service.search_knowledge_base_structured') as mock_search:\n        mock_search.return_value = mock_results\n\n        # Act: Execute function under test\n        result = await query_knowledge_base(query, limit)\n\n        # Assert: Verify results\n        assert \"result 1\" in result\n        assert \"doc1\" in result\n        mock_search.assert_called_once_with(query, limit)\n</code></pre>"},{"location":"testing-strategy/#32-mocking-patterns","title":"3.2 Mocking Patterns","text":"<p>LangFuse Mocking:</p> <pre><code>from unittest.mock import patch, MagicMock\n\ndef test_langfuse_span_creates_span_when_available():\n    \"\"\"Test LangFuse span creation when client available.\"\"\"\n    with patch('docling_mcp.server.get_langfuse_client') as mock_get_client:\n        mock_client = MagicMock()\n        mock_span = MagicMock()\n        mock_client.span.return_value = mock_span\n        mock_get_client.return_value = mock_client\n\n        # Test span creation\n        async with langfuse_span(\"test-span\") as span_ctx:\n            assert span_ctx[\"span\"] == mock_span\n</code></pre> <p>Database Mocking:</p> <pre><code>from unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_search_db_with_mock_connection():\n    \"\"\"Test database search with mocked connection.\"\"\"\n    mock_conn = AsyncMock()\n    mock_conn.fetch.return_value = [\n        {\"id\": 1, \"content\": \"test\", \"similarity\": 0.95}\n    ]\n\n    with patch('utils.db_utils.db_pool.acquire') as mock_acquire:\n        mock_acquire.return_value.__aenter__.return_value = mock_conn\n\n        results = await search_vector_db(embedding, limit=5)\n        assert len(results) == 1\n        assert results[0][\"content\"] == \"test\"\n</code></pre> <p>OpenAI/LLM Mocking:</p> <pre><code>from unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_embedding_generation_with_mock_openai():\n    \"\"\"Test embedding generation with mocked OpenAI client.\"\"\"\n    mock_response = MagicMock()\n    mock_response.data = [MagicMock(embedding=[0.1, 0.2, 0.3])]\n\n    with patch('ingestion.embedder.LangfuseAsyncOpenAI') as mock_client_class:\n        mock_client = AsyncMock()\n        mock_client.embeddings.create = AsyncMock(return_value=mock_response)\n        mock_client_class.return_value = mock_client\n\n        embedder = EmbeddingGenerator()\n        embedding = await embedder.embed_query(\"test query\")\n        assert len(embedding) == 3\n</code></pre>"},{"location":"testing-strategy/#33-async-testing","title":"3.3 Async Testing","text":"<p>pytest-asyncio: - Usare <code>@pytest.mark.asyncio</code> per test async - Configurare <code>asyncio_mode = \"auto\"</code> in <code>pyproject.toml</code></p> <p>Esempio:</p> <pre><code>import pytest\n\n@pytest.mark.asyncio\nasync def test_async_function():\n    \"\"\"Test async function.\"\"\"\n    result = await async_function()\n    assert result is not None\n\n# Event loop fixture in conftest.py\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for test session.\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n</code></pre>"},{"location":"testing-strategy/#34-coverage-requirements","title":"3.4 Coverage Requirements","text":"<p>Per Modulo: - Core modules (<code>core/</code>, <code>ingestion/</code>): &gt;70% coverage - Critical modules (<code>docling_mcp/</code>): &gt;80% coverage - Utility modules (<code>utils/</code>): &gt;60% coverage</p> <p>Verifica Coverage:</p> <pre><code># Run tests with coverage\npytest --cov=core --cov=docling_mcp --cov-report=term-missing\n\n# Generate HTML report\npytest --cov=core --cov=docling_mcp --cov-report=html\n\n# Check specific threshold\npytest --cov=core --cov-fail-under=70\n</code></pre>"},{"location":"testing-strategy/#4-integration-testing","title":"4. Integration Testing","text":""},{"location":"testing-strategy/#41-integration-test-patterns","title":"4.1 Integration Test Patterns","text":"<p>Test Database Integration:</p> <pre><code>import pytest\nfrom utils.db_utils import DatabasePool\n\n@pytest.fixture\nasync def test_db_pool():\n    \"\"\"Create test database pool.\"\"\"\n    pool = DatabasePool(database_url=\"postgresql://test:test@localhost/test_db\")\n    await pool.initialize()\n    yield pool\n    await pool.close()\n\n@pytest.mark.asyncio\nasync def test_search_knowledge_base_with_real_db(test_db_pool):\n    \"\"\"Test search with real database connection.\"\"\"\n    # Setup test data\n    await insert_test_documents(test_db_pool)\n\n    # Execute search\n    results = await search_knowledge_base_structured(\"test query\", limit=5)\n\n    # Verify results\n    assert len(results) &gt; 0\n    assert all(\"content\" in r for r in results)\n</code></pre> <p>Test HTTP Endpoints:</p> <pre><code>from fastapi.testclient import TestClient\n\n@pytest.fixture\ndef test_client():\n    \"\"\"Create test client for HTTP server.\"\"\"\n    from docling_mcp.http_server import app\n    return TestClient(app)\n\ndef test_metrics_endpoint_returns_prometheus_format(test_client):\n    \"\"\"Test /metrics endpoint returns Prometheus format.\"\"\"\n    response = test_client.get(\"/metrics\")\n\n    assert response.status_code == 200\n    assert \"openmetrics-text\" in response.headers.get(\"content-type\", \"\")\n    assert \"# HELP\" in response.text or \"# TYPE\" in response.text\n</code></pre> <p>Test MCP Server Integration:</p> <pre><code>@pytest.mark.asyncio\nasync def test_mcp_tool_integration():\n    \"\"\"Test MCP tool with real service integration.\"\"\"\n    # Initialize test resources\n    await initialize_test_resources()\n\n    try:\n        # Execute MCP tool\n        result = await query_knowledge_base(\"test query\", limit=5)\n\n        # Verify result format\n        assert isinstance(result, str)\n        assert len(result) &gt; 0\n\n        # Verify observability (LangFuse traces, Prometheus metrics)\n        assert_trace_created(\"query_knowledge_base\")\n        assert_metrics_recorded(\"mcp_requests_total\")\n    finally:\n        await cleanup_test_resources()\n</code></pre>"},{"location":"testing-strategy/#42-test-fixtures","title":"4.2 Test Fixtures","text":"<p>Shared Fixtures (<code>conftest.py</code>):</p> <pre><code>import pytest\nfrom unittest.mock import AsyncMock, MagicMock\n\n@pytest.fixture\ndef mock_httpx_response():\n    \"\"\"Create mock httpx response.\"\"\"\n    def _create_response(status_code=200, json_data=None, text=\"\"):\n        response = MagicMock()\n        response.status_code = status_code\n        response.text = text\n        response.json = MagicMock(return_value=json_data or {})\n        return response\n    return _create_response\n\n@pytest.fixture\ndef mock_search_response():\n    \"\"\"Mock successful search response.\"\"\"\n    return {\n        \"results\": [\n            {\n                \"title\": \"Test Document\",\n                \"content\": \"This is test content\",\n                \"source\": \"test-source\",\n                \"similarity\": 0.95\n            }\n        ],\n        \"count\": 1\n    }\n</code></pre>"},{"location":"testing-strategy/#5-end-to-end-testing","title":"5. End-to-End Testing","text":""},{"location":"testing-strategy/#51-playwright-e2e-tests","title":"5.1 Playwright E2E Tests","text":"<p>Setup:</p> <pre><code># Install Playwright\npip install playwright pytest-playwright\n\n# Install browsers\nplaywright install\n</code></pre> <p>Test Structure:</p> <pre><code>import pytest\nfrom playwright.sync_api import Page, expect\n\n@pytest.fixture(scope=\"session\")\ndef streamlit_app_url():\n    \"\"\"Get Streamlit app URL for testing.\"\"\"\n    return \"http://localhost:8501\"\n\ndef test_streamlit_query_workflow(page: Page, streamlit_app_url):\n    \"\"\"Test complete Streamlit query workflow.\"\"\"\n    # Navigate to app\n    page.goto(streamlit_app_url)\n\n    # Wait for app to load\n    page.wait_for_selector('[data-testid=\"query-input\"]')\n\n    # Enter query\n    query_input = page.locator('[data-testid=\"query-input\"]')\n    query_input.fill(\"What is the main topic?\")\n\n    # Submit query\n    submit_button = page.locator('[data-testid=\"submit-button\"]')\n    submit_button.click()\n\n    # Wait for response\n    page.wait_for_selector('[data-testid=\"response\"]', timeout=10000)\n\n    # Verify response\n    response = page.locator('[data-testid=\"response\"]')\n    expect(response).to_contain_text(\"main topic\")\n\n    # Verify observability (check sidebar stats)\n    sidebar_stats = page.locator('[data-testid=\"sidebar-stats\"]')\n    expect(sidebar_stats).to_be_visible()\n</code></pre> <p>Screenshots e Video:</p> <pre><code>def test_streamlit_workflow_with_recording(page: Page):\n    \"\"\"Test with screenshot and video recording.\"\"\"\n    # Enable video recording\n    page.video.path()  # Video saved automatically\n\n    # Take screenshot on failure\n    try:\n        # Test logic\n        pass\n    except Exception:\n        page.screenshot(path=\"test_failure.png\")\n        raise\n</code></pre>"},{"location":"testing-strategy/#52-e2e-test-data","title":"5.2 E2E Test Data","text":"<p>Golden Dataset: - 20+ query-answer pairs per RAGAS evaluation - Stored in <code>tests/fixtures/golden_dataset.json</code></p> <p>Esempio:</p> <pre><code>{\n  \"queries\": [\n    {\n      \"question\": \"What is the main topic of the document?\",\n      \"ground_truth\": \"The document discusses RAG architecture patterns.\",\n      \"expected_sources\": [\"doc1\", \"doc2\"]\n    },\n    {\n      \"question\": \"How does embedding generation work?\",\n      \"ground_truth\": \"Embedding generation uses OpenAI API to convert text to vectors.\",\n      \"expected_sources\": [\"doc3\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"testing-strategy/#6-ragas-evaluation","title":"6. RAGAS Evaluation","text":""},{"location":"testing-strategy/#61-ragas-metrics","title":"6.1 RAGAS Metrics","text":"<p>Metriche Standard: - Faithfulness (0-1): Consistenza fattuale della risposta rispetto al contesto - Answer Relevancy (0-1): Rilevanza della risposta alla domanda - Context Precision (0-1): Precisione del retrieval (chunk rilevanti in top) - Context Recall (0-1): Completezza del retrieval (tutti chunk rilevanti recuperati)</p> <p>Thresholds: - Faithfulness &gt; 0.85 - Answer Relevancy &gt; 0.80 - Context Precision &gt; 0.75 - Context Recall &gt; 0.70</p>"},{"location":"testing-strategy/#62-ragas-implementation","title":"6.2 RAGAS Implementation","text":"<p>Setup:</p> <pre><code>pip install ragas datasets\n</code></pre> <p>Evaluation Script:</p> <pre><code>from ragas import evaluate\nfrom ragas.metrics import (\n    Faithfulness,\n    AnswerRelevancy,\n    ContextPrecision,\n    ContextRecall\n)\nfrom datasets import Dataset\n\nasync def run_ragas_evaluation():\n    \"\"\"Run RAGAS evaluation on golden dataset.\"\"\"\n    # Load golden dataset\n    golden_dataset = load_golden_dataset(\"tests/fixtures/golden_dataset.json\")\n\n    # Generate answers using RAG pipeline\n    evaluation_data = []\n    for item in golden_dataset:\n        answer, contexts = await generate_rag_response(item[\"question\"])\n        evaluation_data.append({\n            \"question\": item[\"question\"],\n            \"answer\": answer,\n            \"contexts\": contexts,\n            \"ground_truths\": [item[\"ground_truth\"]]\n        })\n\n    # Create dataset\n    eval_dataset = Dataset.from_list(evaluation_data)\n\n    # Run evaluation\n    metrics = [\n        Faithfulness(),\n        AnswerRelevancy(),\n        ContextPrecision(),\n        ContextRecall()\n    ]\n\n    results = evaluate(\n        dataset=eval_dataset,\n        metrics=metrics,\n        llm=evaluator_llm  # Separate LLM for evaluation\n    )\n\n    # Verify thresholds\n    assert results[\"faithfulness\"] &gt; 0.85\n    assert results[\"answer_relevancy\"] &gt; 0.80\n    assert results[\"context_precision\"] &gt; 0.75\n    assert results[\"context_recall\"] &gt; 0.70\n\n    return results\n</code></pre>"},{"location":"testing-strategy/#63-langfuse-integration","title":"6.3 LangFuse Integration","text":"<p>Track RAGAS Metrics:</p> <pre><code>from langfuse import Langfuse\n\nlangfuse = Langfuse()\n\nasync def track_ragas_evaluation(results: dict):\n    \"\"\"Track RAGAS evaluation results in LangFuse.\"\"\"\n    trace = langfuse.trace(\n        name=\"ragas_evaluation\",\n        metadata={\n            \"faithfulness\": results[\"faithfulness\"],\n            \"answer_relevancy\": results[\"answer_relevancy\"],\n            \"context_precision\": results[\"context_precision\"],\n            \"context_recall\": results[\"context_recall\"]\n        }\n    )\n\n    # Track individual query scores\n    for i, query_result in enumerate(results[\"individual_scores\"]):\n        trace.span(\n            name=f\"query_{i}\",\n            metadata=query_result\n        )\n\n    trace.end()\n</code></pre>"},{"location":"testing-strategy/#7-test-configuration","title":"7. Test Configuration","text":""},{"location":"testing-strategy/#71-pytest-configuration","title":"7.1 pytest Configuration","text":"<p><code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\", \"*_test.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\nmarkers = [\n    \"unit: Unit tests\",\n    \"integration: Integration tests\",\n    \"e2e: End-to-end tests\",\n    \"slow: Slow running tests\",\n    \"ragas: RAGAS evaluation tests\"\n]\n</code></pre>"},{"location":"testing-strategy/#72-coverage-configuration","title":"7.2 Coverage Configuration","text":"<p>Coverage Thresholds:</p> <pre><code>[tool.coverage.run]\nsource = [\"core\", \"docling_mcp\", \"ingestion\", \"utils\"]\nomit = [\"*/tests/*\", \"*/test_*.py\"]\n\n[tool.coverage.report]\nprecision = 2\nshow_missing = true\nskip_covered = false\nfail_under = 70\n\n[tool.coverage.html]\ndirectory = \"htmlcov\"\n</code></pre>"},{"location":"testing-strategy/#73-test-markers","title":"7.3 Test Markers","text":"<p>Uso dei Markers:</p> <pre><code>import pytest\n\n@pytest.mark.unit\ndef test_unit_functionality():\n    \"\"\"Unit test.\"\"\"\n    pass\n\n@pytest.mark.integration\ndef test_integration_functionality():\n    \"\"\"Integration test.\"\"\"\n    pass\n\n@pytest.mark.e2e\n@pytest.mark.slow\ndef test_e2e_workflow():\n    \"\"\"E2E test (slow).\"\"\"\n    pass\n\n@pytest.mark.ragas\ndef test_ragas_evaluation():\n    \"\"\"RAGAS evaluation test.\"\"\"\n    pass\n</code></pre> <p>Eseguire test per categoria:</p> <pre><code># Solo unit tests\npytest -m unit\n\n# Solo integration tests\npytest -m integration\n\n# Escludere test lenti\npytest -m \"not slow\"\n\n# Solo RAGAS evaluation\npytest -m ragas\n</code></pre>"},{"location":"testing-strategy/#8-cicd-integration","title":"8. CI/CD Integration","text":""},{"location":"testing-strategy/#81-github-actions-workflow","title":"8.1 GitHub Actions Workflow","text":"<p>Test Pipeline:</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install uv\n          uv pip install -e \".[dev]\"\n\n      - name: Run unit tests\n        run: |\n          pytest tests/unit/ -v --cov=core --cov=docling_mcp --cov-report=xml\n\n      - name: Run integration tests\n        run: |\n          pytest tests/integration/ -v\n\n      - name: Check coverage\n        run: |\n          pytest --cov=core --cov=docling_mcp --cov-fail-under=70\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n</code></pre>"},{"location":"testing-strategy/#82-pre-commit-hooks","title":"8.2 Pre-commit Hooks","text":"<p>Test Before Commit:</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: pytest-unit\n        name: Run unit tests\n        entry: pytest tests/unit/ -v\n        language: system\n        pass_filenames: false\n        always_run: true\n</code></pre>"},{"location":"testing-strategy/#9-test-best-practices","title":"9. Test Best Practices","text":""},{"location":"testing-strategy/#91-test-isolation","title":"9.1 Test Isolation","text":"<p>Principi: - Ogni test deve essere indipendente - Non dipendere da ordine di esecuzione - Cleanup dopo ogni test</p> <p>Esempio:</p> <pre><code>@pytest.fixture(autouse=True)\ndef cleanup_test_state():\n    \"\"\"Cleanup test state before and after each test.\"\"\"\n    # Setup\n    yield\n    # Cleanup\n    reset_global_state()\n    clear_test_cache()\n</code></pre>"},{"location":"testing-strategy/#92-test-data-management","title":"9.2 Test Data Management","text":"<p>Pattern: - Usare fixtures per test data riutilizzabili - Golden dataset per RAGAS evaluation - Test database separato per integration tests</p> <p>Esempio:</p> <pre><code>@pytest.fixture\ndef sample_documents():\n    \"\"\"Sample documents for testing.\"\"\"\n    return [\n        {\n            \"id\": \"doc1\",\n            \"title\": \"Test Document 1\",\n            \"content\": \"This is test content for document 1.\",\n            \"source\": \"test-source\"\n        },\n        {\n            \"id\": \"doc2\",\n            \"title\": \"Test Document 2\",\n            \"content\": \"This is test content for document 2.\",\n            \"source\": \"test-source\"\n        }\n    ]\n</code></pre>"},{"location":"testing-strategy/#93-error-testing","title":"9.3 Error Testing","text":"<p>Test Error Cases:</p> <pre><code>def test_search_with_invalid_query_raises_error():\n    \"\"\"Test search raises error for invalid query.\"\"\"\n    with pytest.raises(ValueError, match=\"Query cannot be empty\"):\n        search_knowledge_base(\"\")\n\ndef test_database_connection_error_handled_gracefully():\n    \"\"\"Test graceful handling of database errors.\"\"\"\n    with patch('utils.db_utils.db_pool.acquire', side_effect=ConnectionError):\n        # Should not crash, should return error message\n        result = await query_knowledge_base(\"test\")\n        assert \"error\" in result.lower()\n</code></pre>"},{"location":"testing-strategy/#94-performance-testing","title":"9.4 Performance Testing","text":"<p>Test Performance Critical Paths:</p> <pre><code>import time\n\ndef test_embedding_generation_performance():\n    \"\"\"Test embedding generation meets performance SLO.\"\"\"\n    start_time = time.time()\n    embedding = await generate_embedding(\"test query\")\n    duration = time.time() - start_time\n\n    # SLO: &lt;500ms for embedding generation\n    assert duration &lt; 0.5, f\"Embedding took {duration}s, exceeds 500ms SLO\"\n    assert len(embedding) &gt; 0\n</code></pre>"},{"location":"testing-strategy/#10-test-maintenance","title":"10. Test Maintenance","text":""},{"location":"testing-strategy/#101-test-documentation","title":"10.1 Test Documentation","text":"<p>Documentare Test: - Docstring descrittivi per ogni test - Spiegare cosa testa e perch\u00e9 - Documentare setup requirements</p> <p>Esempio:</p> <pre><code>def test_langfuse_graceful_degradation():\n    \"\"\"\n    Test LangFuse graceful degradation when unavailable.\n\n    This test verifies that the system continues to function\n    normally when LangFuse SDK is not installed or unavailable.\n    This is critical for production reliability.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"testing-strategy/#102-test-refactoring","title":"10.2 Test Refactoring","text":"<p>Quando Refactorare: - Test duplicati \u2192 Estrai fixture comune - Test troppo lunghi \u2192 Suddividi in test pi\u00f9 piccoli - Test instabili \u2192 Isola dipendenze esterne</p> <p>Esempio:</p> <pre><code># \u274c Errato - Test troppo lungo\ndef test_complete_workflow():\n    # Setup (20 lines)\n    # Test step 1 (10 lines)\n    # Test step 2 (10 lines)\n    # Test step 3 (10 lines)\n    # Assertions (10 lines)\n    pass\n\n# \u2705 Corretto - Test focalizzati\ndef test_workflow_step1():\n    \"\"\"Test workflow step 1.\"\"\"\n    pass\n\ndef test_workflow_step2():\n    \"\"\"Test workflow step 2.\"\"\"\n    pass\n\ndef test_workflow_integration():\n    \"\"\"Test complete workflow integration.\"\"\"\n    pass\n</code></pre>"},{"location":"testing-strategy/#11-testing-checklist","title":"11. Testing Checklist","text":"<p>Prima di considerare una feature completa:</p> <ul> <li>[ ] Unit tests scritti per tutte le funzioni pubbliche</li> <li>[ ] Integration tests per interazioni critiche</li> <li>[ ] E2E tests per workflow principali (se applicabile)</li> <li>[ ] Coverage &gt;70% per moduli core</li> <li>[ ] Test error cases (invalid input, failures)</li> <li>[ ] Test performance critical paths (se applicabile)</li> <li>[ ] RAGAS evaluation per funzionalit\u00e0 RAG (se applicabile)</li> <li>[ ] Test documentati con docstring descrittivi</li> <li>[ ] Test isolati e indipendenti</li> <li>[ ] CI/CD pipeline passa tutti i test</li> </ul>"},{"location":"testing-strategy/#12-references","title":"12. References","text":""},{"location":"testing-strategy/#internal-documentation","title":"Internal Documentation","text":"<ul> <li>Coding Standards: Code style guide and testing standards section</li> <li>Unified Project Structure: Test organization standards and directory structure</li> <li>Architecture: System architecture and testing infrastructure decisions</li> <li>Epic 5 Requirements: Complete Epic 5 requirements with acceptance criteria</li> <li>Development Guide: Setup instructions and test execution workflow</li> </ul>"},{"location":"testing-strategy/#external-references","title":"External References","text":"<ul> <li>pytest Documentation: https://docs.pytest.org/</li> <li>pytest-asyncio: https://pytest-asyncio.readthedocs.io/</li> <li>RAGAS Documentation: https://docs.ragas.io/</li> <li>Playwright Documentation: https://playwright.dev/python/</li> <li>PydanticAI Testing: https://ai.pydantic.dev/testing/</li> <li>Coverage.py: https://coverage.readthedocs.io/</li> </ul>"},{"location":"testing-strategy/#changelog","title":"Changelog","text":"<ul> <li>2025-01-27: Initial version based on existing test structure and Epic 5 requirements</li> </ul>"},{"location":"troubleshooting-guide/","title":"Guida al Troubleshooting","text":"<p>Questa guida fornisce soluzioni per i problemi pi\u00f9 comuni nel progetto Docling RAG Agent, con focus particolare sulla configurazione e troubleshooting del server MCP.</p>"},{"location":"troubleshooting-guide/#indice","title":"Indice","text":"<ul> <li>Requisiti Critici</li> <li>Troubleshooting MCP Server</li> <li>Problemi di Connessione Database</li> <li>Problemi di Ambiente</li> <li>Script di Verifica</li> <li>Avvio Automatico API (Windows)</li> </ul>"},{"location":"troubleshooting-guide/#requisiti-critici","title":"Requisiti Critici","text":""},{"location":"troubleshooting-guide/#api-rag","title":"API RAG","text":"<p>Il server MCP richiede che l'API RAG sia in esecuzione su <code>http://localhost:8000</code>.</p>"},{"location":"troubleshooting-guide/#avvio-api-rag-obbligatorio","title":"Avvio API RAG (OBBLIGATORIO)","text":"<pre><code>cd C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\nuv run uvicorn api.main:app --host 0.0.0.0 --port 8000\n</code></pre> <p>L'API deve rimanere in esecuzione in un terminale separato mentre usi il server MCP in Cursor.</p>"},{"location":"troubleshooting-guide/#verifica-api-attiva","title":"Verifica API Attiva","text":"<pre><code>curl http://localhost:8000/health\n# Deve restituire: {\"status\":\"ok\",\"timestamp\":...}\n</code></pre>"},{"location":"troubleshooting-guide/#troubleshooting-mcp-server","title":"Troubleshooting MCP Server","text":""},{"location":"troubleshooting-guide/#problema-tool-non-disponibile","title":"Problema: Tool Non Disponibile","text":"<p>Il tool <code>query_knowledge_base</code> non \u00e8 disponibile in Cursor, anche se \u00e8 registrato correttamente nel server.</p>"},{"location":"troubleshooting-guide/#diagnostica","title":"Diagnostica","text":"<pre><code>uv run python debug_mcp_tools.py\n</code></pre> <p>Risultato atteso: Entrambi i tool sono registrati correttamente:</p> <ul> <li><code>query_knowledge_base</code> \u2705</li> <li><code>list_knowledge_base_documents</code> \u2705</li> </ul>"},{"location":"troubleshooting-guide/#step-1-verifica-configurazione-mcp","title":"Step 1: Verifica Configurazione MCP","text":"<p>Il file di configurazione MCP si trova in:</p> Sistema Operativo Percorso Windows <code>%APPDATA%\\Cursor\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\cline_mcp_settings.json</code> Mac <code>~/Library/Application Support/Cursor/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json</code> Linux <code>~/.config/Cursor/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json</code> <p>In alternativa, configurazione workspace in <code>.cursor/mcp.json</code> (se supportato).</p>"},{"location":"troubleshooting-guide/#step-2-configurazione-corretta","title":"Step 2: Configurazione Corretta","text":"<p>Per Cursor, la configurazione dovrebbe essere:</p> <pre><code>{\n  \"mcpServers\": {\n    \"docling-rag\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--project\",\n        \"C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\",\n        \"python\",\n        \"-m\", \"docling_mcp.server\"\n      ],\n      \"cwd\": \"C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\",\n      \"env\": {\n        \"PYTHONPATH\": \"C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\"\n      }\n    }\n  }\n}\n</code></pre> <p>Nota: Usa percorsi assoluti per Windows.</p>"},{"location":"troubleshooting-guide/#step-3-verifica-server-avviato","title":"Step 3: Verifica Server Avviato","text":"<ol> <li>Apri Cursor Settings (<code>Ctrl+Shift+J</code> o <code>Cmd+Shift+J</code>)</li> <li>Vai a Features &gt; MCP</li> <li>Verifica che <code>docling-rag</code> sia presente e abbia un indicatore verde (attivo)</li> <li>Se non \u00e8 presente o \u00e8 rosso:</li> <li>Controlla i log MCP in Cursor</li> <li>Verifica che il comando <code>uv run python -m docling_mcp.server</code> funzioni manualmente</li> </ol>"},{"location":"troubleshooting-guide/#step-4-test-manuale-server","title":"Step 4: Test Manuale Server","text":"<pre><code>cd C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\nuv run python -m docling_mcp.server\n</code></pre> <p>Il server dovrebbe avviarsi senza errori. Premi <code>Ctrl+C</code> per fermarlo.</p>"},{"location":"troubleshooting-guide/#step-5-restart-cursor","title":"Step 5: Restart Cursor","text":"<p>Dopo aver configurato correttamente:</p> <ol> <li>Chiudi completamente Cursor</li> <li>Riapri Cursor</li> <li>Il server MCP dovrebbe avviarsi automaticamente</li> </ol>"},{"location":"troubleshooting-guide/#step-6-verifica-tool-disponibili","title":"Step 6: Verifica Tool Disponibili","text":"<p>In Cursor, prova a chiamare:</p> <pre><code>Usa il tool mcp_docling-rag_query_knowledge_base per cercare informazioni su Pydantic AI\n</code></pre> <p>Se il tool non \u00e8 ancora disponibile, controlla i log MCP per errori.</p>"},{"location":"troubleshooting-guide/#configurazione-alternativa-se-stdio-non-funziona","title":"Configurazione Alternativa (se stdio non funziona)","text":"<p>Se la configurazione stdio non funziona, prova con percorsi assoluti:</p> <pre><code>{\n  \"mcpServers\": {\n    \"docling-rag\": {\n      \"command\": \"C:/Users/user/.cargo/bin/uv.exe\",\n      \"args\": [\n        \"run\",\n        \"--project\",\n        \"C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\",\n        \"python\",\n        \"-m\", \"docling_mcp.server\"\n      ],\n      \"cwd\": \"C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent\"\n    }\n  }\n}\n</code></pre>"},{"location":"troubleshooting-guide/#configurazione-cursor-metodo-gui","title":"Configurazione Cursor - Metodo GUI","text":"<ol> <li>Apri Cursor Settings:</li> <li><code>Ctrl+Shift+J</code> (Windows/Linux)</li> <li> <p><code>Cmd+Shift+J</code> (Mac)</p> </li> <li> <p>Naviga a Features &gt; MCP (o Tools &amp; Integrations &gt; MCP)</p> </li> <li> <p>Clicca \"Add Server\" o \"Add Custom MCP\"</p> </li> <li> <p>Configura:</p> </li> <li>Name: <code>docling-rag</code></li> <li>Type: <code>stdio</code></li> <li>Command: <code>uv</code></li> <li>Args: <code>run --project C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent python -m docling_mcp.server</code></li> <li> <p>Working Directory: <code>C:/Users/user/Desktop/Claude-Code/ottomator-agents/docling-rag-agent</code></p> </li> <li> <p>Salva e riavvia Cursor completamente</p> </li> </ol>"},{"location":"troubleshooting-guide/#log-per-debug","title":"Log per Debug","text":"<p>Se il problema persiste, controlla:</p> <ul> <li>Log MCP in Cursor: <code>View &gt; Output &gt; MCP Servers</code></li> <li>Log del server: esegui manualmente <code>uv run python -m docling_mcp.server</code> e verifica errori</li> <li>Variabili d'ambiente: verifica che <code>.env</code> sia configurato correttamente</li> <li>Esegui verifica completa: <code>uv run python scripts/verify_mcp_setup.py</code></li> </ul>"},{"location":"troubleshooting-guide/#note-importanti","title":"Note Importanti","text":"<ul> <li>Il server MCP deve essere avviato da Cursor, non manualmente</li> <li>Assicurati che <code>uv</code> sia nel PATH o usa il percorso assoluto</li> <li>Verifica che tutte le dipendenze siano installate: <code>uv sync</code></li> <li>Il server richiede che l'API RAG sia disponibile (verifica <code>client.api_client.RAGClient</code>)</li> </ul>"},{"location":"troubleshooting-guide/#problemi-di-connessione-database","title":"Problemi di Connessione Database","text":""},{"location":"troubleshooting-guide/#problemi-comuni","title":"Problemi Comuni","text":"Problema Causa Soluzione <code>connection refused</code> Database non raggiungibile Verifica <code>DATABASE_URL</code> e che PostgreSQL sia attivo <code>extension \"vector\" not found</code> PGVector non installato Esegui <code>CREATE EXTENSION vector;</code> o usa immagine Docker <code>pgvector/pgvector:pg16</code>"},{"location":"troubleshooting-guide/#verifica-connessione","title":"Verifica Connessione","text":"<pre><code># Verifica connessione\npsql $DATABASE_URL -c \"SELECT 1\"\n\n# Verifica estensione PGVector\npsql $DATABASE_URL -c \"SELECT * FROM pg_extension WHERE extname = 'vector'\"\n\n# Test performance index\npython scripts/optimize_database.py --check\n</code></pre>"},{"location":"troubleshooting-guide/#problemi-di-ambiente","title":"Problemi di Ambiente","text":""},{"location":"troubleshooting-guide/#problemi-comuni_1","title":"Problemi Comuni","text":"Problema Causa Soluzione <code>OPENAI_API_KEY not set</code> Variabile mancante Aggiungi <code>OPENAI_API_KEY</code> nel file <code>.env</code> <code>uv: command not found</code> UV non installato Installa: <code>curl -LsSf https://astral.sh/uv/install.sh \\| sh</code> <code>Python version mismatch</code> Python &lt; 3.10 Aggiorna Python a 3.10+ <code>ImportError: docling</code> Dipendenze mancanti Esegui <code>uv sync</code>"},{"location":"troubleshooting-guide/#reset-ambiente","title":"Reset Ambiente","text":"<pre><code># Rimuovi cache e reinstalla\nrm -rf .venv\nuv sync\n\n# Reset database (ATTENZIONE: cancella tutti i dati)\npsql $DATABASE_URL &lt; sql/optimize_index.sql\n</code></pre>"},{"location":"troubleshooting-guide/#script-di-verifica","title":"Script di Verifica","text":""},{"location":"troubleshooting-guide/#verifica-completa-setup-mcp","title":"Verifica Completa Setup MCP","text":"<pre><code>uv run python scripts/verify_mcp_setup.py\n</code></pre> <p>Lo script verifica:</p> <ul> <li>\u2705 Dipendenze installate</li> <li>\u2705 Variabili d'ambiente configurate</li> <li>\u2705 Server MCP inizializzato correttamente</li> <li>\u2705 Tool registrati (<code>query_knowledge_base</code>, <code>list_knowledge_base_documents</code>)</li> <li>\u2705 API RAG disponibile e accessibile</li> <li>\u26a0\ufe0f Configurazione Cursor (se trovata)</li> </ul>"},{"location":"troubleshooting-guide/#verifica-tool","title":"Verifica Tool","text":"<p>Dopo la configurazione, verifica che entrambi i tool siano disponibili:</p> <ol> <li><code>mcp_docling-rag_query_knowledge_base</code> - Per ricerche semantiche</li> <li><code>mcp_docling-rag_list_knowledge_base_documents</code> - Per elencare documenti</li> </ol>"},{"location":"troubleshooting-guide/#avvio-automatico-api-windows","title":"Avvio Automatico API (Windows)","text":"<p>Script disponibili in <code>scripts/</code>:</p>"},{"location":"troubleshooting-guide/#avvio-manuale","title":"Avvio Manuale","text":"<pre><code># Con finestra visibile\nscripts\\start_api.bat\n\n# Oppure da terminale\nuv run uvicorn api.main:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"troubleshooting-guide/#arresto-api","title":"Arresto API","text":"<pre><code>scripts\\stop_api.bat\n</code></pre>"},{"location":"troubleshooting-guide/#configurazione-avvio-automatico-al-login","title":"Configurazione Avvio Automatico al Login","text":"<ol> <li>Esegui come Amministratore:</li> </ol> <p><code>bash    scripts\\install_autostart.bat</code></p> <ol> <li> <p>L'API si avvier\u00e0 automaticamente al prossimo login Windows</p> </li> <li> <p>Per avviare subito (senza riavvio):</p> </li> </ol> <p><code>bash    schtasks /run /tn \"DoclingRAG-API\"</code></p>"},{"location":"troubleshooting-guide/#rimozione-avvio-automatico","title":"Rimozione Avvio Automatico","text":"<pre><code>scripts\\uninstall_autostart.bat\n</code></pre>"},{"location":"troubleshooting-guide/#file-script","title":"File Script","text":"Script Descrizione <code>start_api.bat</code> Avvio manuale con finestra <code>start_api_hidden.vbs</code> Avvio silenzioso (background) <code>stop_api.bat</code> Arresto API <code>install_autostart.bat</code> Installa avvio automatico (Admin) <code>uninstall_autostart.bat</code> Rimuovi avvio automatico"},{"location":"api-reference/","title":"API Reference Overview","text":"<p>This section provides detailed documentation for the Docling RAG Agent codebase.</p>"},{"location":"api-reference/#modules","title":"Modules","text":"<ul> <li>Core: Core RAG logic and agent implementation.</li> <li>Ingestion: Document processing pipeline.</li> <li>Utilities: Shared utilities and data models.</li> </ul>"},{"location":"api-reference/core/","title":"Core Module","text":""},{"location":"api-reference/core/#rag-service","title":"RAG Service","text":""},{"location":"api-reference/core/#core.rag_service","title":"<code>core.rag_service</code>","text":""},{"location":"api-reference/core/#core.rag_service--core-rag-service","title":"Core RAG Service","text":"<p>Pure business logic for searching the knowledge base. Decoupled from PydanticAI and Streamlit.</p> <p>Performance Optimizations: - Global embedder instance initialized at startup - Persistent LRU cache for embeddings (2000 entries) - Eliminates 300-500ms overhead per query</p>"},{"location":"api-reference/core/#core.rag_service.close_global_embedder","title":"<code>close_global_embedder()</code>  <code>async</code>","text":"<p>Close global embedder and cleanup resources.</p> Source code in <code>core/rag_service.py</code> <pre><code>async def close_global_embedder():\n    \"\"\"Close global embedder and cleanup resources.\"\"\"\n    global _global_embedder, _initialization_task\n\n    if _initialization_task and not _initialization_task.done():\n        _initialization_task.cancel()\n        try:\n            await _initialization_task\n        except asyncio.CancelledError:\n            pass\n\n    if _global_embedder is None:\n        return\n\n    # Cleanup if needed (embedder doesn't require explicit cleanup currently)\n    _global_embedder = None\n    _embedder_ready.clear()\n    _initialization_task = None\n    logger.info(\"\u2713 Global embedder closed\")\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.generate_query_embedding","title":"<code>generate_query_embedding(query: str) -&gt; tuple[List[float], float]</code>  <code>async</code>","text":"<p>Generate embedding for a query string.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query text to embed</p> required <p>Returns:</p> Type Description <code>tuple[List[float], float]</code> <p>Tuple of (embedding vector, duration_ms)</p> Note <p>This function is separated from search to allow timing breakdown in LangFuse spans (AC #2: separate spans for embedding and DB search).</p> Source code in <code>core/rag_service.py</code> <pre><code>async def generate_query_embedding(query: str) -&gt; tuple[List[float], float]:\n    \"\"\"\n    Generate embedding for a query string.\n\n    Args:\n        query: The query text to embed\n\n    Returns:\n        Tuple of (embedding vector, duration_ms)\n\n    Note:\n        This function is separated from search to allow timing breakdown\n        in LangFuse spans (AC #2: separate spans for embedding and DB search).\n    \"\"\"\n    embedder = await get_global_embedder()\n\n    embed_start = time.time()\n    query_embedding = await embedder.embed_query(query)\n    duration_ms = (time.time() - embed_start) * 1000\n\n    return query_embedding, duration_ms\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.get_global_embedder","title":"<code>get_global_embedder()</code>  <code>async</code>","text":"<p>Get the global embedder instance. Waits for initialization if it's still in progress.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If embedder not initialized or failed</p> Source code in <code>core/rag_service.py</code> <pre><code>async def get_global_embedder():\n    \"\"\"\n    Get the global embedder instance.\n    Waits for initialization if it's still in progress.\n\n    Raises:\n        RuntimeError: If embedder not initialized or failed\n    \"\"\"\n    if _global_embedder is None:\n        if not _initialization_task:\n            raise RuntimeError(\n                \"Global embedder not initialized. Call initialize_global_embedder() first.\"\n            )\n\n        if not _embedder_ready.is_set():\n            logger.info(\"Waiting for embedder initialization to complete (first request)...\")\n            try:\n                # Wait up to 60 seconds for initialization\n                await asyncio.wait_for(_embedder_ready.wait(), timeout=60.0)\n            except asyncio.TimeoutError:\n                raise RuntimeError(\n                    \"Timeout waiting for embedder initialization (takes ~40s cold start)\"\n                )\n\n    if _global_embedder is None:\n        raise RuntimeError(\"Global embedder failed to initialize. Check server logs for errors.\")\n    return _global_embedder\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.initialize_global_embedder","title":"<code>initialize_global_embedder()</code>  <code>async</code>","text":"<p>Initialize global embedder instance at server startup.</p> <p>This is a critical performance optimization that: - Eliminates per-query embedder instantiation (300-500ms overhead) - Enables persistent caching across requests - Pre-warms OpenAI API connection</p> <p>OPTIMIZED: Runs in background to prevent blocking server startup (MCP handshake timeout).</p> Source code in <code>core/rag_service.py</code> <pre><code>async def initialize_global_embedder():\n    \"\"\"\n    Initialize global embedder instance at server startup.\n\n    This is a critical performance optimization that:\n    - Eliminates per-query embedder instantiation (300-500ms overhead)\n    - Enables persistent caching across requests\n    - Pre-warms OpenAI API connection\n\n    OPTIMIZED: Runs in background to prevent blocking server startup (MCP handshake timeout).\n    \"\"\"\n    global _global_embedder, _initialization_task\n\n    if _global_embedder is not None:\n        logger.warning(\"Global embedder already initialized\")\n        return\n\n    if _initialization_task is not None:\n        logger.warning(\"Global embedder initialization already in progress\")\n        return\n\n    async def _init_task():\n        global _global_embedder\n        try:\n            start_time = time.time()\n            logger.info(\"Starting background embedder initialization (loading heavy models)...\")\n\n            # Offload heavy import and creation to thread to avoid blocking asyncio loop\n            # This is critical because importing transformers/docling takes ~40s\n            _global_embedder = await asyncio.to_thread(_create_embedder_sync)\n\n            # Enhance cache size (default is 1000, we increase to 2000)\n            if hasattr(_global_embedder, \"generate_embedding\"):\n                logger.info(\"Embedder cache enabled with enhanced capacity\")\n\n            elapsed = (time.time() - start_time) * 1000\n            logger.info(f\"\u2713 Global embedder initialized in {elapsed:.0f}ms\")\n            _embedder_ready.set()\n\n        except Exception as e:\n            logger.error(f\"\u274c Failed to initialize embedder in background: {e}\", exc_info=True)\n            # Don't crash the server, subsequent queries will fail gracefully\n\n    # Start initialization task\n    _initialization_task = asyncio.create_task(_init_task())\n    logger.info(\"Background initialization task started\")\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.search_knowledge_base","title":"<code>search_knowledge_base(query: str, limit: int = 5, source_filter: str | None = None) -&gt; str</code>  <code>async</code>","text":"<p>Search the knowledge base using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to find relevant information</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return (default: 5)</p> <code>5</code> <code>source_filter</code> <code>str | None</code> <p>Optional filter to search only in specific documentation sources.           Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"           If provided, only documents whose source path contains this string will be searched.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted search results with source citations</p> Performance <ul> <li>Uses global embedder instance (eliminates 300-500ms overhead)</li> <li>Leverages persistent cache for common queries</li> <li>Optimized DB connection pooling</li> </ul> Source code in <code>core/rag_service.py</code> <pre><code>async def search_knowledge_base(\n    query: str, limit: int = 5, source_filter: str | None = None\n) -&gt; str:\n    \"\"\"\n    Search the knowledge base using semantic similarity.\n\n    Args:\n        query: The search query to find relevant information\n        limit: Maximum number of results to return (default: 5)\n        source_filter: Optional filter to search only in specific documentation sources.\n                      Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"\n                      If provided, only documents whose source path contains this string will be searched.\n\n    Returns:\n        Formatted search results with source citations\n\n    Performance:\n        - Uses global embedder instance (eliminates 300-500ms overhead)\n        - Leverages persistent cache for common queries\n        - Optimized DB connection pooling\n    \"\"\"\n    start_time = time.time()\n    timing = {}\n\n    try:\n        # Use structured search internally to avoid code duplication\n        # But we need to reimplement here to keep the exact string formatting logic\n        # or refactor completely. For now, let's keep the existing logic but maybe use the structured function?\n        # Actually, let's just call the structured function and format the output.\n\n        structured_data = await search_knowledge_base_structured(query, limit, source_filter)\n        results = structured_data[\"results\"]\n        timing = structured_data[\"timing\"]\n\n        # Format results for response\n        format_start = time.time()\n        if not results:\n            filter_msg = f\" in '{source_filter}'\" if source_filter else \"\"\n            return (\n                f\"No relevant information found in the knowledge base{filter_msg} for your query.\"\n            )\n\n        # Build response with sources\n        response_parts = []\n        for row in results:\n            content = row[\"content\"]\n            doc_title = row[\"title\"]\n\n            response_parts.append(f\"[Source: {doc_title}]\\n{content}\\n\")\n\n        if not response_parts:\n            return \"Found some results but they may not be directly relevant to your query. Please try rephrasing your question.\"\n\n        filter_note = f\" (filtered by: {source_filter})\" if source_filter else \"\"\n        result = f\"Found {len(response_parts)} relevant results{filter_note}:\\n\\n\" + \"\\n---\\n\".join(\n            response_parts\n        )\n\n        timing[\"format_response_ms\"] = (time.time() - format_start) * 1000\n        timing[\"total_ms\"] = (time.time() - start_time) * 1000\n\n        # Log performance metrics\n        logger.info(\n            f\"\u23f1\ufe0f  Search performance: \"\n            f\"embedding={timing.get('embedding_ms', 0):.0f}ms | \"\n            f\"db={timing.get('db_ms', 0):.0f}ms | \"\n            f\"format={timing['format_response_ms']:.0f}ms | \"\n            f\"total={timing['total_ms']:.0f}ms\"\n        )\n\n        return result\n\n    except Exception as e:\n        timing[\"total_ms\"] = (time.time() - start_time) * 1000\n        logger.error(\n            f\"\u274c Knowledge base search failed after {timing['total_ms']:.0f}ms: {e}\", exc_info=True\n        )\n        return f\"I encountered an error searching the knowledge base: {str(e)}\"\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.search_knowledge_base_structured","title":"<code>search_knowledge_base_structured(query: str, limit: int = 5, source_filter: str | None = None) -&gt; Dict[str, Any]</code>  <code>async</code>","text":"<p>Search the knowledge base and return structured results (for API usage).</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing:</p> <code>Dict[str, Any]</code> <ul> <li>results: List of dicts (content, source, title, similarity, metadata)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>timing: Dict of performance metrics</li> </ul> Note <p>This is a convenience wrapper that calls generate_query_embedding() and search_with_embedding() internally. For fine-grained timing control (e.g., separate LangFuse spans), use those functions directly.</p> Source code in <code>core/rag_service.py</code> <pre><code>async def search_knowledge_base_structured(\n    query: str, limit: int = 5, source_filter: str | None = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Search the knowledge base and return structured results (for API usage).\n\n    Returns:\n        Dict containing:\n        - results: List of dicts (content, source, title, similarity, metadata)\n        - timing: Dict of performance metrics\n\n    Note:\n        This is a convenience wrapper that calls generate_query_embedding()\n        and search_with_embedding() internally. For fine-grained timing control\n        (e.g., separate LangFuse spans), use those functions directly.\n    \"\"\"\n    start_time = time.time()\n    timing = {}\n\n    try:\n        # Generate embedding\n        query_embedding, embedding_ms = await generate_query_embedding(query)\n        timing[\"embedding_ms\"] = embedding_ms\n\n        # Search with embedding\n        results, db_ms = await search_with_embedding(query_embedding, limit, source_filter)\n        timing[\"db_ms\"] = db_ms\n        timing[\"total_ms\"] = (time.time() - start_time) * 1000\n\n        return {\"results\": results, \"timing\": timing}\n\n    except Exception as e:\n        logger.error(f\"Structured search failed: {e}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api-reference/core/#core.rag_service.search_with_embedding","title":"<code>search_with_embedding(embedding: List[float], limit: int = 5, source_filter: str | None = None) -&gt; tuple[List[Dict[str, Any]], float]</code>  <code>async</code>","text":"<p>Search the knowledge base using a pre-computed embedding.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>List[float]</code> <p>Pre-computed query embedding vector</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return</p> <code>5</code> <code>source_filter</code> <code>str | None</code> <p>Optional filter for document sources</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[List[Dict[str, Any]], float]</code> <p>Tuple of (results list, duration_ms)</p> Note <p>This function is separated from embedding generation to allow timing breakdown in LangFuse spans (AC #2: separate spans for embedding and DB search).</p> Source code in <code>core/rag_service.py</code> <pre><code>async def search_with_embedding(\n    embedding: List[float], limit: int = 5, source_filter: str | None = None\n) -&gt; tuple[List[Dict[str, Any]], float]:\n    \"\"\"\n    Search the knowledge base using a pre-computed embedding.\n\n    Args:\n        embedding: Pre-computed query embedding vector\n        limit: Maximum number of results to return\n        source_filter: Optional filter for document sources\n\n    Returns:\n        Tuple of (results list, duration_ms)\n\n    Note:\n        This function is separated from embedding generation to allow\n        timing breakdown in LangFuse spans (AC #2: separate spans for embedding and DB search).\n    \"\"\"\n    # Convert to PostgreSQL vector format\n    embedding_str = \"[\" + \",\".join(map(str, embedding)) + \"]\"\n\n    db_start = time.time()\n\n    base_query = \"\"\"\n        SELECT \n            c.id AS chunk_id,\n            c.document_id,\n            c.content,\n            1 - (c.embedding &lt;=&gt; $1::vector) AS similarity,\n            c.metadata,\n            d.title AS document_title,\n            d.source AS document_source\n        FROM chunks c\n        JOIN documents d ON c.document_id = d.id\n        WHERE c.embedding IS NOT NULL\n    \"\"\"\n\n    if source_filter:\n        sql_query = (\n            base_query + \" AND d.source ILIKE $3 ORDER BY c.embedding &lt;=&gt; $1::vector LIMIT $2\"\n        )\n        source_pattern = f\"%{source_filter}%\"\n        args = [embedding_str, limit, source_pattern]\n    else:\n        sql_query = base_query + \" ORDER BY c.embedding &lt;=&gt; $1::vector LIMIT $2\"\n        args = [embedding_str, limit]\n\n    async with global_db_pool.acquire() as conn:\n        results = await conn.fetch(sql_query, *args)\n\n    duration_ms = (time.time() - db_start) * 1000\n\n    # Format results\n    structured_results = []\n    for row in results:\n        structured_results.append(\n            {\n                \"content\": row[\"content\"],\n                \"similarity\": float(row[\"similarity\"]),\n                \"source\": row[\"document_source\"],\n                \"title\": row[\"document_title\"],\n                \"metadata\": json.loads(row[\"metadata\"])\n                if isinstance(row[\"metadata\"], str)\n                else row[\"metadata\"],\n            }\n        )\n\n    return structured_results, duration_ms\n</code></pre>"},{"location":"api-reference/core/#agent","title":"Agent","text":""},{"location":"api-reference/core/#core.agent","title":"<code>core.agent</code>","text":""},{"location":"api-reference/core/#core.agent--pydanticai-agent-definition","title":"PydanticAI Agent Definition","text":"<p>Defines the agent and its tools, wrapping the core RAG service.</p>"},{"location":"api-reference/core/#core.agent.search_knowledge_base","title":"<code>search_knowledge_base(ctx: RunContext[None], query: str, limit: int = 5, source_filter: str | None = None) -&gt; str</code>  <code>async</code>","text":"<p>Search the knowledge base using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to find relevant information</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return (default: 5)</p> <code>5</code> <code>source_filter</code> <code>str | None</code> <p>Optional filter to search only in specific documentation sources.           Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"           If provided, only documents whose source path contains this string will be searched.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted search results with source citations</p> Source code in <code>core/agent.py</code> <pre><code>async def search_knowledge_base(\n    ctx: RunContext[None], query: str, limit: int = 5, source_filter: str | None = None\n) -&gt; str:\n    \"\"\"\n    Search the knowledge base using semantic similarity.\n\n    Args:\n        query: The search query to find relevant information\n        limit: Maximum number of results to return (default: 5)\n        source_filter: Optional filter to search only in specific documentation sources.\n                      Examples: \"langfuse-docs\", \"docling\", \"langfuse-docs/deployment\"\n                      If provided, only documents whose source path contains this string will be searched.\n\n    Returns:\n        Formatted search results with source citations\n    \"\"\"\n    try:\n        # Call API\n        response = await client.search(query, limit, source_filter)\n        results = response.get(\"results\", [])\n\n        if not results:\n            filter_msg = f\" in '{source_filter}'\" if source_filter else \"\"\n            return (\n                f\"No relevant information found in the knowledge base{filter_msg} for your query.\"\n            )\n\n        # Format results for LLM consumption\n        response_parts = []\n        for row in results:\n            # Handle both dictionary access (API) and object access (if we were local)\n            # API returns dicts\n            title = row.get(\"title\", \"Unknown\")\n            content = row.get(\"content\", \"\")\n\n            response_parts.append(f\"[Source: {title}]\\n{content}\\n\")\n\n        return \"\\n---\\n\".join(response_parts)\n\n    except Exception as e:\n        logger.error(f\"Search failed: {e}\")\n        return f\"Error searching knowledge base: {str(e)}\"\n</code></pre>"},{"location":"api-reference/ingestion/","title":"Ingestion Module","text":""},{"location":"api-reference/ingestion/#ingest-pipeline","title":"Ingest Pipeline","text":""},{"location":"api-reference/ingestion/#ingestion.ingest","title":"<code>ingestion.ingest</code>","text":"<p>Main ingestion script for processing markdown documents into vector DB and knowledge graph.</p>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline","title":"<code>DocumentIngestionPipeline</code>","text":"<p>Pipeline for ingesting documents into vector DB and knowledge graph.</p> Source code in <code>ingestion/ingest.py</code> <pre><code>class DocumentIngestionPipeline:\n    \"\"\"Pipeline for ingesting documents into vector DB and knowledge graph.\"\"\"\n\n    def __init__(\n        self,\n        config: IngestionConfig,\n        documents_folder: str = \"documents\",\n        clean_before_ingest: bool = False,\n        fast_mode: bool = False,\n    ):\n        \"\"\"\n        Initialize ingestion pipeline.\n\n        Args:\n            config: Ingestion configuration\n            documents_folder: Folder containing markdown documents\n            clean_before_ingest: Whether to clean existing data before ingestion (default: False)\n            fast_mode: Whether to use fast mode (disable OCR and table structure)\n        \"\"\"\n        self.config = config\n        self.documents_folder = documents_folder\n        self.clean_before_ingest = clean_before_ingest\n        self.fast_mode = fast_mode\n\n        # Initialize components\n        self.chunker_config = ChunkingConfig(\n            chunk_size=config.chunk_size,\n            chunk_overlap=config.chunk_overlap,\n            max_chunk_size=config.max_chunk_size,\n            use_semantic_splitting=config.use_semantic_chunking,\n        )\n\n        self.chunker = create_chunker(self.chunker_config)\n        self.embedder = create_embedder()\n\n        self._initialized = False\n\n    async def initialize(self):\n        \"\"\"Initialize database connections.\"\"\"\n        if self._initialized:\n            return\n\n        logger.info(\"Initializing ingestion pipeline...\")\n\n        # Initialize database connections\n        await initialize_database()\n\n        self._initialized = True\n        logger.info(\"Ingestion pipeline initialized\")\n\n    async def close(self):\n        \"\"\"Close database connections.\"\"\"\n        if self._initialized:\n            await close_database()\n            self._initialized = False\n\n    async def ingest_documents(\n        self, progress_callback: Optional[callable] = None\n    ) -&gt; List[IngestionResult]:\n        \"\"\"\n        Ingest all documents from the documents folder.\n\n        Args:\n            progress_callback: Optional callback for progress updates\n\n        Returns:\n            List of ingestion results\n        \"\"\"\n        if not self._initialized:\n            await self.initialize()\n\n        # Clean existing data if requested\n        if self.clean_before_ingest:\n            await self._clean_databases()\n\n        # Find all supported document files\n        document_files = self._find_document_files()\n\n        if not document_files:\n            logger.warning(f\"No supported document files found in {self.documents_folder}\")\n            return []\n\n        logger.info(f\"Found {len(document_files)} document files to process\")\n\n        results = []\n\n        for i, file_path in enumerate(document_files):\n            try:\n                logger.info(f\"Processing file {i + 1}/{len(document_files)}: {file_path}\")\n\n                result = await self._ingest_single_document(file_path)\n                results.append(result)\n\n                if progress_callback:\n                    progress_callback(i + 1, len(document_files))\n\n            except Exception as e:\n                logger.error(f\"Failed to process {file_path}: {e}\")\n                results.append(\n                    IngestionResult(\n                        document_id=\"\",\n                        title=os.path.basename(file_path),\n                        chunks_created=0,\n                        entities_extracted=0,\n                        relationships_created=0,\n                        processing_time_ms=0,\n                        errors=[str(e)],\n                    )\n                )\n\n        # Log summary\n        total_chunks = sum(r.chunks_created for r in results)\n        total_errors = sum(len(r.errors) for r in results)\n\n        logger.info(\n            f\"Ingestion complete: {len(results)} documents, {total_chunks} chunks, {total_errors} errors\"\n        )\n\n        return results\n\n    async def _ingest_single_document(self, file_path: str) -&gt; IngestionResult:\n        \"\"\"\n        Ingest a single document.\n\n        Args:\n            file_path: Path to the document file\n\n        Returns:\n            Ingestion result\n        \"\"\"\n        start_time = datetime.now()\n\n        # Read document (returns tuple: content, docling_doc)\n        document_content, docling_doc = self._read_document(file_path)\n        document_title = self._extract_title(document_content, file_path)\n        document_source = os.path.relpath(file_path, self.documents_folder)\n\n        # Extract metadata from content\n        document_metadata = self._extract_document_metadata(document_content, file_path)\n\n        logger.info(f\"Processing document: {document_title}\")\n\n        # Chunk the document - pass DoclingDocument for HybridChunker\n        chunks = await self.chunker.chunk_document(\n            content=document_content,\n            title=document_title,\n            source=document_source,\n            metadata=document_metadata,\n            docling_doc=docling_doc,  # Pass DoclingDocument for HybridChunker\n        )\n\n        if not chunks:\n            logger.warning(f\"No chunks created for {document_title}\")\n            return IngestionResult(\n                document_id=\"\",\n                title=document_title,\n                chunks_created=0,\n                entities_extracted=0,\n                relationships_created=0,\n                processing_time_ms=(datetime.now() - start_time).total_seconds() * 1000,\n                errors=[\"No chunks created\"],\n            )\n\n        logger.info(f\"Created {len(chunks)} chunks\")\n\n        # Entity extraction removed (graph-related functionality)\n        entities_extracted = 0\n\n        # Generate embeddings\n        embedded_chunks = await self.embedder.embed_chunks(chunks)\n        logger.info(f\"Generated embeddings for {len(embedded_chunks)} chunks\")\n\n        # Save to PostgreSQL\n        document_id = await self._save_to_postgres(\n            document_title, document_source, document_content, embedded_chunks, document_metadata\n        )\n\n        logger.info(f\"Saved document to PostgreSQL with ID: {document_id}\")\n\n        # Knowledge graph functionality removed\n        relationships_created = 0\n        graph_errors = []\n\n        # Calculate processing time\n        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n\n        return IngestionResult(\n            document_id=document_id,\n            title=document_title,\n            chunks_created=len(chunks),\n            entities_extracted=entities_extracted,\n            relationships_created=relationships_created,\n            processing_time_ms=processing_time,\n            errors=graph_errors,\n        )\n\n    def _find_document_files(self) -&gt; List[str]:\n        \"\"\"Find all supported document files in the documents folder.\"\"\"\n        if not os.path.exists(self.documents_folder):\n            logger.error(f\"Documents folder not found: {self.documents_folder}\")\n            return []\n\n        # Supported file patterns - Docling + text formats\n        patterns = [\n            \"*.md\",\n            \"*.markdown\",\n            \"*.txt\",  # Text formats\n            \"*.pdf\",  # PDF\n            \"*.docx\",\n            \"*.doc\",  # Word\n            \"*.pptx\",\n            \"*.ppt\",  # PowerPoint\n            \"*.xlsx\",\n            \"*.xls\",  # Excel\n            \"*.html\",\n            \"*.htm\",  # HTML\n        ]\n        files = []\n\n        for pattern in patterns:\n            files.extend(\n                glob.glob(os.path.join(self.documents_folder, \"**\", pattern), recursive=True)\n            )\n\n        return sorted(files)\n\n    def _read_document(self, file_path: str) -&gt; tuple[str, Optional[Any]]:\n        \"\"\"\n        Read document content from file - supports multiple formats via Docling.\n\n        Returns:\n            Tuple of (markdown_content, docling_document)\n            docling_document is None for text files\n        \"\"\"\n        file_ext = os.path.splitext(file_path)[1].lower()\n\n        # Docling-supported formats (convert to markdown)\n        docling_formats = [\n            \".pdf\",\n            \".docx\",\n            \".doc\",\n            \".pptx\",\n            \".ppt\",\n            \".xlsx\",\n            \".xls\",\n            \".html\",\n            \".htm\",\n        ]\n\n        if file_ext in docling_formats:\n            try:\n                from docling.datamodel.base_models import InputFormat\n                from docling.datamodel.pipeline_options import PdfPipelineOptions\n                from docling.document_converter import DocumentConverter, PdfFormatOption\n\n                logger.info(\n                    f\"Converting {file_ext} file using Docling: {os.path.basename(file_path)}\"\n                )\n\n                # Configure pipeline options\n                pipeline_options = PdfPipelineOptions()\n\n                # Configure options for PDF files if in fast mode\n                if self.fast_mode and file_ext == \".pdf\":\n                    logger.info(\n                        f\"Fast mode enabled for {os.path.basename(file_path)}: Disabling OCR and heavy layout analysis\"\n                    )\n                    pipeline_options.do_ocr = False\n                    pipeline_options.do_table_structure = False\n                    # Disable picture classification/description for speed\n                    if hasattr(pipeline_options, \"do_picture_classification\"):\n                        pipeline_options.do_picture_classification = False\n                    if hasattr(pipeline_options, \"do_picture_description\"):\n                        pipeline_options.do_picture_description = False\n\n                # Configure converter with options\n                converter = DocumentConverter(\n                    format_options={\n                        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n                    }\n                )\n\n                result = converter.convert(file_path)\n\n                # Export to markdown for consistent processing\n                markdown_content = result.document.export_to_markdown()\n                logger.info(f\"Successfully converted {os.path.basename(file_path)} to markdown\")\n\n                # Return both markdown and DoclingDocument for HybridChunker\n                return (markdown_content, result.document)\n\n            except Exception as e:\n                logger.error(f\"Failed to convert {file_path} with Docling: {e}\")\n                # Fall back to raw text if Docling fails\n                logger.warning(f\"Falling back to raw text extraction for {file_path}\")\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        return (f.read(), None)\n                except Exception:\n                    return (f\"[Error: Could not read file {os.path.basename(file_path)}]\", None)\n\n        # Text-based formats (read directly)\n        else:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    return (f.read(), None)\n            except UnicodeDecodeError:\n                # Try with different encoding\n                with open(file_path, \"r\", encoding=\"latin-1\") as f:\n                    return (f.read(), None)\n\n    def _extract_title(self, content: str, file_path: str) -&gt; str:\n        \"\"\"Extract title from document content or filename.\"\"\"\n        # Try to find markdown title\n        lines = content.split(\"\\n\")\n        for line in lines[:10]:  # Check first 10 lines\n            line = line.strip()\n            if line.startswith(\"# \"):\n                return line[2:].strip()\n\n        # Fallback to filename\n        return os.path.splitext(os.path.basename(file_path))[0]\n\n    def _extract_document_metadata(self, content: str, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata from document content.\"\"\"\n        metadata = {\n            \"file_path\": file_path,\n            \"file_size\": len(content),\n            \"ingestion_date\": datetime.now().isoformat(),\n        }\n\n        # Try to extract YAML frontmatter\n        if content.startswith(\"---\"):\n            try:\n                import yaml\n\n                end_marker = content.find(\"\\n---\\n\", 4)\n                if end_marker != -1:\n                    frontmatter = content[4:end_marker]\n                    yaml_metadata = yaml.safe_load(frontmatter)\n                    if isinstance(yaml_metadata, dict):\n                        metadata.update(yaml_metadata)\n            except ImportError:\n                logger.warning(\"PyYAML not installed, skipping frontmatter extraction\")\n            except Exception as e:\n                logger.warning(f\"Failed to parse frontmatter: {e}\")\n\n        # Extract some basic metadata from content\n        lines = content.split(\"\\n\")\n        metadata[\"line_count\"] = len(lines)\n        metadata[\"word_count\"] = len(content.split())\n\n        return metadata\n\n    async def _save_to_postgres(\n        self,\n        title: str,\n        source: str,\n        content: str,\n        chunks: List[DocumentChunk],\n        metadata: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"\n        Save document and chunks to PostgreSQL.\n\n        Idempotent: If a document with the same source already exists, it will be updated\n        instead of creating a duplicate. Old chunks are deleted and new ones inserted.\n        \"\"\"\n        async with db_pool.acquire() as conn:\n            async with conn.transaction():\n                # Check if document with same source already exists\n                existing_doc = await conn.fetchrow(\n                    \"\"\"\n                    SELECT id FROM documents WHERE source = $1\n                    \"\"\",\n                    source,\n                )\n\n                if existing_doc:\n                    # Document exists: update it and replace chunks\n                    document_id = existing_doc[\"id\"]\n                    logger.info(\n                        f\"Document with source '{source}' already exists (ID: {document_id}), updating...\"\n                    )\n\n                    # Update document\n                    await conn.execute(\n                        \"\"\"\n                        UPDATE documents\n                        SET title = $1, content = $2, metadata = $3\n                        WHERE id = $4\n                        \"\"\",\n                        title,\n                        content,\n                        json.dumps(metadata),\n                        document_id,\n                    )\n\n                    # Delete old chunks\n                    await conn.execute(\n                        \"\"\"\n                        DELETE FROM chunks WHERE document_id = $1\n                        \"\"\",\n                        document_id,\n                    )\n                    logger.info(f\"Deleted old chunks for document {document_id}\")\n                else:\n                    # Document doesn't exist: insert new one\n                    document_result = await conn.fetchrow(\n                        \"\"\"\n                        INSERT INTO documents (title, source, content, metadata)\n                        VALUES ($1, $2, $3, $4)\n                        RETURNING id::text\n                        \"\"\",\n                        title,\n                        source,\n                        content,\n                        json.dumps(metadata),\n                    )\n                    document_id = document_result[\"id\"]\n                    logger.info(f\"Created new document with ID: {document_id}\")\n\n                # Insert chunks (same for both update and insert cases)\n                for chunk in chunks:\n                    # Convert embedding to PostgreSQL vector string format\n                    embedding_data = None\n                    if hasattr(chunk, \"embedding\") and chunk.embedding:\n                        # PostgreSQL vector format: '[1.0,2.0,3.0]' (no spaces after commas)\n                        embedding_data = \"[\" + \",\".join(map(str, chunk.embedding)) + \"]\"\n\n                    await conn.execute(\n                        \"\"\"\n                        INSERT INTO chunks (document_id, content, embedding, chunk_index, metadata, token_count)\n                        VALUES ($1::uuid, $2, $3::vector, $4, $5, $6)\n                        \"\"\",\n                        document_id,\n                        chunk.content,\n                        embedding_data,\n                        chunk.index,\n                        json.dumps(chunk.metadata),\n                        chunk.token_count,\n                    )\n\n                return document_id\n\n    async def _clean_databases(self):\n        \"\"\"Clean existing data from databases.\"\"\"\n        logger.warning(\"Cleaning existing data from databases...\")\n\n        # Clean PostgreSQL\n        async with db_pool.acquire() as conn:\n            async with conn.transaction():\n                await conn.execute(\"DELETE FROM chunks\")\n                await conn.execute(\"DELETE FROM documents\")\n\n        logger.info(\"Cleaned PostgreSQL database\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.__init__","title":"<code>__init__(config: IngestionConfig, documents_folder: str = 'documents', clean_before_ingest: bool = False, fast_mode: bool = False)</code>","text":"<p>Initialize ingestion pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>IngestionConfig</code> <p>Ingestion configuration</p> required <code>documents_folder</code> <code>str</code> <p>Folder containing markdown documents</p> <code>'documents'</code> <code>clean_before_ingest</code> <code>bool</code> <p>Whether to clean existing data before ingestion (default: False)</p> <code>False</code> <code>fast_mode</code> <code>bool</code> <p>Whether to use fast mode (disable OCR and table structure)</p> <code>False</code> Source code in <code>ingestion/ingest.py</code> <pre><code>def __init__(\n    self,\n    config: IngestionConfig,\n    documents_folder: str = \"documents\",\n    clean_before_ingest: bool = False,\n    fast_mode: bool = False,\n):\n    \"\"\"\n    Initialize ingestion pipeline.\n\n    Args:\n        config: Ingestion configuration\n        documents_folder: Folder containing markdown documents\n        clean_before_ingest: Whether to clean existing data before ingestion (default: False)\n        fast_mode: Whether to use fast mode (disable OCR and table structure)\n    \"\"\"\n    self.config = config\n    self.documents_folder = documents_folder\n    self.clean_before_ingest = clean_before_ingest\n    self.fast_mode = fast_mode\n\n    # Initialize components\n    self.chunker_config = ChunkingConfig(\n        chunk_size=config.chunk_size,\n        chunk_overlap=config.chunk_overlap,\n        max_chunk_size=config.max_chunk_size,\n        use_semantic_splitting=config.use_semantic_chunking,\n    )\n\n    self.chunker = create_chunker(self.chunker_config)\n    self.embedder = create_embedder()\n\n    self._initialized = False\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close database connections.</p> Source code in <code>ingestion/ingest.py</code> <pre><code>async def close(self):\n    \"\"\"Close database connections.\"\"\"\n    if self._initialized:\n        await close_database()\n        self._initialized = False\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.ingest_documents","title":"<code>ingest_documents(progress_callback: Optional[callable] = None) -&gt; List[IngestionResult]</code>  <code>async</code>","text":"<p>Ingest all documents from the documents folder.</p> <p>Parameters:</p> Name Type Description Default <code>progress_callback</code> <code>Optional[callable]</code> <p>Optional callback for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>List[IngestionResult]</code> <p>List of ingestion results</p> Source code in <code>ingestion/ingest.py</code> <pre><code>async def ingest_documents(\n    self, progress_callback: Optional[callable] = None\n) -&gt; List[IngestionResult]:\n    \"\"\"\n    Ingest all documents from the documents folder.\n\n    Args:\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        List of ingestion results\n    \"\"\"\n    if not self._initialized:\n        await self.initialize()\n\n    # Clean existing data if requested\n    if self.clean_before_ingest:\n        await self._clean_databases()\n\n    # Find all supported document files\n    document_files = self._find_document_files()\n\n    if not document_files:\n        logger.warning(f\"No supported document files found in {self.documents_folder}\")\n        return []\n\n    logger.info(f\"Found {len(document_files)} document files to process\")\n\n    results = []\n\n    for i, file_path in enumerate(document_files):\n        try:\n            logger.info(f\"Processing file {i + 1}/{len(document_files)}: {file_path}\")\n\n            result = await self._ingest_single_document(file_path)\n            results.append(result)\n\n            if progress_callback:\n                progress_callback(i + 1, len(document_files))\n\n        except Exception as e:\n            logger.error(f\"Failed to process {file_path}: {e}\")\n            results.append(\n                IngestionResult(\n                    document_id=\"\",\n                    title=os.path.basename(file_path),\n                    chunks_created=0,\n                    entities_extracted=0,\n                    relationships_created=0,\n                    processing_time_ms=0,\n                    errors=[str(e)],\n                )\n            )\n\n    # Log summary\n    total_chunks = sum(r.chunks_created for r in results)\n    total_errors = sum(len(r.errors) for r in results)\n\n    logger.info(\n        f\"Ingestion complete: {len(results)} documents, {total_chunks} chunks, {total_errors} errors\"\n    )\n\n    return results\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.DocumentIngestionPipeline.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize database connections.</p> Source code in <code>ingestion/ingest.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize database connections.\"\"\"\n    if self._initialized:\n        return\n\n    logger.info(\"Initializing ingestion pipeline...\")\n\n    # Initialize database connections\n    await initialize_database()\n\n    self._initialized = True\n    logger.info(\"Ingestion pipeline initialized\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.ingest.main","title":"<code>main()</code>  <code>async</code>","text":"<p>Main function for running ingestion.</p> Source code in <code>ingestion/ingest.py</code> <pre><code>async def main():\n    \"\"\"Main function for running ingestion.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Ingest documents into vector DB\")\n    parser.add_argument(\"--documents\", \"-d\", default=\"documents\", help=\"Documents folder path\")\n    parser.add_argument(\n        \"--clean\",\n        action=\"store_true\",\n        help=\"Clean existing data before ingestion (default: incremental update, no clean)\",\n    )\n    parser.add_argument(\n        \"--chunk-size\", type=int, default=1000, help=\"Chunk size for splitting documents\"\n    )\n    parser.add_argument(\"--chunk-overlap\", type=int, default=200, help=\"Chunk overlap size\")\n    parser.add_argument(\"--no-semantic\", action=\"store_true\", help=\"Disable semantic chunking\")\n    parser.add_argument(\n        \"--fast\",\n        action=\"store_true\",\n        help=\"Enable fast mode (disable OCR and table structure recognition)\",\n    )\n    # Graph-related arguments removed\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose logging\")\n\n    args = parser.parse_args()\n\n    # Configure logging\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    logging.basicConfig(\n        level=log_level, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n\n    # Create ingestion configuration\n    config = IngestionConfig(\n        chunk_size=args.chunk_size,\n        chunk_overlap=args.chunk_overlap,\n        use_semantic_chunking=not args.no_semantic,\n    )\n\n    # Create and run pipeline - incremental by default (no clean) unless --clean is specified\n    pipeline = DocumentIngestionPipeline(\n        config=config,\n        documents_folder=args.documents,\n        clean_before_ingest=args.clean,  # Only clean if explicitly requested\n        fast_mode=args.fast,\n    )\n\n    def progress_callback(current: int, total: int):\n        print(f\"Progress: {current}/{total} documents processed\")\n\n    try:\n        start_time = datetime.now()\n\n        results = await pipeline.ingest_documents(progress_callback)\n\n        end_time = datetime.now()\n        total_time = (end_time - start_time).total_seconds()\n\n        # Print summary\n        print(\"\\n\" + \"=\" * 50)\n        print(\"INGESTION SUMMARY\")\n        print(\"=\" * 50)\n        print(f\"Documents processed: {len(results)}\")\n        print(f\"Total chunks created: {sum(r.chunks_created for r in results)}\")\n        # Graph-related stats removed\n        print(f\"Total errors: {sum(len(r.errors) for r in results)}\")\n        print(f\"Total processing time: {total_time:.2f} seconds\")\n        print()\n\n        # Print individual results\n        for result in results:\n            status = \"\u2713\" if not result.errors else \"\u2717\"\n            print(f\"{status} {result.title}: {result.chunks_created} chunks\")\n\n            if result.errors:\n                for error in result.errors:\n                    print(f\"  Error: {error}\")\n\n    except KeyboardInterrupt:\n        print(\"\\nIngestion interrupted by user\")\n    except Exception as e:\n        logger.error(f\"Ingestion failed: {e}\")\n        raise\n    finally:\n        await pipeline.close()\n</code></pre>"},{"location":"api-reference/ingestion/#chunker","title":"Chunker","text":""},{"location":"api-reference/ingestion/#ingestion.chunker","title":"<code>ingestion.chunker</code>","text":"<p>Docling HybridChunker implementation for intelligent document splitting.</p> <p>This module uses Docling's built-in HybridChunker which combines: - Token-aware chunking (uses actual tokenizer) - Document structure preservation (headings, sections, tables) - Semantic boundary respect (paragraphs, code blocks) - Contextualized output (chunks include heading hierarchy)</p> <p>Benefits over custom chunking: - Fast (no LLM API calls) - Token-precise (not character-based estimates) - Better for RAG (chunks include document context) - Battle-tested (maintained by Docling team)</p>"},{"location":"api-reference/ingestion/#ingestion.chunker.ChunkingConfig","title":"<code>ChunkingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for chunking.</p> Source code in <code>ingestion/chunker.py</code> <pre><code>@dataclass\nclass ChunkingConfig:\n    \"\"\"Configuration for chunking.\"\"\"\n\n    chunk_size: int = 1000  # Target characters per chunk\n    chunk_overlap: int = 200  # Character overlap between chunks\n    max_chunk_size: int = 2000  # Maximum chunk size\n    min_chunk_size: int = 100  # Minimum chunk size\n    use_semantic_splitting: bool = True  # Use HybridChunker (recommended)\n    preserve_structure: bool = True  # Preserve document structure\n    max_tokens: int = 512  # Maximum tokens for embedding models\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        if self.chunk_overlap &gt;= self.chunk_size:\n            raise ValueError(\"Chunk overlap must be less than chunk size\")\n        if self.min_chunk_size &lt;= 0:\n            raise ValueError(\"Minimum chunk size must be positive\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.ChunkingConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration.</p> Source code in <code>ingestion/chunker.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration.\"\"\"\n    if self.chunk_overlap &gt;= self.chunk_size:\n        raise ValueError(\"Chunk overlap must be less than chunk size\")\n    if self.min_chunk_size &lt;= 0:\n        raise ValueError(\"Minimum chunk size must be positive\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker","title":"<code>DoclingHybridChunker</code>","text":"<p>Docling HybridChunker wrapper for intelligent document splitting.</p> <p>This chunker uses Docling's built-in HybridChunker which: - Respects document structure (sections, paragraphs, tables) - Is token-aware (fits embedding model limits) - Preserves semantic coherence - Includes heading context in chunks</p> Source code in <code>ingestion/chunker.py</code> <pre><code>class DoclingHybridChunker:\n    \"\"\"\n    Docling HybridChunker wrapper for intelligent document splitting.\n\n    This chunker uses Docling's built-in HybridChunker which:\n    - Respects document structure (sections, paragraphs, tables)\n    - Is token-aware (fits embedding model limits)\n    - Preserves semantic coherence\n    - Includes heading context in chunks\n    \"\"\"\n\n    def __init__(self, config: ChunkingConfig):\n        \"\"\"\n        Initialize chunker.\n\n        Args:\n            config: Chunking configuration\n        \"\"\"\n        self.config = config\n\n        # Initialize tokenizer for token-aware chunking\n        model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n        logger.info(f\"Initializing tokenizer: {model_id}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        # Create HybridChunker\n        self.chunker = HybridChunker(\n            tokenizer=self.tokenizer,\n            max_tokens=config.max_tokens,\n            merge_peers=True,  # Merge small adjacent chunks\n        )\n\n        logger.info(f\"HybridChunker initialized (max_tokens={config.max_tokens})\")\n\n    async def chunk_document(\n        self,\n        content: str,\n        title: str,\n        source: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        docling_doc: Optional[DoclingDocument] = None,\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Chunk a document using Docling's HybridChunker.\n\n        Args:\n            content: Document content (markdown format)\n            title: Document title\n            source: Document source\n            metadata: Additional metadata\n            docling_doc: Optional pre-converted DoclingDocument (for efficiency)\n\n        Returns:\n            List of document chunks with contextualized content\n        \"\"\"\n        if not content.strip():\n            return []\n\n        base_metadata = {\n            \"title\": title,\n            \"source\": source,\n            \"chunk_method\": \"hybrid\",\n            **(metadata or {}),\n        }\n\n        # If we don't have a DoclingDocument, we need to create one from markdown\n        if docling_doc is None:\n            # For markdown content, we need to convert it to DoclingDocument\n            # This is a simplified version - in practice, content comes from\n            # Docling's document converter in the ingestion pipeline\n            logger.warning(\"No DoclingDocument provided, using simple chunking fallback\")\n            return self._simple_fallback_chunk(content, base_metadata)\n\n        try:\n            # Use HybridChunker to chunk the DoclingDocument\n            chunk_iter = self.chunker.chunk(dl_doc=docling_doc)\n            chunks = list(chunk_iter)\n\n            # Convert Docling chunks to DocumentChunk objects\n            document_chunks = []\n            current_pos = 0\n\n            for i, chunk in enumerate(chunks):\n                # Get contextualized text (includes heading hierarchy)\n                contextualized_text = self.chunker.contextualize(chunk=chunk)\n\n                # Count actual tokens\n                token_count = len(self.tokenizer.encode(contextualized_text))\n\n                # Create chunk metadata\n                chunk_metadata = {\n                    **base_metadata,\n                    \"total_chunks\": len(chunks),\n                    \"token_count\": token_count,\n                    \"has_context\": True,  # Flag indicating contextualized chunk\n                }\n\n                # Estimate character positions\n                start_char = current_pos\n                end_char = start_char + len(contextualized_text)\n\n                document_chunks.append(\n                    DocumentChunk(\n                        content=contextualized_text.strip(),\n                        index=i,\n                        start_char=start_char,\n                        end_char=end_char,\n                        metadata=chunk_metadata,\n                        token_count=token_count,\n                    )\n                )\n\n                current_pos = end_char\n\n            logger.info(f\"Created {len(document_chunks)} chunks using HybridChunker\")\n            return document_chunks\n\n        except Exception as e:\n            logger.error(f\"HybridChunker failed: {e}, falling back to simple chunking\")\n            return self._simple_fallback_chunk(content, base_metadata)\n\n    def _simple_fallback_chunk(\n        self, content: str, base_metadata: Dict[str, Any]\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Simple fallback chunking when HybridChunker can't be used.\n\n        This is used when:\n        - No DoclingDocument is provided\n        - HybridChunker fails\n\n        Args:\n            content: Content to chunk\n            base_metadata: Base metadata for chunks\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        chunks = []\n        chunk_size = self.config.chunk_size\n        overlap = self.config.chunk_overlap\n\n        # Simple sliding window approach\n        start = 0\n        chunk_index = 0\n\n        while start &lt; len(content):\n            end = start + chunk_size\n\n            if end &gt;= len(content):\n                # Last chunk\n                chunk_text = content[start:]\n            else:\n                # Try to end at sentence boundary\n                chunk_end = end\n                for i in range(end, max(start + self.config.min_chunk_size, end - 200), -1):\n                    if i &lt; len(content) and content[i] in \".!?\\n\":\n                        chunk_end = i + 1\n                        break\n                chunk_text = content[start:chunk_end]\n                end = chunk_end\n\n            if chunk_text.strip():\n                token_count = len(self.tokenizer.encode(chunk_text))\n\n                chunks.append(\n                    DocumentChunk(\n                        content=chunk_text.strip(),\n                        index=chunk_index,\n                        start_char=start,\n                        end_char=end,\n                        metadata={\n                            **base_metadata,\n                            \"chunk_method\": \"simple_fallback\",\n                            \"total_chunks\": -1,  # Will update after\n                        },\n                        token_count=token_count,\n                    )\n                )\n\n                chunk_index += 1\n\n            # Move forward with overlap\n            start = end - overlap\n\n        # Update total chunks\n        for chunk in chunks:\n            chunk.metadata[\"total_chunks\"] = len(chunks)\n\n        logger.info(f\"Created {len(chunks)} chunks using simple fallback\")\n        return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker.__init__","title":"<code>__init__(config: ChunkingConfig)</code>","text":"<p>Initialize chunker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChunkingConfig</code> <p>Chunking configuration</p> required Source code in <code>ingestion/chunker.py</code> <pre><code>def __init__(self, config: ChunkingConfig):\n    \"\"\"\n    Initialize chunker.\n\n    Args:\n        config: Chunking configuration\n    \"\"\"\n    self.config = config\n\n    # Initialize tokenizer for token-aware chunking\n    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n    logger.info(f\"Initializing tokenizer: {model_id}\")\n    self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    # Create HybridChunker\n    self.chunker = HybridChunker(\n        tokenizer=self.tokenizer,\n        max_tokens=config.max_tokens,\n        merge_peers=True,  # Merge small adjacent chunks\n    )\n\n    logger.info(f\"HybridChunker initialized (max_tokens={config.max_tokens})\")\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DoclingHybridChunker.chunk_document","title":"<code>chunk_document(content: str, title: str, source: str, metadata: Optional[Dict[str, Any]] = None, docling_doc: Optional[DoclingDocument] = None) -&gt; List[DocumentChunk]</code>  <code>async</code>","text":"<p>Chunk a document using Docling's HybridChunker.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Document content (markdown format)</p> required <code>title</code> <code>str</code> <p>Document title</p> required <code>source</code> <code>str</code> <p>Document source</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata</p> <code>None</code> <code>docling_doc</code> <code>Optional[DoclingDocument]</code> <p>Optional pre-converted DoclingDocument (for efficiency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DocumentChunk]</code> <p>List of document chunks with contextualized content</p> Source code in <code>ingestion/chunker.py</code> <pre><code>async def chunk_document(\n    self,\n    content: str,\n    title: str,\n    source: str,\n    metadata: Optional[Dict[str, Any]] = None,\n    docling_doc: Optional[DoclingDocument] = None,\n) -&gt; List[DocumentChunk]:\n    \"\"\"\n    Chunk a document using Docling's HybridChunker.\n\n    Args:\n        content: Document content (markdown format)\n        title: Document title\n        source: Document source\n        metadata: Additional metadata\n        docling_doc: Optional pre-converted DoclingDocument (for efficiency)\n\n    Returns:\n        List of document chunks with contextualized content\n    \"\"\"\n    if not content.strip():\n        return []\n\n    base_metadata = {\n        \"title\": title,\n        \"source\": source,\n        \"chunk_method\": \"hybrid\",\n        **(metadata or {}),\n    }\n\n    # If we don't have a DoclingDocument, we need to create one from markdown\n    if docling_doc is None:\n        # For markdown content, we need to convert it to DoclingDocument\n        # This is a simplified version - in practice, content comes from\n        # Docling's document converter in the ingestion pipeline\n        logger.warning(\"No DoclingDocument provided, using simple chunking fallback\")\n        return self._simple_fallback_chunk(content, base_metadata)\n\n    try:\n        # Use HybridChunker to chunk the DoclingDocument\n        chunk_iter = self.chunker.chunk(dl_doc=docling_doc)\n        chunks = list(chunk_iter)\n\n        # Convert Docling chunks to DocumentChunk objects\n        document_chunks = []\n        current_pos = 0\n\n        for i, chunk in enumerate(chunks):\n            # Get contextualized text (includes heading hierarchy)\n            contextualized_text = self.chunker.contextualize(chunk=chunk)\n\n            # Count actual tokens\n            token_count = len(self.tokenizer.encode(contextualized_text))\n\n            # Create chunk metadata\n            chunk_metadata = {\n                **base_metadata,\n                \"total_chunks\": len(chunks),\n                \"token_count\": token_count,\n                \"has_context\": True,  # Flag indicating contextualized chunk\n            }\n\n            # Estimate character positions\n            start_char = current_pos\n            end_char = start_char + len(contextualized_text)\n\n            document_chunks.append(\n                DocumentChunk(\n                    content=contextualized_text.strip(),\n                    index=i,\n                    start_char=start_char,\n                    end_char=end_char,\n                    metadata=chunk_metadata,\n                    token_count=token_count,\n                )\n            )\n\n            current_pos = end_char\n\n        logger.info(f\"Created {len(document_chunks)} chunks using HybridChunker\")\n        return document_chunks\n\n    except Exception as e:\n        logger.error(f\"HybridChunker failed: {e}, falling back to simple chunking\")\n        return self._simple_fallback_chunk(content, base_metadata)\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DocumentChunk","title":"<code>DocumentChunk</code>  <code>dataclass</code>","text":"<p>Represents a document chunk with optional embedding.</p> Source code in <code>ingestion/chunker.py</code> <pre><code>@dataclass\nclass DocumentChunk:\n    \"\"\"Represents a document chunk with optional embedding.\"\"\"\n\n    content: str\n    index: int\n    start_char: int\n    end_char: int\n    metadata: Dict[str, Any]\n    token_count: Optional[int] = None\n    embedding: Optional[List[float]] = None  # For embedder compatibility\n\n    def __post_init__(self):\n        \"\"\"Calculate token count if not provided.\"\"\"\n        if self.token_count is None:\n            # Rough estimation: ~4 characters per token\n            self.token_count = len(self.content) // 4\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.DocumentChunk.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Calculate token count if not provided.</p> Source code in <code>ingestion/chunker.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Calculate token count if not provided.\"\"\"\n    if self.token_count is None:\n        # Rough estimation: ~4 characters per token\n        self.token_count = len(self.content) // 4\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker","title":"<code>SimpleChunker</code>","text":"<p>Simple non-semantic chunker for faster processing without Docling.</p> <p>This is kept as a lightweight alternative when: - Speed is critical - Document structure is simple - Token precision is not required</p> Source code in <code>ingestion/chunker.py</code> <pre><code>class SimpleChunker:\n    \"\"\"\n    Simple non-semantic chunker for faster processing without Docling.\n\n    This is kept as a lightweight alternative when:\n    - Speed is critical\n    - Document structure is simple\n    - Token precision is not required\n    \"\"\"\n\n    def __init__(self, config: ChunkingConfig):\n        \"\"\"Initialize simple chunker.\"\"\"\n        self.config = config\n\n    async def chunk_document(\n        self,\n        content: str,\n        title: str,\n        source: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        **kwargs,  # Ignore extra args like docling_doc\n    ) -&gt; List[DocumentChunk]:\n        \"\"\"\n        Chunk document using simple paragraph-based rules.\n\n        Args:\n            content: Document content\n            title: Document title\n            source: Document source\n            metadata: Additional metadata\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        if not content.strip():\n            return []\n\n        base_metadata = {\n            \"title\": title,\n            \"source\": source,\n            \"chunk_method\": \"simple\",\n            **(metadata or {}),\n        }\n\n        # Split on double newlines (paragraphs)\n        import re\n\n        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n\n        chunks = []\n        current_chunk = \"\"\n        current_pos = 0\n        chunk_index = 0\n\n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n\n            # Check if adding this paragraph exceeds chunk size\n            potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n\n            if len(potential_chunk) &lt;= self.config.chunk_size:\n                current_chunk = potential_chunk\n            else:\n                # Save current chunk if it exists\n                if current_chunk:\n                    chunks.append(\n                        self._create_chunk(\n                            current_chunk,\n                            chunk_index,\n                            current_pos,\n                            current_pos + len(current_chunk),\n                            base_metadata.copy(),\n                        )\n                    )\n\n                    current_pos += len(current_chunk)\n                    chunk_index += 1\n\n                # Start new chunk with current paragraph\n                current_chunk = paragraph\n\n        # Add final chunk\n        if current_chunk:\n            chunks.append(\n                self._create_chunk(\n                    current_chunk,\n                    chunk_index,\n                    current_pos,\n                    current_pos + len(current_chunk),\n                    base_metadata.copy(),\n                )\n            )\n\n        # Update total chunks in metadata\n        for chunk in chunks:\n            chunk.metadata[\"total_chunks\"] = len(chunks)\n\n        return chunks\n\n    def _create_chunk(\n        self, content: str, index: int, start_pos: int, end_pos: int, metadata: Dict[str, Any]\n    ) -&gt; DocumentChunk:\n        \"\"\"Create a DocumentChunk object.\"\"\"\n        return DocumentChunk(\n            content=content.strip(),\n            index=index,\n            start_char=start_pos,\n            end_char=end_pos,\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker.__init__","title":"<code>__init__(config: ChunkingConfig)</code>","text":"<p>Initialize simple chunker.</p> Source code in <code>ingestion/chunker.py</code> <pre><code>def __init__(self, config: ChunkingConfig):\n    \"\"\"Initialize simple chunker.\"\"\"\n    self.config = config\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.SimpleChunker.chunk_document","title":"<code>chunk_document(content: str, title: str, source: str, metadata: Optional[Dict[str, Any]] = None, **kwargs) -&gt; List[DocumentChunk]</code>  <code>async</code>","text":"<p>Chunk document using simple paragraph-based rules.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Document content</p> required <code>title</code> <code>str</code> <p>Document title</p> required <code>source</code> <code>str</code> <p>Document source</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DocumentChunk]</code> <p>List of document chunks</p> Source code in <code>ingestion/chunker.py</code> <pre><code>async def chunk_document(\n    self,\n    content: str,\n    title: str,\n    source: str,\n    metadata: Optional[Dict[str, Any]] = None,\n    **kwargs,  # Ignore extra args like docling_doc\n) -&gt; List[DocumentChunk]:\n    \"\"\"\n    Chunk document using simple paragraph-based rules.\n\n    Args:\n        content: Document content\n        title: Document title\n        source: Document source\n        metadata: Additional metadata\n\n    Returns:\n        List of document chunks\n    \"\"\"\n    if not content.strip():\n        return []\n\n    base_metadata = {\n        \"title\": title,\n        \"source\": source,\n        \"chunk_method\": \"simple\",\n        **(metadata or {}),\n    }\n\n    # Split on double newlines (paragraphs)\n    import re\n\n    paragraphs = re.split(r\"\\n\\s*\\n\", content)\n\n    chunks = []\n    current_chunk = \"\"\n    current_pos = 0\n    chunk_index = 0\n\n    for paragraph in paragraphs:\n        paragraph = paragraph.strip()\n        if not paragraph:\n            continue\n\n        # Check if adding this paragraph exceeds chunk size\n        potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n\n        if len(potential_chunk) &lt;= self.config.chunk_size:\n            current_chunk = potential_chunk\n        else:\n            # Save current chunk if it exists\n            if current_chunk:\n                chunks.append(\n                    self._create_chunk(\n                        current_chunk,\n                        chunk_index,\n                        current_pos,\n                        current_pos + len(current_chunk),\n                        base_metadata.copy(),\n                    )\n                )\n\n                current_pos += len(current_chunk)\n                chunk_index += 1\n\n            # Start new chunk with current paragraph\n            current_chunk = paragraph\n\n    # Add final chunk\n    if current_chunk:\n        chunks.append(\n            self._create_chunk(\n                current_chunk,\n                chunk_index,\n                current_pos,\n                current_pos + len(current_chunk),\n                base_metadata.copy(),\n            )\n        )\n\n    # Update total chunks in metadata\n    for chunk in chunks:\n        chunk.metadata[\"total_chunks\"] = len(chunks)\n\n    return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.chunker.create_chunker","title":"<code>create_chunker(config: ChunkingConfig)</code>","text":"<p>Create appropriate chunker based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChunkingConfig</code> <p>Chunking configuration</p> required <p>Returns:</p> Type Description <p>Chunker instance</p> Source code in <code>ingestion/chunker.py</code> <pre><code>def create_chunker(config: ChunkingConfig):\n    \"\"\"\n    Create appropriate chunker based on configuration.\n\n    Args:\n        config: Chunking configuration\n\n    Returns:\n        Chunker instance\n    \"\"\"\n    if config.use_semantic_splitting:\n        return DoclingHybridChunker(config)\n    else:\n        return SimpleChunker(config)\n</code></pre>"},{"location":"api-reference/ingestion/#embedder","title":"Embedder","text":""},{"location":"api-reference/ingestion/#ingestion.embedder","title":"<code>ingestion.embedder</code>","text":""},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder","title":"<code>BaseEmbedder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for embedding providers.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>class BaseEmbedder(ABC):\n    \"\"\"Abstract base class for embedding providers.\"\"\"\n\n    @abstractmethod\n    async def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query string.\"\"\"\n        pass\n\n    @abstractmethod\n    async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of texts.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder.embed_documents","title":"<code>embed_documents(texts: List[str]) -&gt; List[List[float]]</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Embed a list of texts.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>@abstractmethod\nasync def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Embed a list of texts.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.BaseEmbedder.embed_query","title":"<code>embed_query(text: str) -&gt; List[float]</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Embed a single query string.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>@abstractmethod\nasync def embed_query(self, text: str) -&gt; List[float]:\n    \"\"\"Embed a single query string.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingCache","title":"<code>EmbeddingCache</code>","text":"<p>Simple in-memory cache for embeddings.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>class EmbeddingCache:\n    \"\"\"Simple in-memory cache for embeddings.\"\"\"\n\n    def __init__(self, max_size: int = 1000):\n        self.cache: Dict[str, List[float]] = {}\n        self.max_size = max_size\n\n    def get(self, text: str) -&gt; Optional[List[float]]:\n        return self.cache.get(text)\n\n    def set(self, text: str, embedding: List[float]):\n        if len(self.cache) &gt;= self.max_size:\n            # Simple eviction: remove first key (FIFO-ish)\n            try:\n                first_key = next(iter(self.cache))\n                del self.cache[first_key]\n            except StopIteration:\n                pass\n        self.cache[text] = embedding\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator","title":"<code>EmbeddingGenerator</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Generates embeddings using OpenAI compatible API.</p> Cost Tracking <p>Uses langfuse.openai wrapper when available for automatic cost tracking. Falls back to direct OpenAI client if LangFuse unavailable.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>class EmbeddingGenerator(BaseEmbedder):\n    \"\"\"\n    Generates embeddings using OpenAI compatible API.\n\n    Cost Tracking:\n        Uses langfuse.openai wrapper when available for automatic cost tracking.\n        Falls back to direct OpenAI client if LangFuse unavailable.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"text-embedding-3-small\",\n        batch_size: int = 100,\n        use_cache: bool = True,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.use_cache = use_cache\n        self.cost_tracking_enabled = _langfuse_openai_available\n\n        # Use provided config or fallback to env vars/provider config\n        provider_config = get_provider_config()\n        self.api_key = api_key or provider_config.api_key\n        self.base_url = base_url or provider_config.base_url\n\n        # Initialize OpenAI client (LangFuse wrapper if available for cost tracking)\n        self.client = LangfuseAsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n\n        if self.use_cache:\n            self.cache = EmbeddingCache()\n        else:\n            self.cache = None\n\n        cost_status = \"enabled\" if self.cost_tracking_enabled else \"disabled\"\n        logger.info(\n            f\"Initialized EmbeddingGenerator with model={self.model_name}, cost_tracking={cost_status}\"\n        )\n\n    async def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query string.\"\"\"\n        # Check cache first\n        if self.use_cache and self.cache:\n            cached = self.cache.get(text)\n            if cached:\n                return cached\n\n        try:\n            embedding = await self._generate_single_embedding(text)\n\n            if self.use_cache and self.cache:\n                self.cache.set(text, embedding)\n\n            return embedding\n        except Exception as e:\n            logger.error(f\"Failed to embed query: {e}\")\n            raise\n\n    async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of texts (chunks).\"\"\"\n        all_embeddings: List[List[float]] = []\n\n        # Process in batches\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i : i + self.batch_size]\n\n            # Check cache for each item in batch\n            batch_embeddings: List[List[float] | None] = [None] * len(batch)\n            indices_to_fetch = []\n            texts_to_fetch = []\n\n            if self.use_cache and self.cache:\n                for j, text in enumerate(batch):\n                    cached = self.cache.get(text)\n                    if cached:\n                        batch_embeddings[j] = cached\n                    else:\n                        indices_to_fetch.append(j)\n                        texts_to_fetch.append(text)\n            else:\n                indices_to_fetch = list(range(len(batch)))\n                texts_to_fetch = batch\n\n            # Fetch missing embeddings\n            if texts_to_fetch:\n                try:\n                    fetched_embeddings = await self._generate_batch_embeddings(texts_to_fetch)\n\n                    # Fill back into batch_embeddings and update cache\n                    for idx, embedding in zip(indices_to_fetch, fetched_embeddings):\n                        batch_embeddings[idx] = embedding\n                        if self.use_cache and self.cache:\n                            self.cache.set(batch[idx], embedding)\n\n                except Exception as e:\n                    logger.error(f\"Failed to embed batch: {e}\")\n                    raise\n\n            # Filter out Nones (should not happen if logic is correct)\n            valid_embeddings = [e for e in batch_embeddings if e is not None]\n            all_embeddings.extend(valid_embeddings)\n\n        return all_embeddings\n\n    async def embed_chunks(\n        self,\n        # Typed as Any to avoid circular import with chunker.DocumentChunk\n        chunks: List[Any],\n        progress_callback: Optional[Callable] = None,\n    ) -&gt; List[Any]:\n        \"\"\"\n        Generate embeddings for document chunks.\n        Kept for backward compatibility with ingest.py\n        \"\"\"\n        if not chunks:\n            return chunks\n\n        logger.info(f\"Generating embeddings for {len(chunks)} chunks\")\n\n        # Extract texts\n        texts = [chunk.content for chunk in chunks]\n\n        # Generate all embeddings\n        embeddings = await self.embed_documents(texts)\n\n        # Assign back to chunks\n        for i, chunk in enumerate(chunks):\n            chunk.embedding = embeddings[i]\n            if chunk.metadata:\n                chunk.metadata[\"embedding_model\"] = self.model_name\n                chunk.metadata[\"embedding_generated_at\"] = datetime.now().isoformat()\n\n        return chunks\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def _generate_single_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Generate embedding for a single text with retry logic.\"\"\"\n        response = await self.client.embeddings.create(\n            model=self.model_name, input=text, encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def _generate_batch_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Generate embeddings for a batch of texts with retry logic.\"\"\"\n        # Filter empty strings to avoid API errors\n        processed_texts = [t if t.strip() else \" \" for t in texts]\n\n        response = await self.client.embeddings.create(\n            model=self.model_name, input=processed_texts, encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_chunks","title":"<code>embed_chunks(chunks: List[Any], progress_callback: Optional[Callable] = None) -&gt; List[Any]</code>  <code>async</code>","text":"<p>Generate embeddings for document chunks. Kept for backward compatibility with ingest.py</p> Source code in <code>ingestion/embedder.py</code> <pre><code>async def embed_chunks(\n    self,\n    # Typed as Any to avoid circular import with chunker.DocumentChunk\n    chunks: List[Any],\n    progress_callback: Optional[Callable] = None,\n) -&gt; List[Any]:\n    \"\"\"\n    Generate embeddings for document chunks.\n    Kept for backward compatibility with ingest.py\n    \"\"\"\n    if not chunks:\n        return chunks\n\n    logger.info(f\"Generating embeddings for {len(chunks)} chunks\")\n\n    # Extract texts\n    texts = [chunk.content for chunk in chunks]\n\n    # Generate all embeddings\n    embeddings = await self.embed_documents(texts)\n\n    # Assign back to chunks\n    for i, chunk in enumerate(chunks):\n        chunk.embedding = embeddings[i]\n        if chunk.metadata:\n            chunk.metadata[\"embedding_model\"] = self.model_name\n            chunk.metadata[\"embedding_generated_at\"] = datetime.now().isoformat()\n\n    return chunks\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_documents","title":"<code>embed_documents(texts: List[str]) -&gt; List[List[float]]</code>  <code>async</code>","text":"<p>Embed a list of texts (chunks).</p> Source code in <code>ingestion/embedder.py</code> <pre><code>async def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Embed a list of texts (chunks).\"\"\"\n    all_embeddings: List[List[float]] = []\n\n    # Process in batches\n    for i in range(0, len(texts), self.batch_size):\n        batch = texts[i : i + self.batch_size]\n\n        # Check cache for each item in batch\n        batch_embeddings: List[List[float] | None] = [None] * len(batch)\n        indices_to_fetch = []\n        texts_to_fetch = []\n\n        if self.use_cache and self.cache:\n            for j, text in enumerate(batch):\n                cached = self.cache.get(text)\n                if cached:\n                    batch_embeddings[j] = cached\n                else:\n                    indices_to_fetch.append(j)\n                    texts_to_fetch.append(text)\n        else:\n            indices_to_fetch = list(range(len(batch)))\n            texts_to_fetch = batch\n\n        # Fetch missing embeddings\n        if texts_to_fetch:\n            try:\n                fetched_embeddings = await self._generate_batch_embeddings(texts_to_fetch)\n\n                # Fill back into batch_embeddings and update cache\n                for idx, embedding in zip(indices_to_fetch, fetched_embeddings):\n                    batch_embeddings[idx] = embedding\n                    if self.use_cache and self.cache:\n                        self.cache.set(batch[idx], embedding)\n\n            except Exception as e:\n                logger.error(f\"Failed to embed batch: {e}\")\n                raise\n\n        # Filter out Nones (should not happen if logic is correct)\n        valid_embeddings = [e for e in batch_embeddings if e is not None]\n        all_embeddings.extend(valid_embeddings)\n\n    return all_embeddings\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.EmbeddingGenerator.embed_query","title":"<code>embed_query(text: str) -&gt; List[float]</code>  <code>async</code>","text":"<p>Embed a single query string.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>async def embed_query(self, text: str) -&gt; List[float]:\n    \"\"\"Embed a single query string.\"\"\"\n    # Check cache first\n    if self.use_cache and self.cache:\n        cached = self.cache.get(text)\n        if cached:\n            return cached\n\n    try:\n        embedding = await self._generate_single_embedding(text)\n\n        if self.use_cache and self.cache:\n            self.cache.set(text, embedding)\n\n        return embedding\n    except Exception as e:\n        logger.error(f\"Failed to embed query: {e}\")\n        raise\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.create_embedder","title":"<code>create_embedder(use_cache: bool = True, batch_size: int = 100, max_retries: int = 3, retry_delay: float = 1.0, model_name: Optional[str] = None) -&gt; BaseEmbedder</code>","text":"<p>Factory function to create an embedder instance.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>def create_embedder(\n    use_cache: bool = True,\n    batch_size: int = 100,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n    model_name: Optional[str] = None,\n) -&gt; BaseEmbedder:\n    \"\"\"Factory function to create an embedder instance.\"\"\"\n\n    # Get model from env if not provided\n    if not model_name:\n        model_name = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n\n    return EmbeddingGenerator(model_name=model_name, batch_size=batch_size, use_cache=use_cache)\n</code></pre>"},{"location":"api-reference/ingestion/#ingestion.embedder.main","title":"<code>main()</code>  <code>async</code>","text":"<p>Example usage.</p> Source code in <code>ingestion/embedder.py</code> <pre><code>async def main():\n    \"\"\"Example usage.\"\"\"\n    embedder = create_embedder()\n    res = await embedder.embed_query(\"Hello world\")\n    print(f\"Embedding dimension: {len(res)}\")\n</code></pre>"},{"location":"api-reference/utils/","title":"Utilities Module","text":""},{"location":"api-reference/utils/#database-utilities","title":"Database Utilities","text":""},{"location":"api-reference/utils/#utils.db_utils","title":"<code>utils.db_utils</code>","text":"<p>Database utilities for PostgreSQL connection and operations.</p>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool","title":"<code>DatabasePool</code>","text":"<p>Manages PostgreSQL connection pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>class DatabasePool:\n    \"\"\"Manages PostgreSQL connection pool.\"\"\"\n\n    def __init__(self, database_url: Optional[str] = None):\n        \"\"\"\n        Initialize database pool.\n\n        Args:\n            database_url: PostgreSQL connection URL. If not provided, reads from DATABASE_URL env var.\n        \"\"\"\n        # Don't validate DATABASE_URL here - only check during initialize()\n        # This allows importing the module without requiring DATABASE_URL to be set\n        self.database_url = database_url or os.getenv(\"DATABASE_URL\")\n        self.pool: Optional[Pool] = None\n\n    async def initialize(self):\n        \"\"\"Create optimized connection pool.\"\"\"\n        # Validate DATABASE_URL here, not in __init__\n        if not self.database_url:\n            raise ValueError(\"DATABASE_URL environment variable not set\")\n\n        if not self.pool:\n            # Optimized pool settings for MCP server workload\n            # - min_size=2: Keep minimal connections warm (MCP has bursty traffic)\n            # - max_size=10: Sufficient for concurrent queries (was 20, reduced overhead)\n            # - max_queries=50000: Connection recycling threshold\n            # - statement_cache_size=100: Enable prepared statements (was 0)\n            #   Note: Set to 0 only if using PgBouncer in transaction mode\n            self.pool = await asyncpg.create_pool(\n                self.database_url,\n                min_size=2,  # Reduced from 5 (lower idle overhead)\n                max_size=10,  # Reduced from 20 (sufficient for MCP workload)\n                max_inactive_connection_lifetime=300,\n                command_timeout=60,\n                max_queries=50000,  # Recycle connections periodically\n                statement_cache_size=100,  # Enable prepared statement cache\n                # Set to 0 if using PgBouncer in transaction pooling mode\n            )\n            logger.info(\n                \"\u2713 Database connection pool initialized (min=2, max=10, statement_cache=100)\"\n            )\n\n    async def close(self):\n        \"\"\"Close connection pool.\"\"\"\n        if self.pool:\n            await self.pool.close()\n            self.pool = None\n            logger.info(\"Database connection pool closed\")\n\n    @asynccontextmanager\n    async def acquire(self):\n        \"\"\"Acquire a connection from the pool.\"\"\"\n        if not self.pool:\n            await self.initialize()\n\n        async with self.pool.acquire() as connection:\n            yield connection\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.__init__","title":"<code>__init__(database_url: Optional[str] = None)</code>","text":"<p>Initialize database pool.</p> <p>Parameters:</p> Name Type Description Default <code>database_url</code> <code>Optional[str]</code> <p>PostgreSQL connection URL. If not provided, reads from DATABASE_URL env var.</p> <code>None</code> Source code in <code>utils/db_utils.py</code> <pre><code>def __init__(self, database_url: Optional[str] = None):\n    \"\"\"\n    Initialize database pool.\n\n    Args:\n        database_url: PostgreSQL connection URL. If not provided, reads from DATABASE_URL env var.\n    \"\"\"\n    # Don't validate DATABASE_URL here - only check during initialize()\n    # This allows importing the module without requiring DATABASE_URL to be set\n    self.database_url = database_url or os.getenv(\"DATABASE_URL\")\n    self.pool: Optional[Pool] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.acquire","title":"<code>acquire()</code>  <code>async</code>","text":"<p>Acquire a connection from the pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>@asynccontextmanager\nasync def acquire(self):\n    \"\"\"Acquire a connection from the pool.\"\"\"\n    if not self.pool:\n        await self.initialize()\n\n    async with self.pool.acquire() as connection:\n        yield connection\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close connection pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def close(self):\n    \"\"\"Close connection pool.\"\"\"\n    if self.pool:\n        await self.pool.close()\n        self.pool = None\n        logger.info(\"Database connection pool closed\")\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.DatabasePool.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Create optimized connection pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def initialize(self):\n    \"\"\"Create optimized connection pool.\"\"\"\n    # Validate DATABASE_URL here, not in __init__\n    if not self.database_url:\n        raise ValueError(\"DATABASE_URL environment variable not set\")\n\n    if not self.pool:\n        # Optimized pool settings for MCP server workload\n        # - min_size=2: Keep minimal connections warm (MCP has bursty traffic)\n        # - max_size=10: Sufficient for concurrent queries (was 20, reduced overhead)\n        # - max_queries=50000: Connection recycling threshold\n        # - statement_cache_size=100: Enable prepared statements (was 0)\n        #   Note: Set to 0 only if using PgBouncer in transaction mode\n        self.pool = await asyncpg.create_pool(\n            self.database_url,\n            min_size=2,  # Reduced from 5 (lower idle overhead)\n            max_size=10,  # Reduced from 20 (sufficient for MCP workload)\n            max_inactive_connection_lifetime=300,\n            command_timeout=60,\n            max_queries=50000,  # Recycle connections periodically\n            statement_cache_size=100,  # Enable prepared statement cache\n            # Set to 0 if using PgBouncer in transaction pooling mode\n        )\n        logger.info(\n            \"\u2713 Database connection pool initialized (min=2, max=10, statement_cache=100)\"\n        )\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.close_database","title":"<code>close_database()</code>  <code>async</code>","text":"<p>Close database connection pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def close_database():\n    \"\"\"Close database connection pool.\"\"\"\n    await db_pool.close()\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.execute_query","title":"<code>execute_query(query: str, *params) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Execute a custom query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query</p> required <code>*params</code> <p>Query parameters</p> <code>()</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Query results</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def execute_query(query: str, *params) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Execute a custom query.\n\n    Args:\n        query: SQL query\n        *params: Query parameters\n\n    Returns:\n        Query results\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        results = await conn.fetch(query, *params)\n        return [dict(row) for row in results]\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.get_document","title":"<code>get_document(document_id: str) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Get document by ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>Document UUID</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Document data or None if not found</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def get_document(document_id: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Get document by ID.\n\n    Args:\n        document_id: Document UUID\n\n    Returns:\n        Document data or None if not found\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        result = await conn.fetchrow(\n            \"\"\"\n            SELECT \n                id::text,\n                title,\n                source,\n                content,\n                metadata,\n                created_at,\n                updated_at\n            FROM documents\n            WHERE id = $1::uuid\n            \"\"\",\n            document_id,\n        )\n\n        if result:\n            return {\n                \"id\": result[\"id\"],\n                \"title\": result[\"title\"],\n                \"source\": result[\"source\"],\n                \"content\": result[\"content\"],\n                \"metadata\": json.loads(result[\"metadata\"]),\n                \"created_at\": result[\"created_at\"].isoformat(),\n                \"updated_at\": result[\"updated_at\"].isoformat(),\n            }\n\n        return None\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.initialize_database","title":"<code>initialize_database()</code>  <code>async</code>","text":"<p>Initialize database connection pool.</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def initialize_database():\n    \"\"\"Initialize database connection pool.\"\"\"\n    await db_pool.initialize()\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.list_documents","title":"<code>list_documents(limit: int = 100, offset: int = 0, metadata_filter: Optional[Dict[str, Any]] = None) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>List documents with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of documents to return</p> <code>100</code> <code>offset</code> <code>int</code> <p>Number of documents to skip</p> <code>0</code> <code>metadata_filter</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata filter</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of documents</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def list_documents(\n    limit: int = 100, offset: int = 0, metadata_filter: Optional[Dict[str, Any]] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    List documents with optional filtering.\n\n    Args:\n        limit: Maximum number of documents to return\n        offset: Number of documents to skip\n        metadata_filter: Optional metadata filter\n\n    Returns:\n        List of documents\n    \"\"\"\n    async with db_pool.acquire() as conn:\n        query = \"\"\"\n            SELECT \n                d.id::text,\n                d.title,\n                d.source,\n                d.metadata,\n                d.created_at,\n                d.updated_at,\n                COUNT(c.id) AS chunk_count\n            FROM documents d\n            LEFT JOIN chunks c ON d.id = c.document_id\n        \"\"\"\n\n        params: list[str | int] = []\n        conditions = []\n\n        if metadata_filter:\n            conditions.append(f\"d.metadata @&gt; ${len(params) + 1}::jsonb\")\n            params.append(json.dumps(metadata_filter))\n\n        if conditions:\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        query += \"\"\"\n            GROUP BY d.id, d.title, d.source, d.metadata, d.created_at, d.updated_at\n            ORDER BY d.created_at DESC\n            LIMIT $%d OFFSET $%d\n        \"\"\" % (len(params) + 1, len(params) + 2)\n\n        params.extend([limit, offset])\n\n        results = await conn.fetch(query, *params)\n\n        return [\n            {\n                \"id\": row[\"id\"],\n                \"title\": row[\"title\"],\n                \"source\": row[\"source\"],\n                \"metadata\": json.loads(row[\"metadata\"]),\n                \"created_at\": row[\"created_at\"].isoformat(),\n                \"updated_at\": row[\"updated_at\"].isoformat(),\n                \"chunk_count\": row[\"chunk_count\"],\n            }\n            for row in results\n        ]\n</code></pre>"},{"location":"api-reference/utils/#utils.db_utils.test_connection","title":"<code>test_connection() -&gt; bool</code>  <code>async</code>","text":"<p>Test database connection using a dedicated connection (not the shared pool).</p> <p>This avoids race conditions when health check runs from a different thread than the MCP server.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>utils/db_utils.py</code> <pre><code>async def test_connection() -&gt; bool:\n    \"\"\"\n    Test database connection using a dedicated connection (not the shared pool).\n\n    This avoids race conditions when health check runs from a different thread\n    than the MCP server.\n\n    Returns:\n        True if connection successful, False otherwise\n    \"\"\"\n    try:\n        database_url = os.getenv(\"DATABASE_URL\")\n        if not database_url:\n            logger.debug(\"DATABASE_URL not set\")\n            return False\n\n        # Create a dedicated connection for health check (thread-safe)\n        conn = await asyncpg.connect(database_url, timeout=5)\n        try:\n            await conn.fetchval(\"SELECT 1\")\n            return True\n        finally:\n            await conn.close()\n    except Exception as e:\n        logger.error(f\"Database connection test failed: {e}\")\n        return False\n</code></pre>"},{"location":"api-reference/utils/#models","title":"Models","text":""},{"location":"api-reference/utils/#utils.models","title":"<code>utils.models</code>","text":"<p>Pydantic models for data validation and serialization.</p>"},{"location":"api-reference/utils/#utils.models.AgentContext","title":"<code>AgentContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Agent execution context.</p> Source code in <code>utils/models.py</code> <pre><code>class AgentContext(BaseModel):\n    \"\"\"Agent execution context.\"\"\"\n\n    session_id: str\n    messages: List[Message] = Field(default_factory=list)\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n    search_results: List[ChunkResult] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.AgentDependencies","title":"<code>AgentDependencies</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dependencies for the agent.</p> Source code in <code>utils/models.py</code> <pre><code>class AgentDependencies(BaseModel):\n    \"\"\"Dependencies for the agent.\"\"\"\n\n    session_id: str\n    database_url: Optional[str] = None\n    openai_api_key: Optional[str] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChatResponse","title":"<code>ChatResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Chat response model.</p> Source code in <code>utils/models.py</code> <pre><code>class ChatResponse(BaseModel):\n    \"\"\"Chat response model.\"\"\"\n\n    message: str\n    session_id: str\n    sources: List[DocumentMetadata] = Field(default_factory=list)\n    tools_used: List[ToolCall] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document chunk model.</p> Source code in <code>utils/models.py</code> <pre><code>class Chunk(BaseModel):\n    \"\"\"Document chunk model.\"\"\"\n\n    id: Optional[str] = None\n    document_id: str\n    content: str\n    embedding: Optional[List[float]] = None\n    chunk_index: int\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    token_count: Optional[int] = None\n    created_at: Optional[datetime] = None\n\n    @field_validator(\"embedding\")\n    @classmethod\n    def validate_embedding(cls, v: Optional[List[float]]) -&gt; Optional[List[float]]:\n        \"\"\"Validate embedding dimensions.\"\"\"\n        if v is not None and len(v) != 1536:  # OpenAI text-embedding-3-small\n            raise ValueError(f\"Embedding must have 1536 dimensions, got {len(v)}\")\n        return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Chunk.validate_embedding","title":"<code>validate_embedding(v: Optional[List[float]]) -&gt; Optional[List[float]]</code>  <code>classmethod</code>","text":"<p>Validate embedding dimensions.</p> Source code in <code>utils/models.py</code> <pre><code>@field_validator(\"embedding\")\n@classmethod\ndef validate_embedding(cls, v: Optional[List[float]]) -&gt; Optional[List[float]]:\n    \"\"\"Validate embedding dimensions.\"\"\"\n    if v is not None and len(v) != 1536:  # OpenAI text-embedding-3-small\n        raise ValueError(f\"Embedding must have 1536 dimensions, got {len(v)}\")\n    return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChunkResult","title":"<code>ChunkResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Chunk search result model.</p> Source code in <code>utils/models.py</code> <pre><code>class ChunkResult(BaseModel):\n    \"\"\"Chunk search result model.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    document_title: str\n    document_source: str\n\n    @field_validator(\"score\")\n    @classmethod\n    def validate_score(cls, v: float) -&gt; float:\n        \"\"\"Ensure score is between 0 and 1.\"\"\"\n        return max(0.0, min(1.0, v))\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ChunkResult.validate_score","title":"<code>validate_score(v: float) -&gt; float</code>  <code>classmethod</code>","text":"<p>Ensure score is between 0 and 1.</p> Source code in <code>utils/models.py</code> <pre><code>@field_validator(\"score\")\n@classmethod\ndef validate_score(cls, v: float) -&gt; float:\n    \"\"\"Ensure score is between 0 and 1.\"\"\"\n    return max(0.0, min(1.0, v))\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document model.</p> Source code in <code>utils/models.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"Document model.\"\"\"\n\n    id: Optional[str] = None\n    title: str\n    source: str\n    content: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.DocumentMetadata","title":"<code>DocumentMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document metadata model.</p> Source code in <code>utils/models.py</code> <pre><code>class DocumentMetadata(BaseModel):\n    \"\"\"Document metadata model.\"\"\"\n\n    id: str\n    title: str\n    source: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime\n    updated_at: datetime\n    chunk_count: Optional[int] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionConfig","title":"<code>IngestionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for document ingestion.</p> Source code in <code>utils/models.py</code> <pre><code>class IngestionConfig(BaseModel):\n    \"\"\"Configuration for document ingestion.\"\"\"\n\n    chunk_size: int = Field(default=1000, ge=100, le=5000)\n    chunk_overlap: int = Field(default=200, ge=0, le=1000)\n    max_chunk_size: int = Field(default=2000, ge=500, le=10000)\n    use_semantic_chunking: bool = True\n\n    @field_validator(\"chunk_overlap\")\n    @classmethod\n    def validate_overlap(cls, v: int, info) -&gt; int:\n        \"\"\"Ensure overlap is less than chunk size.\"\"\"\n        chunk_size = info.data.get(\"chunk_size\", 1000)\n        if v &gt;= chunk_size:\n            raise ValueError(f\"Chunk overlap ({v}) must be less than chunk size ({chunk_size})\")\n        return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionConfig.validate_overlap","title":"<code>validate_overlap(v: int, info) -&gt; int</code>  <code>classmethod</code>","text":"<p>Ensure overlap is less than chunk size.</p> Source code in <code>utils/models.py</code> <pre><code>@field_validator(\"chunk_overlap\")\n@classmethod\ndef validate_overlap(cls, v: int, info) -&gt; int:\n    \"\"\"Ensure overlap is less than chunk size.\"\"\"\n    chunk_size = info.data.get(\"chunk_size\", 1000)\n    if v &gt;= chunk_size:\n        raise ValueError(f\"Chunk overlap ({v}) must be less than chunk size ({chunk_size})\")\n    return v\n</code></pre>"},{"location":"api-reference/utils/#utils.models.IngestionResult","title":"<code>IngestionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of document ingestion.</p> Source code in <code>utils/models.py</code> <pre><code>class IngestionResult(BaseModel):\n    \"\"\"Result of document ingestion.\"\"\"\n\n    document_id: str\n    title: str\n    chunks_created: int\n    processing_time_ms: float\n    errors: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message model.</p> Source code in <code>utils/models.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"Message model.\"\"\"\n\n    id: Optional[str] = None\n    session_id: str\n    role: MessageRole\n    content: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n\n    model_config = ConfigDict(use_enum_values=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.MessageRole","title":"<code>MessageRole</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enum.</p> Source code in <code>utils/models.py</code> <pre><code>class MessageRole(str, Enum):\n    \"\"\"Message role enum.\"\"\"\n\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    SYSTEM = \"system\"\n</code></pre>"},{"location":"api-reference/utils/#utils.models.QueryLog","title":"<code>QueryLog</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Query log entry model for session tracking.</p> Source code in <code>utils/models.py</code> <pre><code>class QueryLog(BaseModel):\n    \"\"\"Query log entry model for session tracking.\"\"\"\n\n    session_id: UUID\n    query_text: str\n    response_text: Optional[str] = None\n    cost: Decimal = Decimal(\"0.0\")\n    latency_ms: Decimal = Decimal(\"0.0\")\n    timestamp: datetime\n    langfuse_trace_id: Optional[str] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchRequest","title":"<code>SearchRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search request model.</p> Source code in <code>utils/models.py</code> <pre><code>class SearchRequest(BaseModel):\n    \"\"\"Search request model.\"\"\"\n\n    query: str = Field(..., description=\"Search query\")\n    search_type: SearchType = Field(default=SearchType.SEMANTIC, description=\"Type of search\")\n    limit: int = Field(default=10, ge=1, le=50, description=\"Maximum results\")\n    filters: Dict[str, Any] = Field(default_factory=dict, description=\"Search filters\")\n\n    model_config = ConfigDict(use_enum_values=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchResponse","title":"<code>SearchResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search response model.</p> Source code in <code>utils/models.py</code> <pre><code>class SearchResponse(BaseModel):\n    \"\"\"Search response model.\"\"\"\n\n    results: List[ChunkResult] = Field(default_factory=list)\n    total_results: int = 0\n    search_type: SearchType\n    query_time_ms: float\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SearchType","title":"<code>SearchType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Search type enum.</p> Source code in <code>utils/models.py</code> <pre><code>class SearchType(str, Enum):\n    \"\"\"Search type enum.\"\"\"\n\n    SEMANTIC = \"semantic\"\n    KEYWORD = \"keyword\"\n    HYBRID = \"hybrid\"\n</code></pre>"},{"location":"api-reference/utils/#utils.models.Session","title":"<code>Session</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Session model.</p> Source code in <code>utils/models.py</code> <pre><code>class Session(BaseModel):\n    \"\"\"Session model.\"\"\"\n\n    id: Optional[str] = None\n    user_id: Optional[str] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n    expires_at: Optional[datetime] = None\n</code></pre>"},{"location":"api-reference/utils/#utils.models.SessionStats","title":"<code>SessionStats</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Session statistics model for Streamlit UI observability.</p> Source code in <code>utils/models.py</code> <pre><code>class SessionStats(BaseModel):\n    \"\"\"Session statistics model for Streamlit UI observability.\"\"\"\n\n    session_id: UUID\n    query_count: int = 0\n    total_cost: Decimal = Decimal(\"0.0\")\n    avg_latency_ms: Decimal = Decimal(\"0.0\")\n    created_at: datetime\n    last_activity: datetime\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.StreamDelta","title":"<code>StreamDelta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming response delta.</p> Source code in <code>utils/models.py</code> <pre><code>class StreamDelta(BaseModel):\n    \"\"\"Streaming response delta.\"\"\"\n\n    content: str\n    delta_type: Literal[\"text\", \"tool_call\", \"end\"] = \"text\"\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/utils/#utils.models.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool call information model.</p> Source code in <code>utils/models.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"Tool call information model.\"\"\"\n\n    tool_name: str\n    args: Dict[str, Any] = Field(default_factory=dict)\n    tool_call_id: Optional[str] = None\n</code></pre>"},{"location":"api-reference/utils/#providers","title":"Providers","text":""},{"location":"api-reference/utils/#utils.providers","title":"<code>utils.providers</code>","text":"<p>Simplified provider configuration for OpenAI models only.</p>"},{"location":"api-reference/utils/#utils.providers.ProviderConfig","title":"<code>ProviderConfig</code>  <code>dataclass</code>","text":"<p>Configuration for AI provider.</p> Source code in <code>utils/providers.py</code> <pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"Configuration for AI provider.\"\"\"\n\n    api_key: Optional[str] = None\n    base_url: Optional[str] = None\n    embedding_model: str = \"text-embedding-3-small\"\n    llm_model: str = \"gpt-4.1-mini\"\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_embedding_client","title":"<code>get_embedding_client() -&gt; openai.AsyncOpenAI</code>","text":"<p>Get OpenAI client for embeddings.</p> <p>Returns:</p> Type Description <code>AsyncOpenAI</code> <p>Configured OpenAI client for embeddings</p> Source code in <code>utils/providers.py</code> <pre><code>def get_embedding_client() -&gt; openai.AsyncOpenAI:\n    \"\"\"\n    Get OpenAI client for embeddings.\n\n    Returns:\n        Configured OpenAI client for embeddings\n    \"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n    return openai.AsyncOpenAI(api_key=api_key)\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_embedding_model","title":"<code>get_embedding_model() -&gt; str</code>","text":"<p>Get embedding model name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Embedding model name</p> Source code in <code>utils/providers.py</code> <pre><code>def get_embedding_model() -&gt; str:\n    \"\"\"\n    Get embedding model name.\n\n    Returns:\n        Embedding model name\n    \"\"\"\n    return os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_ingestion_model","title":"<code>get_ingestion_model() -&gt; OpenAIModel</code>","text":"<p>Get model for ingestion tasks (uses same model as main LLM).</p> <p>Returns:</p> Type Description <code>OpenAIModel</code> <p>Configured model for ingestion tasks</p> Source code in <code>utils/providers.py</code> <pre><code>def get_ingestion_model() -&gt; OpenAIModel:\n    \"\"\"\n    Get model for ingestion tasks (uses same model as main LLM).\n\n    Returns:\n        Configured model for ingestion tasks\n    \"\"\"\n    return get_llm_model()\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_llm_model","title":"<code>get_llm_model() -&gt; OpenAIModel</code>","text":"<p>Get LLM model configuration for OpenAI.</p> <p>Returns:</p> Type Description <code>OpenAIModel</code> <p>Configured OpenAI model</p> Source code in <code>utils/providers.py</code> <pre><code>def get_llm_model() -&gt; OpenAIModel:\n    \"\"\"\n    Get LLM model configuration for OpenAI.\n\n    Returns:\n        Configured OpenAI model\n    \"\"\"\n    llm_choice = os.getenv(\"LLM_CHOICE\", \"gpt-4.1-mini\")\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n    return OpenAIModel(llm_choice, provider=OpenAIProvider(api_key=api_key))\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_model_info","title":"<code>get_model_info() -&gt; dict</code>","text":"<p>Get information about current model configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with model configuration info</p> Source code in <code>utils/providers.py</code> <pre><code>def get_model_info() -&gt; dict:\n    \"\"\"\n    Get information about current model configuration.\n\n    Returns:\n        Dictionary with model configuration info\n    \"\"\"\n    return {\n        \"llm_provider\": \"openai\",\n        \"llm_model\": os.getenv(\"LLM_CHOICE\", \"gpt-4.1-mini\"),\n        \"embedding_provider\": \"openai\",\n        \"embedding_model\": get_embedding_model(),\n    }\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.get_provider_config","title":"<code>get_provider_config() -&gt; ProviderConfig</code>","text":"<p>Get provider configuration from environment.</p> Source code in <code>utils/providers.py</code> <pre><code>def get_provider_config() -&gt; ProviderConfig:\n    \"\"\"Get provider configuration from environment.\"\"\"\n    return ProviderConfig(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n        embedding_model=os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\"),\n        llm_model=os.getenv(\"LLM_CHOICE\", \"gpt-4.1-mini\"),\n    )\n</code></pre>"},{"location":"api-reference/utils/#utils.providers.validate_configuration","title":"<code>validate_configuration() -&gt; bool</code>","text":"<p>Validate that required environment variables are set.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p> Source code in <code>utils/providers.py</code> <pre><code>def validate_configuration() -&gt; bool:\n    \"\"\"\n    Validate that required environment variables are set.\n\n    Returns:\n        True if configuration is valid\n    \"\"\"\n    required_vars = [\"OPENAI_API_KEY\", \"DATABASE_URL\"]\n\n    missing_vars = []\n    for var in required_vars:\n        if not os.getenv(var):\n            missing_vars.append(var)\n\n    if missing_vars:\n        print(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n        return False\n\n    return True\n</code></pre>"}]}