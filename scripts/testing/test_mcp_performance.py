#!/usr/bin/env python3
"""
Test MCP Server Performance
============================
Validates optimizations and measures actual performance.

Usage:
    uv run python scripts/test_mcp_performance.py
"""

import asyncio
import sys
import time
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.rag_service import (
    close_global_embedder,
    initialize_global_embedder,
    search_knowledge_base,
)
from utils.db_utils import close_database, initialize_database


async def test_initialization():
    """Test server initialization time."""
    print("üß™ Test 1: Initialization Performance")
    print("-" * 50)

    start = time.time()

    # Initialize database
    await initialize_database()
    db_time = time.time() - start
    print(f"‚úì Database pool initialized: {db_time * 1000:.0f}ms")

    # Initialize embedder
    start = time.time()
    await initialize_global_embedder()
    embedder_time = time.time() - start
    print(f"‚úì Global embedder initialized: {embedder_time * 1000:.0f}ms")

    total_time = (db_time + embedder_time) * 1000
    print(f"\n‚è±Ô∏è  Total startup time: {total_time:.0f}ms")

    if total_time < 2000:
        print("‚úÖ PASS: Startup time <2s")
    else:
        print("‚ö†Ô∏è  SLOW: Startup time >2s")

    return total_time


async def test_query_performance():
    """Test query performance with timing breakdown."""
    print("\n\nüß™ Test 2: Query Performance")
    print("-" * 50)

    test_queries = ["What is Docling?", "How to use PydanticAI?", "Langfuse deployment guide"]

    results = []

    for i, query in enumerate(test_queries, 1):
        print(f"\nQuery {i}: '{query}'")

        start = time.time()
        try:
            result = await search_knowledge_base(query, limit=3)
            elapsed = (time.time() - start) * 1000

            print(f"‚è±Ô∏è  Latency: {elapsed:.0f}ms")

            if "Found" in result:
                print("‚úì Results returned")
            else:
                print(f"‚ö†Ô∏è  No results: {result[:100]}")

            results.append(elapsed)

        except Exception as e:
            print(f"‚ùå Error: {e}")
            results.append(999999)

    # Calculate statistics
    if results:
        avg_latency = sum(results) / len(results)
        max_latency = max(results)
        min_latency = min(results)

        print("\nüìä Query Statistics:")
        print(f"   Avg: {avg_latency:.0f}ms")
        print(f"   Min: {min_latency:.0f}ms")
        print(f"   Max: {max_latency:.0f}ms")

        # Performance assessment
        if avg_latency < 200:
            print("\n‚úÖ EXCELLENT: Average latency <200ms")
        elif avg_latency < 500:
            print("\nüü° GOOD: Average latency <500ms")
        else:
            print("\nüî¥ NEEDS OPTIMIZATION: Average latency >500ms")

        return avg_latency

    return None


async def test_cache_effectiveness():
    """Test embedding cache effectiveness."""
    print("\n\nüß™ Test 3: Cache Effectiveness")
    print("-" * 50)

    query = "Test query for cache performance"

    # First query (cache miss)
    print("\nFirst query (cache miss):")
    start = time.time()
    await search_knowledge_base(query, limit=3)
    first_latency = (time.time() - start) * 1000
    print(f"‚è±Ô∏è  Latency: {first_latency:.0f}ms")

    # Second query (cache hit)
    print("\nSecond query (should hit cache):")
    start = time.time()
    await search_knowledge_base(query, limit=3)
    second_latency = (time.time() - start) * 1000
    print(f"‚è±Ô∏è  Latency: {second_latency:.0f}ms")

    # Calculate improvement
    improvement = ((first_latency - second_latency) / first_latency) * 100

    print("\nüìä Cache Performance:")
    print(f"   First query: {first_latency:.0f}ms")
    print(f"   Cached query: {second_latency:.0f}ms")
    print(f"   Improvement: {improvement:.1f}%")

    if improvement > 20:
        print("\n‚úÖ PASS: Cache providing >20% speedup")
    else:
        print("\n‚ö†Ô∏è  WARNING: Cache not effective (<20% speedup)")

    return improvement


async def main():
    """Run all performance tests."""
    print("=" * 50)
    print("MCP Server Performance Tests")
    print("=" * 50)

    try:
        # Test 1: Initialization
        startup_time = await test_initialization()

        # Test 2: Query performance
        avg_latency = await test_query_performance()

        # Test 3: Cache effectiveness
        cache_improvement = await test_cache_effectiveness()

        # Final summary
        print("\n\n" + "=" * 50)
        print("üìã SUMMARY")
        print("=" * 50)
        print(f"Startup time: {startup_time:.0f}ms")
        if avg_latency:
            print(f"Average query latency: {avg_latency:.0f}ms")
        if cache_improvement:
            print(f"Cache speedup: {cache_improvement:.1f}%")

        print("\nüéØ Performance Targets:")
        print(f"   Startup: <2000ms - {'‚úÖ PASS' if startup_time < 2000 else '‚ùå FAIL'}")
        if avg_latency:
            print(f"   Query: <300ms - {'‚úÖ PASS' if avg_latency < 300 else '‚ö†Ô∏è  NEEDS WORK'}")
        if cache_improvement:
            print(f"   Cache: >20% - {'‚úÖ PASS' if cache_improvement > 20 else '‚ö†Ô∏è  NEEDS WORK'}")

    except Exception as e:
        print(f"\n‚ùå Test suite failed: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # Cleanup
        print("\nüßπ Cleaning up...")
        await close_global_embedder()
        await close_database()
        print("‚úì Cleanup complete")


if __name__ == "__main__":
    asyncio.run(main())
